{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/mlboot_dataset/'\n",
    "model_name = 'e9'\n",
    "results_dir = './results/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 2053602) (609018, 20275) (609018, 1057788)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir + 'preprocessed_new.csv') \n",
    "q = pd.read_csv(data_dir + 'sessions.csv')\n",
    "df = df.merge(q, on='uid', how='left')\n",
    "del q\n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index\n",
    "\n",
    "mat1 = sp.load_npz(data_dir+'dmat1.npz').tolil()\n",
    "mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil()\n",
    "mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil()\n",
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_f1'] = mat1.tocsr().max(axis=1).todense()\n",
    "df['max_f2'] = mat2.tocsr().max(axis=1).todense()\n",
    "df['max_f3'] = mat3.tocsr().max(axis=1).todense()\n",
    "\n",
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "limit = 11\n",
    "mat1 = mat1.tocsc()[:, np.where((train_mat1.getnnz(axis=0) > limit) & (test_mat1.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat2 = mat2.tocsc()[:, np.where((train_mat2.getnnz(axis=0) > limit) & (test_mat2.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat3 = mat3.tocsc()[:, np.where((train_mat3.getnnz(axis=0) > limit) & (test_mat3.getnnz(axis=0) > 0))[0]].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 195734) (609018, 20268) (609018, 9415)\n"
     ]
    }
   ],
   "source": [
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'pca_cat10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_description_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd\n",
    "data_svd = pd.DataFrame(np.load(data_dir + 'apca_dim10.npy'), index=df.index)\n",
    "data_svd.columns = ['asvd_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'bin_pca_dim10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_title_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "del mat1,mat2,mat3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
       "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
       "       'records', 'max_days', 'min_days',\n",
       "       ...\n",
       "       'svd_title_1', 'svd_title_2', 'svd_title_3', 'svd_title_4',\n",
       "       'svd_title_5', 'svd_title_6', 'svd_title_7', 'svd_title_8',\n",
       "       'svd_title_9', 'svd_title_10'],\n",
       "      dtype='object', length=117)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_baseline_sparse_10folds.npy'))\n",
    "sample_sub.columns = ['uid','lgbm1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_all_in_focal_loss.npy'))\n",
    "sample_sub.columns = ['uid','nnet2']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m3.npy'))\n",
    "sample_sub.columns = ['uid','nnet3']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m1.npy'))\n",
    "sample_sub.columns = ['uid','nnet1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_ftrl_50.npy'))\n",
    "sample_sub.columns = ['uid','ftrl_50']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_ftrl_70.npy'))\n",
    "sample_sub.columns = ['uid','ftrl']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model.npy'))\n",
    "sample_sub.columns = ['uid','nnet4']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_catpca.npy'))\n",
    "sample_sub.columns = ['uid','nnet5']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br.npy'))\n",
    "sample_sub.columns = ['uid','nnet6']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_vw_03perc_099powert_ftrl.npy'))\n",
    "sample_sub.columns = ['uid','vw']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_vw_07perc.npy'))\n",
    "sample_sub.columns = ['uid','vw2']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_sgd_091perc.npy'))\n",
    "sample_sub.columns = ['uid','sgd1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_tfifg_5folds.npy'))\n",
    "sample_sub.columns = ['uid','tfidf1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_tfifg_svd_5folds.npy'))\n",
    "sample_sub.columns = ['uid','tfidf2']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_wcc.npy'))\n",
    "sample_sub.columns = ['uid','nnet7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_tanh.npy'))\n",
    "sample_sub.columns = ['uid','nnet8']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_binary_lgbm.npy'))\n",
    "sample_sub.columns = ['uid','lgbmb']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_v4.npy'))\n",
    "sample_sub.columns = ['uid','nnet10']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_v5.npy'))\n",
    "sample_sub.columns = ['uid','nnet11']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_xgb_single.npy'))\n",
    "sample_sub.columns = ['uid','xgb_single']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_3br_bin_v2.npy'))\n",
    "sample_sub.columns = ['uid','nnet12']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_bin.npy'))\n",
    "sample_sub.columns = ['uid','nnet13']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_3br_bin_v4.npy'))\n",
    "sample_sub.columns = ['uid','nnet14']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_3br_bin_v5.npy'))\n",
    "sample_sub.columns = ['uid','nnet15']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_sparse_10folds_timp.npy'))\n",
    "sample_sub.columns = ['uid','timp']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_3br_bin_v8.npy'))\n",
    "sample_sub.columns = ['uid','nnet16']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_3br_bin_v9.npy'))\n",
    "sample_sub.columns = ['uid','nnet17']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_we_v2.npy'))\n",
    "sample_sub.columns = ['uid','nnet18']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_all_in_focal_loss.npy'))\n",
    "sample_sub.columns = ['uid','nnet19']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_fm_svd_3br.npy'))\n",
    "sample_sub.columns = ['uid','nnet20']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_deep_tree.npy'))\n",
    "sample_sub.columns = ['uid','deeptree']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_fm_svd_3br_v3.npy'))\n",
    "sample_sub.columns = ['uid','nnet21']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_we_v5.npy'))\n",
    "sample_sub.columns = ['uid','nnet22']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_deep_tree1.npy'))\n",
    "sample_sub.columns = ['uid','dt1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_deep_tree2.npy'))\n",
    "sample_sub.columns = ['uid','dt2']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['lgbm1'] = minmax_scale(np.load(results_dir + 'train_baseline_sparse_10folds.npy'))\n",
    "X['lgbmb'] = minmax_scale(np.load(results_dir + 'train_binary_lgbm.npy'))\n",
    "X['nnet2'] = minmax_scale(np.load(results_dir + 'train_all_in_focal_loss.npy'))\n",
    "X['nnet1'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m1.npy'))\n",
    "X['nnet3'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m3.npy'))\n",
    "X['nnet4'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model.npy'))\n",
    "X['nnet5'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_catpca.npy'))\n",
    "X['nnet6'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br.npy'))\n",
    "X['nnet7'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_wcc.npy'))\n",
    "X['nnet8'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_tanh.npy'))\n",
    "X['ftrl_50']  = minmax_scale(np.load(results_dir + 'train_ftrl_50.npy'))\n",
    "X['ftrl']  = minmax_scale(np.load(results_dir + 'train_ftrl_70.npy'))\n",
    "X['nnet10'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_v4.npy'))\n",
    "X['nnet11'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_v5.npy'))\n",
    "X['xgb_single']  = minmax_scale(np.load(results_dir + 'train_xgb_single.npy'))\n",
    "X['nnet12'] = minmax_scale(np.load(results_dir + 'train_nn_3br_bin_v2.npy'))\n",
    "X['nnet13'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_bin.npy'))\n",
    "X['vw'] = minmax_scale(np.load(results_dir + 'train_vw_03perc_099powert_ftrl.npy'))\n",
    "X['vw2'] = minmax_scale(np.load(results_dir + 'train_vw_07perc.npy'))\n",
    "X['sgd1'] = minmax_scale(np.load(results_dir + 'train_sgd_091perc.npy'))\n",
    "X['nnet14'] = minmax_scale(np.load(results_dir + 'train_nn_3br_bin_v4.npy'))\n",
    "X['nnet15'] = minmax_scale(np.load(results_dir + 'train_nn_3br_bin_v5.npy'))\n",
    "X['timp'] = minmax_scale(np.load(results_dir + 'train_sparse_10folds_timp.npy'))\n",
    "X['nnet16'] = minmax_scale(np.load(results_dir + 'train_nn_3br_bin_v8.npy'))\n",
    "X['nnet17'] = minmax_scale(np.load(results_dir + 'train_nn_3br_bin_v9.npy'))\n",
    "X['nnet18'] = minmax_scale(np.load(results_dir + 'train_nn_we_v2.npy'))\n",
    "X['tfidf1'] = minmax_scale(np.load(results_dir + 'train_tfifg_5folds.npy'))\n",
    "X['tfidf2'] = minmax_scale(np.load(results_dir + 'train_tfifg_svd_5folds.npy'))\n",
    "X['nnet19'] = minmax_scale(np.load(results_dir + 'train_all_in_focal_loss.npy'))\n",
    "X['nnet20'] = minmax_scale(np.load(results_dir + 'train_fm_svd_3br.npy'))\n",
    "X['deeptree'] = minmax_scale(np.load(results_dir + 'train_deep_tree.npy'))\n",
    "X['nnet21'] = minmax_scale(np.load(results_dir + 'train_fm_svd_3br_v3.npy'))\n",
    "X['nnet22'] = minmax_scale(np.load(results_dir + 'train_nn_we_v5.npy'))\n",
    "X['dt1'] = minmax_scale(np.load(results_dir + 'train_deep_tree1.npy'))\n",
    "X['dt2'] = minmax_scale(np.load(results_dir + 'train_deep_tree2.npy'))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm1 0.682562603224\n",
      "nnet2 0.681848937554\n",
      "nnet1 0.623698784984\n",
      "nnet3 0.574520607881\n",
      "nnet4 0.647936920552\n",
      "nnet5 0.648281500429\n",
      "ftrl 0.622924063126\n",
      "nnet6 0.658849449887\n",
      "nnet8 0.657409030233\n",
      "ftrl_50 0.623440994459\n",
      "nnet11 0.660053076044\n",
      "xgb_single 0.660175309563\n",
      "lgbmb 0.66792997872\n",
      "nnet12 0.67274153339\n",
      "nnet13 0.67264163497\n",
      "nnet14 0.671105467501\n",
      "nnet15 0.678910109891\n",
      "nnet18 0.677717274239\n",
      "tfidf1 0.679597805043\n",
      "tfidf2 0.682393866377\n",
      "nnet19 0.681848937554\n",
      "nnet20 0.682987673047\n"
     ]
    }
   ],
   "source": [
    "for f in ['lgbm1','nnet2','nnet1','nnet3','nnet4','nnet5','ftrl',\n",
    "          'nnet6','nnet8','ftrl_50','nnet11','xgb_single','lgbmb',\n",
    "          'nnet12','nnet13','nnet14','nnet15','nnet18','tfidf1','tfidf2','nnet19','nnet20']:\n",
    "    print(f,roc_auc_score(X.target, X[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "      <th>tfidf2</th>\n",
       "      <th>vw2</th>\n",
       "      <th>sgd1</th>\n",
       "      <th>nnet18</th>\n",
       "      <th>nnet17</th>\n",
       "      <th>nnet19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815829</td>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.769820</td>\n",
       "      <td>0.780070</td>\n",
       "      <td>0.795601</td>\n",
       "      <td>0.814261</td>\n",
       "      <td>0.713636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.815829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780991</td>\n",
       "      <td>0.845585</td>\n",
       "      <td>0.806378</td>\n",
       "      <td>0.703029</td>\n",
       "      <td>0.686586</td>\n",
       "      <td>0.769341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf2</th>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.780991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.740278</td>\n",
       "      <td>0.707507</td>\n",
       "      <td>0.690532</td>\n",
       "      <td>0.672900</td>\n",
       "      <td>0.871544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vw2</th>\n",
       "      <td>0.769820</td>\n",
       "      <td>0.845585</td>\n",
       "      <td>0.740278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918504</td>\n",
       "      <td>0.725528</td>\n",
       "      <td>0.699212</td>\n",
       "      <td>0.756955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgd1</th>\n",
       "      <td>0.780070</td>\n",
       "      <td>0.806378</td>\n",
       "      <td>0.707507</td>\n",
       "      <td>0.918504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727218</td>\n",
       "      <td>0.727685</td>\n",
       "      <td>0.735116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet18</th>\n",
       "      <td>0.795601</td>\n",
       "      <td>0.703029</td>\n",
       "      <td>0.690532</td>\n",
       "      <td>0.725528</td>\n",
       "      <td>0.727218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.813697</td>\n",
       "      <td>0.751477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet17</th>\n",
       "      <td>0.814261</td>\n",
       "      <td>0.686586</td>\n",
       "      <td>0.672900</td>\n",
       "      <td>0.699212</td>\n",
       "      <td>0.727685</td>\n",
       "      <td>0.813697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet19</th>\n",
       "      <td>0.713636</td>\n",
       "      <td>0.769341</td>\n",
       "      <td>0.871544</td>\n",
       "      <td>0.756955</td>\n",
       "      <td>0.735116</td>\n",
       "      <td>0.751477</td>\n",
       "      <td>0.730332</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nnet12    nnet13    tfidf2       vw2      sgd1    nnet18    nnet17  \\\n",
       "nnet12  1.000000  0.815829  0.671611  0.769820  0.780070  0.795601  0.814261   \n",
       "nnet13  0.815829  1.000000  0.780991  0.845585  0.806378  0.703029  0.686586   \n",
       "tfidf2  0.671611  0.780991  1.000000  0.740278  0.707507  0.690532  0.672900   \n",
       "vw2     0.769820  0.845585  0.740278  1.000000  0.918504  0.725528  0.699212   \n",
       "sgd1    0.780070  0.806378  0.707507  0.918504  1.000000  0.727218  0.727685   \n",
       "nnet18  0.795601  0.703029  0.690532  0.725528  0.727218  1.000000  0.813697   \n",
       "nnet17  0.814261  0.686586  0.672900  0.699212  0.727685  0.813697  1.000000   \n",
       "nnet19  0.713636  0.769341  0.871544  0.756955  0.735116  0.751477  0.730332   \n",
       "\n",
       "          nnet19  \n",
       "nnet12  0.713636  \n",
       "nnet13  0.769341  \n",
       "tfidf2  0.871544  \n",
       "vw2     0.756955  \n",
       "sgd1    0.735116  \n",
       "nnet18  0.751477  \n",
       "nnet17  0.730332  \n",
       "nnet19  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['nnet12','nnet13','tfidf2','vw2','sgd1','nnet18','nnet17','nnet19']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "      <th>tfidf2</th>\n",
       "      <th>vw2</th>\n",
       "      <th>sgd1</th>\n",
       "      <th>nnet18</th>\n",
       "      <th>nnet17</th>\n",
       "      <th>nnet19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>0.690019</td>\n",
       "      <td>0.795288</td>\n",
       "      <td>0.776215</td>\n",
       "      <td>0.827933</td>\n",
       "      <td>0.833681</td>\n",
       "      <td>0.720283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.872521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785687</td>\n",
       "      <td>0.845255</td>\n",
       "      <td>0.793905</td>\n",
       "      <td>0.740004</td>\n",
       "      <td>0.721948</td>\n",
       "      <td>0.781601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf2</th>\n",
       "      <td>0.690019</td>\n",
       "      <td>0.785687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703277</td>\n",
       "      <td>0.655306</td>\n",
       "      <td>0.715536</td>\n",
       "      <td>0.700106</td>\n",
       "      <td>0.861969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vw2</th>\n",
       "      <td>0.795288</td>\n",
       "      <td>0.845255</td>\n",
       "      <td>0.703277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922127</td>\n",
       "      <td>0.726212</td>\n",
       "      <td>0.718369</td>\n",
       "      <td>0.729844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgd1</th>\n",
       "      <td>0.776215</td>\n",
       "      <td>0.793905</td>\n",
       "      <td>0.655306</td>\n",
       "      <td>0.922127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685385</td>\n",
       "      <td>0.709385</td>\n",
       "      <td>0.691483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet18</th>\n",
       "      <td>0.827933</td>\n",
       "      <td>0.740004</td>\n",
       "      <td>0.715536</td>\n",
       "      <td>0.726212</td>\n",
       "      <td>0.685385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858265</td>\n",
       "      <td>0.758730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet17</th>\n",
       "      <td>0.833681</td>\n",
       "      <td>0.721948</td>\n",
       "      <td>0.700106</td>\n",
       "      <td>0.718369</td>\n",
       "      <td>0.709385</td>\n",
       "      <td>0.858265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet19</th>\n",
       "      <td>0.720283</td>\n",
       "      <td>0.781601</td>\n",
       "      <td>0.861969</td>\n",
       "      <td>0.729844</td>\n",
       "      <td>0.691483</td>\n",
       "      <td>0.758730</td>\n",
       "      <td>0.739510</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nnet12    nnet13    tfidf2       vw2      sgd1    nnet18    nnet17  \\\n",
       "nnet12  1.000000  0.872521  0.690019  0.795288  0.776215  0.827933  0.833681   \n",
       "nnet13  0.872521  1.000000  0.785687  0.845255  0.793905  0.740004  0.721948   \n",
       "tfidf2  0.690019  0.785687  1.000000  0.703277  0.655306  0.715536  0.700106   \n",
       "vw2     0.795288  0.845255  0.703277  1.000000  0.922127  0.726212  0.718369   \n",
       "sgd1    0.776215  0.793905  0.655306  0.922127  1.000000  0.685385  0.709385   \n",
       "nnet18  0.827933  0.740004  0.715536  0.726212  0.685385  1.000000  0.858265   \n",
       "nnet17  0.833681  0.721948  0.700106  0.718369  0.709385  0.858265  1.000000   \n",
       "nnet19  0.720283  0.781601  0.861969  0.729844  0.691483  0.758730  0.739510   \n",
       "\n",
       "          nnet19  \n",
       "nnet12  0.720283  \n",
       "nnet13  0.781601  \n",
       "tfidf2  0.861969  \n",
       "vw2     0.729844  \n",
       "sgd1    0.691483  \n",
       "nnet18  0.758730  \n",
       "nnet17  0.739510  \n",
       "nnet19  1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te[['nnet12','nnet13','tfidf2','vw2','sgd1','nnet18','nnet17','nnet19']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr()\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = test_mat.sum(axis=0)\n",
    "ixs = np.asarray(mat)[0].argsort()[-5000:][::-1]\n",
    "train_mat = train_mat[:,ixs]\n",
    "test_mat = test_mat[:,ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_pca = np.load(data_dir + 'svd_tfidf300.npy')\n",
    "train_mat_pcat = mat_pca[df_train_index.tolist()]\n",
    "test_mat_pcat = mat_pca[df_test_index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_deep_tree3.npy'))\n",
    "sample_sub.columns = ['uid','dt3']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "X['dt3'] = minmax_scale(np.load(results_dir + 'train_deep_tree3.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.697703\tvalid_1's auc: 0.70875\n",
      "[200]\ttraining's auc: 0.698523\tvalid_1's auc: 0.708756\n",
      "[300]\ttraining's auc: 0.699408\tvalid_1's auc: 0.708692\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.69561\tvalid_1's auc: 0.709791\n",
      "0 0.709791259567\n",
      "prepare test\n",
      "[ 0.5072458   0.50363608  0.50170444 ...,  0.50263838  0.50170444\n",
      "  0.49307455]\n",
      "fold 1\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.699895\tvalid_1's auc: 0.68808\n",
      "[200]\ttraining's auc: 0.700848\tvalid_1's auc: 0.6884\n",
      "[300]\ttraining's auc: 0.701732\tvalid_1's auc: 0.688419\n",
      "[400]\ttraining's auc: 0.702636\tvalid_1's auc: 0.688527\n",
      "[500]\ttraining's auc: 0.70357\tvalid_1's auc: 0.688615\n",
      "[600]\ttraining's auc: 0.704543\tvalid_1's auc: 0.688672\n",
      "[700]\ttraining's auc: 0.705597\tvalid_1's auc: 0.688744\n",
      "[800]\ttraining's auc: 0.706648\tvalid_1's auc: 0.688825\n",
      "[900]\ttraining's auc: 0.707607\tvalid_1's auc: 0.688858\n",
      "[1000]\ttraining's auc: 0.708617\tvalid_1's auc: 0.688885\n",
      "[1100]\ttraining's auc: 0.709595\tvalid_1's auc: 0.688804\n",
      "[1200]\ttraining's auc: 0.710519\tvalid_1's auc: 0.688817\n",
      "Early stopping, best iteration is:\n",
      "[958]\ttraining's auc: 0.70819\tvalid_1's auc: 0.688894\n",
      "1 0.688894111958\n",
      "prepare test\n",
      "[ 0.71543059  0.58918764  0.49497479 ...,  0.52989047  0.50758372\n",
      "  0.2850274 ]\n",
      "fold 2\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.700223\tvalid_1's auc: 0.682318\n",
      "[200]\ttraining's auc: 0.701117\tvalid_1's auc: 0.682959\n",
      "[300]\ttraining's auc: 0.70201\tvalid_1's auc: 0.683313\n",
      "[400]\ttraining's auc: 0.702947\tvalid_1's auc: 0.683493\n",
      "[500]\ttraining's auc: 0.703915\tvalid_1's auc: 0.683617\n",
      "[600]\ttraining's auc: 0.704928\tvalid_1's auc: 0.683686\n",
      "[700]\ttraining's auc: 0.705941\tvalid_1's auc: 0.68366\n",
      "[800]\ttraining's auc: 0.706952\tvalid_1's auc: 0.683676\n",
      "[900]\ttraining's auc: 0.70793\tvalid_1's auc: 0.68362\n",
      "[1000]\ttraining's auc: 0.708848\tvalid_1's auc: 0.683556\n",
      "Early stopping, best iteration is:\n",
      "[739]\ttraining's auc: 0.706345\tvalid_1's auc: 0.683723\n",
      "2 0.683722964907\n",
      "prepare test\n",
      "[ 0.72008581  0.59505784  0.49921874 ...,  0.52331622  0.51014015\n",
      "  0.2721781 ]\n",
      "fold 3\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698458\tvalid_1's auc: 0.701491\n",
      "[200]\ttraining's auc: 0.699167\tvalid_1's auc: 0.701668\n",
      "[300]\ttraining's auc: 0.700045\tvalid_1's auc: 0.701925\n",
      "[400]\ttraining's auc: 0.700945\tvalid_1's auc: 0.702155\n",
      "[500]\ttraining's auc: 0.701866\tvalid_1's auc: 0.70229\n",
      "[600]\ttraining's auc: 0.702773\tvalid_1's auc: 0.702394\n",
      "[700]\ttraining's auc: 0.703812\tvalid_1's auc: 0.702495\n",
      "[800]\ttraining's auc: 0.704866\tvalid_1's auc: 0.70251\n",
      "[ 0.71241142  0.58602256  0.51625323 ...,  0.50951869  0.51369989\n",
      "  0.28560684]\n",
      "fold 5\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.699065\tvalid_1's auc: 0.694947\n",
      "[200]\ttraining's auc: 0.699937\tvalid_1's auc: 0.695201\n",
      "[300]\ttraining's auc: 0.700884\tvalid_1's auc: 0.695213\n",
      "[400]\ttraining's auc: 0.701829\tvalid_1's auc: 0.695176\n",
      "[500]\ttraining's auc: 0.702772\tvalid_1's auc: 0.695082\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's auc: 0.700457\tvalid_1's auc: 0.695246\n",
      "5 0.695245766021\n",
      "prepare test\n",
      "[ 0.66098785  0.56717673  0.49709713 ...,  0.5295496   0.50918642\n",
      "  0.34864486]\n",
      "fold 6\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.699659\tvalid_1's auc: 0.689303\n",
      "[200]\ttraining's auc: 0.700494\tvalid_1's auc: 0.689826\n",
      "[300]\ttraining's auc: 0.701409\tvalid_1's auc: 0.690207\n",
      "[400]\ttraining's auc: 0.702294\tvalid_1's auc: 0.690464\n",
      "[500]\ttraining's auc: 0.703206\tvalid_1's auc: 0.690541\n",
      "[600]\ttraining's auc: 0.704148\tvalid_1's auc: 0.690651\n",
      "[700]\ttraining's auc: 0.70523\tvalid_1's auc: 0.69072\n",
      "[800]\ttraining's auc: 0.70622\tvalid_1's auc: 0.690797\n",
      "[900]\ttraining's auc: 0.707123\tvalid_1's auc: 0.69085\n",
      "[1000]\ttraining's auc: 0.708068\tvalid_1's auc: 0.690877\n",
      "[1100]\ttraining's auc: 0.709022\tvalid_1's auc: 0.690907\n",
      "[1200]\ttraining's auc: 0.709922\tvalid_1's auc: 0.690934\n",
      "[1300]\ttraining's auc: 0.710852\tvalid_1's auc: 0.690892\n",
      "[1400]\ttraining's auc: 0.711786\tvalid_1's auc: 0.690942\n",
      "Early stopping, best iteration is:\n",
      "[1164]\ttraining's auc: 0.709609\tvalid_1's auc: 0.690973\n",
      "6 0.690973248566\n",
      "prepare test\n",
      "[ 0.72690493  0.59294796  0.48502412 ...,  0.51369794  0.4839151\n",
      "  0.24168509]\n",
      "fold 7\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.697953\tvalid_1's auc: 0.704322\n",
      "[200]\ttraining's auc: 0.69881\tvalid_1's auc: 0.704609\n",
      "[300]\ttraining's auc: 0.699716\tvalid_1's auc: 0.704966\n",
      "[400]\ttraining's auc: 0.700604\tvalid_1's auc: 0.705223\n",
      "[500]\ttraining's auc: 0.701535\tvalid_1's auc: 0.70545\n",
      "[600]\ttraining's auc: 0.702529\tvalid_1's auc: 0.70562\n",
      "[700]\ttraining's auc: 0.703602\tvalid_1's auc: 0.705737\n",
      "[800]\ttraining's auc: 0.704663\tvalid_1's auc: 0.705789\n",
      "[900]\ttraining's auc: 0.705616\tvalid_1's auc: 0.705863\n",
      "[1000]\ttraining's auc: 0.706602\tvalid_1's auc: 0.705857\n",
      "[1100]\ttraining's auc: 0.707491\tvalid_1's auc: 0.705875\n",
      "[1200]\ttraining's auc: 0.708405\tvalid_1's auc: 0.705813\n",
      "[1300]\ttraining's auc: 0.709297\tvalid_1's auc: 0.705843\n",
      "Early stopping, best iteration is:\n",
      "[1091]\ttraining's auc: 0.707399\tvalid_1's auc: 0.705895\n",
      "7 0.705895382807\n",
      "prepare test\n",
      "[ 0.71862701  0.59356501  0.50429435 ...,  0.52601518  0.53128908\n",
      "  0.25434207]\n",
      "fold 8\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698944\tvalid_1's auc: 0.693265\n",
      "[200]\ttraining's auc: 0.699869\tvalid_1's auc: 0.693779\n",
      "[300]\ttraining's auc: 0.700736\tvalid_1's auc: 0.694388\n",
      "[400]\ttraining's auc: 0.701643\tvalid_1's auc: 0.69474\n",
      "[500]\ttraining's auc: 0.702561\tvalid_1's auc: 0.694896\n",
      "[600]\ttraining's auc: 0.703547\tvalid_1's auc: 0.695126\n",
      "[700]\ttraining's auc: 0.7046\tvalid_1's auc: 0.695167\n",
      "[800]\ttraining's auc: 0.705645\tvalid_1's auc: 0.695274\n",
      "[900]\ttraining's auc: 0.706571\tvalid_1's auc: 0.695342\n",
      "[1000]\ttraining's auc: 0.707508\tvalid_1's auc: 0.695359\n",
      "[1100]\ttraining's auc: 0.708437\tvalid_1's auc: 0.695412\n",
      "[1200]\ttraining's auc: 0.709325\tvalid_1's auc: 0.695444\n",
      "[1300]\ttraining's auc: 0.710203\tvalid_1's auc: 0.695473\n",
      "[1400]\ttraining's auc: 0.71119\tvalid_1's auc: 0.695421\n",
      "[1500]\ttraining's auc: 0.712156\tvalid_1's auc: 0.695407\n",
      "Early stopping, best iteration is:\n",
      "[1260]\ttraining's auc: 0.70986\tvalid_1's auc: 0.695506\n",
      "8 0.695505659057\n",
      "prepare test\n",
      "[ 0.73172687  0.59800199  0.50754023 ...,  0.51575548  0.50678235\n",
      "  0.23326476]\n",
      "fold 9\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698969\tvalid_1's auc: 0.695846\n",
      "[200]\ttraining's auc: 0.69975\tvalid_1's auc: 0.696262\n",
      "[300]\ttraining's auc: 0.700579\tvalid_1's auc: 0.696615\n",
      "[400]\ttraining's auc: 0.701493\tvalid_1's auc: 0.696931\n",
      "[500]\ttraining's auc: 0.7024\tvalid_1's auc: 0.697111\n",
      "[600]\ttraining's auc: 0.703311\tvalid_1's auc: 0.697272\n",
      "[700]\ttraining's auc: 0.704324\tvalid_1's auc: 0.697428\n",
      "[800]\ttraining's auc: 0.705311\tvalid_1's auc: 0.697483\n",
      "[900]\ttraining's auc: 0.706257\tvalid_1's auc: 0.697605\n",
      "[1000]\ttraining's auc: 0.707147\tvalid_1's auc: 0.697689\n",
      "[1100]\ttraining's auc: 0.708166\tvalid_1's auc: 0.697728\n",
      "[1200]\ttraining's auc: 0.709041\tvalid_1's auc: 0.697743\n",
      "[1300]\ttraining's auc: 0.709951\tvalid_1's auc: 0.697787\n",
      "[1400]\ttraining's auc: 0.710883\tvalid_1's auc: 0.69776\n",
      "[1500]\ttraining's auc: 0.71185\tvalid_1's auc: 0.697797\n",
      "[1600]\ttraining's auc: 0.712787\tvalid_1's auc: 0.697762\n",
      "[1700]\ttraining's auc: 0.713681\tvalid_1's auc: 0.69777\n",
      "Early stopping, best iteration is:\n",
      "[1489]\ttraining's auc: 0.711742\tvalid_1's auc: 0.697819\n",
      "9 0.697818645787\n",
      "prepare test\n",
      "[ 0.7210725   0.60311001  0.49174282 ...,  0.52226198  0.50302452\n",
      "  0.24923617]\n",
      "[0.70979125956684586, 0.6888941119581643, 0.68372296490670192, 0.7025618764576167, 0.69216980624869784, 0.69524576602130495, 0.69097324856608922, 0.70589538280742103, 0.69550565905665995, 0.69781864578662833]\n",
      "0.696257872138 0.00758807780438\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.925506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.536310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.757470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.377797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.381895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.925506\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.536310\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.757470\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.377797\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.381895"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'lgb_e9'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "              'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days','max_f1','max_f2','max_f3',\n",
    "              'svd_description_1','svd_description_2','svd_description_3','svd_description_4','svd_description_5',\n",
    "              'svd_description_6','svd_description_7','svd_description_8','svd_description_9','svd_description_10',\n",
    "              'nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','dt2','dt1',\n",
    "              'nnet3','nnet1','nnet14','nnet15','timp','nnet19','nnet20','nnet21','nnet22',\n",
    "              'ftrl','vw2','sgd1','nnet16','nnet17','nnet18','tfidf2','tfidf1','deeptree',\n",
    "              'vw','ftrl_50',\n",
    "              'nnet4','nnet5','nnet6','nnet7',\n",
    "              'nnet8','lgbmb','most_freq_cat_te'] + ['svd_title_'+str(i+1) for i in range(10)] + ['asvd_'+str(i+1) for i in range(10)]\n",
    "\n",
    "\n",
    "#,'most_freq_cat_te','nnet4','nnet5','nnet6','nnet7','nnet10','nnet11','nnet8',\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 16,\n",
    "    'max_depth' : 4,\n",
    "    #'min_data' : 30,\n",
    "    'lambda_l1' : 1.5,\n",
    "    'lambda_l2' : 30.2,\n",
    "    'bagging_freq' : 2,\n",
    "    'bagging_fraction' : 0.8,\n",
    "    'is_unbalance': True,\n",
    "    'learning_rate': 0.005,\n",
    "    'feature_fraction': 0.5,\n",
    "    'min_data_in_leaf':100,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr().astype(np.bool).astype(np.float32)\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr().astype(np.bool).astype(np.float32)\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols]\n",
    "    X_va = X_va[train_cols]\n",
    "    X_te = X_te[train_cols]\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=0.1)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([     X_tr, sp_train_mat   ]).tocsr() #, train_mat_pcat[train_index] \n",
    "    #print(X_tr.shape)\n",
    "    #print('prepare valid')\n",
    "    X_va = sp.hstack([        X_va, sp_val_mat    ]).tocsr()     #, train_mat_pcat[test_index]\n",
    "    #print('prepare test')\n",
    "    X_te = sp.hstack([       X_te, sp_test_mat    ]).tocsr()      #, test_mat_pcat\n",
    "\n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = lgb.Dataset(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = lgb.Dataset(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "\n",
    "    model = lgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      valid_sets=[tr_data,va_data],\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=300,\n",
    "                      verbose_eval=100)\n",
    "    \n",
    "    yhat = model.predict(X_va, model.best_iteration)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict(X_te, model.best_iteration)\n",
    "    print(ytst)\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,tr_data,va_data, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('lgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))    \n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70935540026201749, 0.6890247976060333, 0.68378549419421653, 0.70274745749668643, 0.69224803818126024, 0.6952706514919984, 0.69102802236127214, 0.70608650004544038, 0.69540192462882777, 0.69774039503128993]\n",
      "0.69626886813 0.00751979706633\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.70919731520431251, 0.6891197652556077, 0.68374056085963286, 0.70273171958127179, 0.69249429319641487, \n",
    " 0.69528635958606888, 0.69101162136385164, 0.70591099061834595, 0.69552760688284754, 0.69797274585547964] \n",
    "0.69629929784 0.00745819458099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgbe7</th>\n",
       "      <th>lgbe8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgbe7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbe8</th>\n",
       "      <td>0.996499</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          xgbe7     lgbe8\n",
       "xgbe7  1.000000  0.996499\n",
       "lgbe8  0.996499  1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_xgb_e7.npy'))\n",
    "sample_sub.columns = ['uid','xgbe7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_lgb_e8.npy'))\n",
    "sample_sub.columns = ['uid','lgbe8']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "x_te[['xgbe7','lgbe8']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.900125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.509793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.711866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.377108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.389375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.900125\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.509793\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.711866\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.377108\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.389375"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'blend'\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = (x_te.xgbe7 + x_te.lgbe8 + x_te.lgbe7 + x_te.lgbf7)*0.25\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgbe7</th>\n",
       "      <th>lgbe8</th>\n",
       "      <th>lgbe7</th>\n",
       "      <th>lgbf7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgbe7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996499</td>\n",
       "      <td>0.991828</td>\n",
       "      <td>0.997217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbe8</th>\n",
       "      <td>0.996499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992490</td>\n",
       "      <td>0.998762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbe7</th>\n",
       "      <td>0.991828</td>\n",
       "      <td>0.992490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbf7</th>\n",
       "      <td>0.997217</td>\n",
       "      <td>0.998762</td>\n",
       "      <td>0.994776</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          xgbe7     lgbe8     lgbe7     lgbf7\n",
       "xgbe7  1.000000  0.996499  0.991828  0.997217\n",
       "lgbe8  0.996499  1.000000  0.992490  0.998762\n",
       "lgbe7  0.991828  0.992490  1.000000  0.994776\n",
       "lgbf7  0.997217  0.998762  0.994776  1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_lgb_e7.npy'))\n",
    "sample_sub.columns = ['uid','lgbe7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_lgb_f7.npy'))\n",
    "sample_sub.columns = ['uid','lgbf7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "\n",
    "x_te[['xgbe7','lgbe8','lgbe7','lgbf7']].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.70584483206119242, 0.68852257053257726, 0.68120543644470821, 0.69943609670565343, 0.68975482586128489, \n",
    " 0.69245894848299694, 0.68597921405405382, 0.70357452572825019, 0.69242992054470409, 0.69611483413886988]\n",
    "0.693532120455 0.00738040471586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688782\tvalid_1's auc: 0.680434\n",
      "[200]\ttraining's auc: 0.689369\tvalid_1's auc: 0.680718\n",
      "[300]\ttraining's auc: 0.689748\tvalid_1's auc: 0.681187\n",
      "[400]\ttraining's auc: 0.690082\tvalid_1's auc: 0.681541\n",
      "[500]\ttraining's auc: 0.690511\tvalid_1's auc: 0.681837\n",
      "[600]\ttraining's auc: 0.690828\tvalid_1's auc: 0.682075\n",
      "[700]\ttraining's auc: 0.691088\tvalid_1's auc: 0.682264\n",
      "[800]\ttraining's auc: 0.691347\tvalid_1's auc: 0.682386\n",
      "[900]\ttraining's auc: 0.691583\tvalid_1's auc: 0.682475\n",
      "[1000]\ttraining's auc: 0.691845\tvalid_1's auc: 0.682563\n",
      "[1100]\ttraining's auc: 0.692089\tvalid_1's auc: 0.682652\n",
      "[1200]\ttraining's auc: 0.692325\tvalid_1's auc: 0.68269\n",
      "[1300]\ttraining's auc: 0.692538\tvalid_1's auc: 0.682712\n",
      "[1400]\ttraining's auc: 0.692752\tvalid_1's auc: 0.682727\n",
      "[1500]\ttraining's auc: 0.692951\tvalid_1's auc: 0.68275\n",
      "[1600]\ttraining's auc: 0.693164\tvalid_1's auc: 0.682802\n",
      "[1700]\ttraining's auc: 0.693368\tvalid_1's auc: 0.682826\n",
      "[1800]\ttraining's auc: 0.693588\tvalid_1's auc: 0.682832\n",
      "[1900]\ttraining's auc: 0.693813\tvalid_1's auc: 0.682817\n",
      "[2000]\ttraining's auc: 0.694056\tvalid_1's auc: 0.682845\n",
      "[2100]\ttraining's auc: 0.694308\tvalid_1's auc: 0.682856\n",
      "[2200]\ttraining's auc: 0.694573\tvalid_1's auc: 0.682807\n",
      "[2300]\ttraining's auc: 0.694823\tvalid_1's auc: 0.682801\n",
      "[2400]\ttraining's auc: 0.69508\tvalid_1's auc: 0.682783\n",
      "Early stopping, best iteration is:\n",
      "[2100]\ttraining's auc: 0.694308\tvalid_1's auc: 0.682856\n",
      "0 0.682855654011\n",
      "prepare test\n",
      "fold 1\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.68803\tvalid_1's auc: 0.686672\n",
      "[200]\ttraining's auc: 0.68849\tvalid_1's auc: 0.686788\n",
      "[300]\ttraining's auc: 0.688778\tvalid_1's auc: 0.687\n",
      "[400]\ttraining's auc: 0.689064\tvalid_1's auc: 0.687205\n",
      "[500]\ttraining's auc: 0.689408\tvalid_1's auc: 0.687485\n",
      "[600]\ttraining's auc: 0.689794\tvalid_1's auc: 0.687926\n",
      "[700]\ttraining's auc: 0.690118\tvalid_1's auc: 0.688263\n",
      "[800]\ttraining's auc: 0.690398\tvalid_1's auc: 0.688464\n",
      "[900]\ttraining's auc: 0.690656\tvalid_1's auc: 0.688628\n",
      "[1000]\ttraining's auc: 0.690914\tvalid_1's auc: 0.688776\n",
      "[1100]\ttraining's auc: 0.691179\tvalid_1's auc: 0.688871\n",
      "[1200]\ttraining's auc: 0.691431\tvalid_1's auc: 0.688967\n",
      "[1300]\ttraining's auc: 0.691684\tvalid_1's auc: 0.689033\n",
      "[1400]\ttraining's auc: 0.691929\tvalid_1's auc: 0.689101\n",
      "[1500]\ttraining's auc: 0.692172\tvalid_1's auc: 0.689192\n",
      "[1600]\ttraining's auc: 0.692409\tvalid_1's auc: 0.689297\n",
      "[1700]\ttraining's auc: 0.692653\tvalid_1's auc: 0.689369\n",
      "[1800]\ttraining's auc: 0.692899\tvalid_1's auc: 0.689434\n",
      "[1900]\ttraining's auc: 0.693131\tvalid_1's auc: 0.689488\n",
      "[2000]\ttraining's auc: 0.693359\tvalid_1's auc: 0.68953\n",
      "[2100]\ttraining's auc: 0.693598\tvalid_1's auc: 0.689583\n",
      "[2200]\ttraining's auc: 0.693838\tvalid_1's auc: 0.689637\n",
      "[2300]\ttraining's auc: 0.694105\tvalid_1's auc: 0.689704\n",
      "[2400]\ttraining's auc: 0.69436\tvalid_1's auc: 0.689759\n",
      "[2500]\ttraining's auc: 0.694621\tvalid_1's auc: 0.689814\n",
      "[2600]\ttraining's auc: 0.694878\tvalid_1's auc: 0.689868\n",
      "[2700]\ttraining's auc: 0.695137\tvalid_1's auc: 0.689939\n",
      "[2800]\ttraining's auc: 0.69539\tvalid_1's auc: 0.690003\n",
      "[2900]\ttraining's auc: 0.695635\tvalid_1's auc: 0.690059\n",
      "[3000]\ttraining's auc: 0.695883\tvalid_1's auc: 0.690128\n",
      "[3100]\ttraining's auc: 0.696144\tvalid_1's auc: 0.690181\n",
      "[3200]\ttraining's auc: 0.696426\tvalid_1's auc: 0.690234\n",
      "[3300]\ttraining's auc: 0.696718\tvalid_1's auc: 0.690284\n",
      "[3400]\ttraining's auc: 0.697\tvalid_1's auc: 0.690289\n",
      "[3500]\ttraining's auc: 0.697291\tvalid_1's auc: 0.690315\n",
      "[3600]\ttraining's auc: 0.69759\tvalid_1's auc: 0.69033\n",
      "[3700]\ttraining's auc: 0.697894\tvalid_1's auc: 0.690339\n",
      "[3800]\ttraining's auc: 0.698179\tvalid_1's auc: 0.690344\n",
      "[3900]\ttraining's auc: 0.698455\tvalid_1's auc: 0.69036\n",
      "[4000]\ttraining's auc: 0.698714\tvalid_1's auc: 0.6904\n",
      "[4100]\ttraining's auc: 0.698962\tvalid_1's auc: 0.690442\n",
      "[4200]\ttraining's auc: 0.69923\tvalid_1's auc: 0.690458\n",
      "[4300]\ttraining's auc: 0.699482\tvalid_1's auc: 0.690461\n",
      "[4400]\ttraining's auc: 0.699732\tvalid_1's auc: 0.690457\n",
      "[4500]\ttraining's auc: 0.699988\tvalid_1's auc: 0.690467\n",
      "[4600]\ttraining's auc: 0.700237\tvalid_1's auc: 0.690462\n",
      "[4700]\ttraining's auc: 0.700478\tvalid_1's auc: 0.690453\n",
      "[4800]\ttraining's auc: 0.700723\tvalid_1's auc: 0.690439\n",
      "Early stopping, best iteration is:\n",
      "[4512]\ttraining's auc: 0.700018\tvalid_1's auc: 0.690473\n",
      "1 0.690473482077\n",
      "prepare test\n",
      "fold 2\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.687897\tvalid_1's auc: 0.687679\n",
      "[200]\ttraining's auc: 0.688505\tvalid_1's auc: 0.687971\n",
      "[300]\ttraining's auc: 0.688897\tvalid_1's auc: 0.688232\n",
      "[400]\ttraining's auc: 0.68925\tvalid_1's auc: 0.68838\n",
      "[500]\ttraining's auc: 0.689585\tvalid_1's auc: 0.68848\n",
      "[600]\ttraining's auc: 0.689937\tvalid_1's auc: 0.688536\n",
      "[700]\ttraining's auc: 0.690246\tvalid_1's auc: 0.688574\n",
      "[800]\ttraining's auc: 0.69053\tvalid_1's auc: 0.688574\n",
      "[900]\ttraining's auc: 0.690794\tvalid_1's auc: 0.688556\n",
      "[1000]\ttraining's auc: 0.691036\tvalid_1's auc: 0.688521\n",
      "Early stopping, best iteration is:\n",
      "[767]\ttraining's auc: 0.690435\tvalid_1's auc: 0.688596\n",
      "2 0.688595677634\n",
      "prepare test\n",
      "fold 3\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688611\tvalid_1's auc: 0.681558\n",
      "[200]\ttraining's auc: 0.688942\tvalid_1's auc: 0.681723\n",
      "[300]\ttraining's auc: 0.689405\tvalid_1's auc: 0.681981\n",
      "[400]\ttraining's auc: 0.689836\tvalid_1's auc: 0.682159\n",
      "[500]\ttraining's auc: 0.69026\tvalid_1's auc: 0.68261\n",
      "[600]\ttraining's auc: 0.690625\tvalid_1's auc: 0.683\n",
      "[700]\ttraining's auc: 0.690936\tvalid_1's auc: 0.683255\n",
      "[800]\ttraining's auc: 0.691192\tvalid_1's auc: 0.683397\n",
      "[900]\ttraining's auc: 0.691395\tvalid_1's auc: 0.683551\n",
      "[1000]\ttraining's auc: 0.691634\tvalid_1's auc: 0.683672\n",
      "[1100]\ttraining's auc: 0.691877\tvalid_1's auc: 0.683843\n",
      "[1200]\ttraining's auc: 0.692116\tvalid_1's auc: 0.683963\n",
      "[1300]\ttraining's auc: 0.692337\tvalid_1's auc: 0.684054\n",
      "[1400]\ttraining's auc: 0.692579\tvalid_1's auc: 0.684135\n",
      "[1500]\ttraining's auc: 0.692796\tvalid_1's auc: 0.68423\n",
      "[1600]\ttraining's auc: 0.693033\tvalid_1's auc: 0.68428\n",
      "[1700]\ttraining's auc: 0.693263\tvalid_1's auc: 0.684328\n",
      "[1800]\ttraining's auc: 0.693501\tvalid_1's auc: 0.684393\n",
      "[1900]\ttraining's auc: 0.693769\tvalid_1's auc: 0.684411\n",
      "[2000]\ttraining's auc: 0.694031\tvalid_1's auc: 0.684399\n",
      "[2100]\ttraining's auc: 0.694289\tvalid_1's auc: 0.684376\n",
      "[2200]\ttraining's auc: 0.694553\tvalid_1's auc: 0.684387\n",
      "Early stopping, best iteration is:\n",
      "[1937]\ttraining's auc: 0.693863\tvalid_1's auc: 0.68442\n",
      "3 0.684419555855\n",
      "prepare test\n",
      "fold 4\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688906\tvalid_1's auc: 0.682953\n",
      "[200]\ttraining's auc: 0.689386\tvalid_1's auc: 0.682946\n",
      "[300]\ttraining's auc: 0.689657\tvalid_1's auc: 0.683262\n",
      "[400]\ttraining's auc: 0.689896\tvalid_1's auc: 0.683462\n",
      "[500]\ttraining's auc: 0.69017\tvalid_1's auc: 0.683643\n",
      "[600]\ttraining's auc: 0.690428\tvalid_1's auc: 0.683747\n",
      "[700]\ttraining's auc: 0.690663\tvalid_1's auc: 0.683901\n",
      "[800]\ttraining's auc: 0.690916\tvalid_1's auc: 0.684049\n",
      "[900]\ttraining's auc: 0.691138\tvalid_1's auc: 0.684172\n",
      "[1000]\ttraining's auc: 0.691358\tvalid_1's auc: 0.684302\n",
      "[1100]\ttraining's auc: 0.691595\tvalid_1's auc: 0.684494\n",
      "[1200]\ttraining's auc: 0.691813\tvalid_1's auc: 0.684628\n",
      "[1300]\ttraining's auc: 0.692018\tvalid_1's auc: 0.684754\n",
      "[1400]\ttraining's auc: 0.692237\tvalid_1's auc: 0.684893\n",
      "[1500]\ttraining's auc: 0.692458\tvalid_1's auc: 0.685019\n",
      "[1600]\ttraining's auc: 0.692668\tvalid_1's auc: 0.685113\n",
      "[1700]\ttraining's auc: 0.692893\tvalid_1's auc: 0.685188\n",
      "[1800]\ttraining's auc: 0.693119\tvalid_1's auc: 0.685269\n",
      "[1900]\ttraining's auc: 0.693338\tvalid_1's auc: 0.685379\n",
      "[2000]\ttraining's auc: 0.693583\tvalid_1's auc: 0.685446\n",
      "[2100]\ttraining's auc: 0.693811\tvalid_1's auc: 0.685476\n",
      "[2200]\ttraining's auc: 0.694037\tvalid_1's auc: 0.685507\n",
      "[2300]\ttraining's auc: 0.694295\tvalid_1's auc: 0.685536\n",
      "[2400]\ttraining's auc: 0.694554\tvalid_1's auc: 0.685556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500]\ttraining's auc: 0.694809\tvalid_1's auc: 0.685585\n",
      "[2600]\ttraining's auc: 0.695072\tvalid_1's auc: 0.685611\n",
      "[2700]\ttraining's auc: 0.695337\tvalid_1's auc: 0.685642\n",
      "[2800]\ttraining's auc: 0.6956\tvalid_1's auc: 0.685645\n",
      "[2900]\ttraining's auc: 0.695853\tvalid_1's auc: 0.685642\n",
      "[3000]\ttraining's auc: 0.696117\tvalid_1's auc: 0.685645\n",
      "[3100]\ttraining's auc: 0.696391\tvalid_1's auc: 0.685631\n",
      "[3200]\ttraining's auc: 0.696669\tvalid_1's auc: 0.685628\n",
      "Early stopping, best iteration is:\n",
      "[2959]\ttraining's auc: 0.696009\tvalid_1's auc: 0.68565\n",
      "4 0.685650472164\n",
      "prepare test\n",
      "fold 5\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.68827\tvalid_1's auc: 0.68582\n",
      "[200]\ttraining's auc: 0.688599\tvalid_1's auc: 0.686177\n",
      "[300]\ttraining's auc: 0.689037\tvalid_1's auc: 0.686457\n",
      "[400]\ttraining's auc: 0.689382\tvalid_1's auc: 0.686671\n",
      "[500]\ttraining's auc: 0.689775\tvalid_1's auc: 0.68678\n",
      "[600]\ttraining's auc: 0.690088\tvalid_1's auc: 0.68699\n",
      "[700]\ttraining's auc: 0.690347\tvalid_1's auc: 0.687086\n",
      "[800]\ttraining's auc: 0.690576\tvalid_1's auc: 0.687199\n",
      "[900]\ttraining's auc: 0.690823\tvalid_1's auc: 0.687286\n",
      "[1000]\ttraining's auc: 0.691055\tvalid_1's auc: 0.687357\n",
      "[1100]\ttraining's auc: 0.691272\tvalid_1's auc: 0.68746\n",
      "[1200]\ttraining's auc: 0.691487\tvalid_1's auc: 0.687606\n",
      "[1300]\ttraining's auc: 0.6917\tvalid_1's auc: 0.687699\n",
      "[1400]\ttraining's auc: 0.691914\tvalid_1's auc: 0.687838\n",
      "[1500]\ttraining's auc: 0.692128\tvalid_1's auc: 0.687971\n",
      "[1600]\ttraining's auc: 0.692348\tvalid_1's auc: 0.688065\n",
      "[1700]\ttraining's auc: 0.692569\tvalid_1's auc: 0.688149\n",
      "[1800]\ttraining's auc: 0.692811\tvalid_1's auc: 0.688252\n",
      "[1900]\ttraining's auc: 0.693063\tvalid_1's auc: 0.688336\n",
      "[2000]\ttraining's auc: 0.693301\tvalid_1's auc: 0.688422\n",
      "[2100]\ttraining's auc: 0.693537\tvalid_1's auc: 0.688522\n",
      "[2200]\ttraining's auc: 0.693767\tvalid_1's auc: 0.688568\n",
      "[2300]\ttraining's auc: 0.694006\tvalid_1's auc: 0.688608\n",
      "[2400]\ttraining's auc: 0.694247\tvalid_1's auc: 0.688667\n",
      "[2500]\ttraining's auc: 0.694487\tvalid_1's auc: 0.688703\n",
      "[2600]\ttraining's auc: 0.694707\tvalid_1's auc: 0.688727\n",
      "[2700]\ttraining's auc: 0.694932\tvalid_1's auc: 0.68874\n",
      "[2800]\ttraining's auc: 0.695158\tvalid_1's auc: 0.688781\n",
      "[2900]\ttraining's auc: 0.695401\tvalid_1's auc: 0.688828\n",
      "[3000]\ttraining's auc: 0.695628\tvalid_1's auc: 0.688872\n",
      "[3100]\ttraining's auc: 0.695883\tvalid_1's auc: 0.688922\n",
      "[3200]\ttraining's auc: 0.69614\tvalid_1's auc: 0.68896\n",
      "[3300]\ttraining's auc: 0.696416\tvalid_1's auc: 0.688976\n",
      "[3400]\ttraining's auc: 0.696699\tvalid_1's auc: 0.689017\n",
      "[3500]\ttraining's auc: 0.696993\tvalid_1's auc: 0.689052\n",
      "[3600]\ttraining's auc: 0.697305\tvalid_1's auc: 0.689092\n",
      "[3700]\ttraining's auc: 0.697616\tvalid_1's auc: 0.68912\n",
      "[3800]\ttraining's auc: 0.697928\tvalid_1's auc: 0.68915\n",
      "[3900]\ttraining's auc: 0.69823\tvalid_1's auc: 0.689183\n",
      "[4000]\ttraining's auc: 0.698497\tvalid_1's auc: 0.689196\n",
      "[4100]\ttraining's auc: 0.698768\tvalid_1's auc: 0.689233\n",
      "[4200]\ttraining's auc: 0.699041\tvalid_1's auc: 0.689245\n",
      "[4300]\ttraining's auc: 0.699319\tvalid_1's auc: 0.689242\n",
      "[4400]\ttraining's auc: 0.699607\tvalid_1's auc: 0.689246\n",
      "[4500]\ttraining's auc: 0.699903\tvalid_1's auc: 0.689249\n",
      "[4600]\ttraining's auc: 0.700185\tvalid_1's auc: 0.689236\n",
      "[4700]\ttraining's auc: 0.700458\tvalid_1's auc: 0.689207\n",
      "Early stopping, best iteration is:\n",
      "[4474]\ttraining's auc: 0.699823\tvalid_1's auc: 0.689256\n",
      "5 0.689256016897\n",
      "prepare test\n",
      "fold 6\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688559\tvalid_1's auc: 0.685646\n",
      "[200]\ttraining's auc: 0.689041\tvalid_1's auc: 0.685962\n",
      "[300]\ttraining's auc: 0.689383\tvalid_1's auc: 0.686052\n",
      "[400]\ttraining's auc: 0.689712\tvalid_1's auc: 0.686178\n",
      "[500]\ttraining's auc: 0.690023\tvalid_1's auc: 0.686434\n",
      "[600]\ttraining's auc: 0.690261\tvalid_1's auc: 0.686589\n",
      "[700]\ttraining's auc: 0.690481\tvalid_1's auc: 0.686681\n",
      "[800]\ttraining's auc: 0.690711\tvalid_1's auc: 0.686873\n",
      "[900]\ttraining's auc: 0.690946\tvalid_1's auc: 0.687074\n",
      "[1000]\ttraining's auc: 0.69117\tvalid_1's auc: 0.687195\n",
      "[1100]\ttraining's auc: 0.691406\tvalid_1's auc: 0.687293\n",
      "[1200]\ttraining's auc: 0.691651\tvalid_1's auc: 0.687404\n",
      "[1300]\ttraining's auc: 0.691886\tvalid_1's auc: 0.687507\n",
      "[1400]\ttraining's auc: 0.692107\tvalid_1's auc: 0.687546\n",
      "[1500]\ttraining's auc: 0.692334\tvalid_1's auc: 0.68758\n",
      "[1600]\ttraining's auc: 0.692578\tvalid_1's auc: 0.687567\n",
      "[1700]\ttraining's auc: 0.692823\tvalid_1's auc: 0.687525\n",
      "[1800]\ttraining's auc: 0.693079\tvalid_1's auc: 0.687533\n",
      "Early stopping, best iteration is:\n",
      "[1522]\ttraining's auc: 0.692392\tvalid_1's auc: 0.687584\n",
      "6 0.687583903933\n",
      "prepare test\n",
      "fold 7\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688485\tvalid_1's auc: 0.683377\n",
      "[200]\ttraining's auc: 0.688764\tvalid_1's auc: 0.683258\n",
      "[300]\ttraining's auc: 0.689203\tvalid_1's auc: 0.683403\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's auc: 0.688508\tvalid_1's auc: 0.684045\n",
      "7 0.684044639416\n",
      "prepare test\n",
      "fold 8\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688397\tvalid_1's auc: 0.691516\n",
      "[200]\ttraining's auc: 0.688981\tvalid_1's auc: 0.69175\n",
      "[300]\ttraining's auc: 0.689236\tvalid_1's auc: 0.691751\n",
      "[400]\ttraining's auc: 0.689467\tvalid_1's auc: 0.691801\n",
      "[500]\ttraining's auc: 0.689665\tvalid_1's auc: 0.691771\n",
      "[600]\ttraining's auc: 0.689858\tvalid_1's auc: 0.69174\n",
      "[700]\ttraining's auc: 0.69011\tvalid_1's auc: 0.691741\n",
      "Early stopping, best iteration is:\n",
      "[402]\ttraining's auc: 0.689478\tvalid_1's auc: 0.691813\n",
      "8 0.691812957391\n",
      "prepare test\n",
      "fold 9\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688583\tvalid_1's auc: 0.684301\n",
      "[200]\ttraining's auc: 0.688993\tvalid_1's auc: 0.684234\n",
      "[300]\ttraining's auc: 0.689373\tvalid_1's auc: 0.68414\n",
      "[400]\ttraining's auc: 0.689836\tvalid_1's auc: 0.684309\n",
      "[500]\ttraining's auc: 0.690274\tvalid_1's auc: 0.68469\n",
      "[600]\ttraining's auc: 0.690595\tvalid_1's auc: 0.684881\n",
      "[700]\ttraining's auc: 0.690874\tvalid_1's auc: 0.685048\n",
      "[800]\ttraining's auc: 0.691094\tvalid_1's auc: 0.685174\n",
      "[900]\ttraining's auc: 0.691297\tvalid_1's auc: 0.685261\n",
      "[1000]\ttraining's auc: 0.691516\tvalid_1's auc: 0.685333\n",
      "[1100]\ttraining's auc: 0.691741\tvalid_1's auc: 0.685351\n",
      "[1200]\ttraining's auc: 0.691971\tvalid_1's auc: 0.685374\n",
      "[1300]\ttraining's auc: 0.692194\tvalid_1's auc: 0.685396\n",
      "[1400]\ttraining's auc: 0.692407\tvalid_1's auc: 0.685437\n",
      "[1500]\ttraining's auc: 0.692627\tvalid_1's auc: 0.685481\n",
      "[1600]\ttraining's auc: 0.692841\tvalid_1's auc: 0.685481\n",
      "[1700]\ttraining's auc: 0.693058\tvalid_1's auc: 0.685471\n",
      "[1800]\ttraining's auc: 0.693262\tvalid_1's auc: 0.685474\n",
      "Early stopping, best iteration is:\n",
      "[1511]\ttraining's auc: 0.692649\tvalid_1's auc: 0.685487\n",
      "9 0.685486562807\n",
      "prepare test\n",
      "[0.68285565401054749, 0.69047348207724357, 0.68859567763395668, 0.6844195558552767, 0.68565047216409147, 0.68925601689664151, 0.68758390393251834, 0.68404463941572935, 0.69181295739052984, 0.68548656280730147]\n",
      "0.687017892218 0.00282636423315\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.727645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.578061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.673377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.476768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.444018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.727645\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.578061\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.673377\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.476768\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.444018"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'lgb_b2'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "              'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days','max_f1','max_f2','max_f3',\n",
    "              'svd_description_1','svd_description_2','svd_description_3','svd_description_4','svd_description_5',\n",
    "              'svd_description_6','svd_description_7','svd_description_8','svd_description_9','svd_description_10',\n",
    "              'nnet4','nnet5','nnet6','nnet7','nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','nnet3','nnet1',\n",
    "              'nnet8','lgbmb','most_freq_cat_te'] + ['svd_title_'+str(i+1) for i in range(10)]\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 16,\n",
    "    'max_depth' : 4,\n",
    "    #'min_data' : 100,\n",
    "    #'lambda_l2' : 15.5,\n",
    "    'min_sum_hessian_in_leaf' : 0.2,\n",
    "    'lambda_l1' : 6.2,\n",
    "    'is_unbalance': True,\n",
    "    'learning_rate': 0.001,\n",
    "    'feature_fraction': 0.7,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=13)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr()\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr()\n",
    "mat_spca = np.load(data_dir + 'spca_dim100.npy')\n",
    "train_mat_spca = mat_spca[df_train_index.tolist()]\n",
    "test_mat_spca = mat_spca[df_test_index.tolist()]\n",
    "del mat_spca\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols]\n",
    "    X_va = X_va[train_cols]\n",
    "    X_te = X_te[train_cols]\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=0.1)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([\n",
    "        X_tr, sp_train_mat, train_mat_spca[train_index]\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    print('prepare valid')\n",
    "    X_va = sp.hstack([\n",
    "        X_va, sp_val_mat, train_mat_spca[test_index]\n",
    "    ]).tocsr()    \n",
    "    print('prepare test')\n",
    "    X_te = sp.hstack([\n",
    "        X_te, sp_test_mat, test_mat_spca\n",
    "    ]).tocsr()     \n",
    "\n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = lgb.Dataset(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = lgb.Dataset(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "\n",
    "    model = lgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      valid_sets=[tr_data,va_data],\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=300,\n",
    "                      verbose_eval=100)\n",
    "    \n",
    "    yhat = model.predict(X_va, model.best_iteration)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict(X_te, model.best_iteration)\n",
    "    y_pred += ytst*0.1\n",
    "    \n",
    "    del X_tr,X_va,tr_data,va_data, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('lgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))    \n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.727645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.578061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.673377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.476768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.444018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.727645\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.578061\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.673377\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.476768\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.444018"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_xgb_b1.npy'))\n",
    "sample_sub.columns = ['uid','xgb_sess4']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_lgb_b1.npy'))\n",
    "sample_sub.columns = ['uid','lgb_sess6']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_lgb_b2.npy'))\n",
    "sample_sub.columns = ['uid','lgb_sess7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['xgb_sess4'] = minmax_scale(np.load(results_dir + 'train_xgb_b1.npy'))\n",
    "X['lgb_sess6'] = minmax_scale(np.load(results_dir + 'train_lgb_b1.npy'))\n",
    "X['lgb_sess7'] = minmax_scale(np.load(results_dir + 'train_lgb_b2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnet12 0.67274153339\n",
      "nnet13 0.67264163497\n",
      "lgbm1 0.682562603224\n",
      "lgbmb 0.66792997872\n",
      "nnet1 0.623698784984\n",
      "nnet3 0.574520607881\n",
      "xgb_single 0.660175309563\n"
     ]
    }
   ],
   "source": [
    "meta = ['nnet12','nnet13','lgbm1','lgbmb','nnet1','nnet3','xgb_single'] #\n",
    "for f in meta:\n",
    "    print(f,roc_auc_score(X.target, X[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "      <th>lgbm1</th>\n",
       "      <th>lgbmb</th>\n",
       "      <th>nnet1</th>\n",
       "      <th>nnet3</th>\n",
       "      <th>xgb_single</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815829</td>\n",
       "      <td>0.736898</td>\n",
       "      <td>0.844497</td>\n",
       "      <td>0.612611</td>\n",
       "      <td>0.445006</td>\n",
       "      <td>0.825464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.815829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853592</td>\n",
       "      <td>0.710129</td>\n",
       "      <td>0.463131</td>\n",
       "      <td>0.288266</td>\n",
       "      <td>0.687111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm1</th>\n",
       "      <td>0.736898</td>\n",
       "      <td>0.853592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.752149</td>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.698921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbmb</th>\n",
       "      <td>0.844497</td>\n",
       "      <td>0.710129</td>\n",
       "      <td>0.752149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.668780</td>\n",
       "      <td>0.501951</td>\n",
       "      <td>0.907113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet1</th>\n",
       "      <td>0.612611</td>\n",
       "      <td>0.463131</td>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.668780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.487426</td>\n",
       "      <td>0.661838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet3</th>\n",
       "      <td>0.445006</td>\n",
       "      <td>0.288266</td>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.501951</td>\n",
       "      <td>0.487426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_single</th>\n",
       "      <td>0.825464</td>\n",
       "      <td>0.687111</td>\n",
       "      <td>0.698921</td>\n",
       "      <td>0.907113</td>\n",
       "      <td>0.661838</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nnet12    nnet13     lgbm1     lgbmb     nnet1     nnet3  \\\n",
       "nnet12      1.000000  0.815829  0.736898  0.844497  0.612611  0.445006   \n",
       "nnet13      0.815829  1.000000  0.853592  0.710129  0.463131  0.288266   \n",
       "lgbm1       0.736898  0.853592  1.000000  0.752149  0.494446  0.295207   \n",
       "lgbmb       0.844497  0.710129  0.752149  1.000000  0.668780  0.501951   \n",
       "nnet1       0.612611  0.463131  0.494446  0.668780  1.000000  0.487426   \n",
       "nnet3       0.445006  0.288266  0.295207  0.501951  0.487426  1.000000   \n",
       "xgb_single  0.825464  0.687111  0.698921  0.907113  0.661838  0.490909   \n",
       "\n",
       "            xgb_single  \n",
       "nnet12        0.825464  \n",
       "nnet13        0.687111  \n",
       "lgbm1         0.698921  \n",
       "lgbmb         0.907113  \n",
       "nnet1         0.661838  \n",
       "nnet3         0.490909  \n",
       "xgb_single    1.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[meta].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "      <th>lgbm1</th>\n",
       "      <th>lgbmb</th>\n",
       "      <th>nnet1</th>\n",
       "      <th>nnet3</th>\n",
       "      <th>xgb_single</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>0.793141</td>\n",
       "      <td>0.837708</td>\n",
       "      <td>0.643211</td>\n",
       "      <td>0.436315</td>\n",
       "      <td>0.821308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.872521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871866</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.511538</td>\n",
       "      <td>0.316164</td>\n",
       "      <td>0.681750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm1</th>\n",
       "      <td>0.793141</td>\n",
       "      <td>0.871866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778259</td>\n",
       "      <td>0.536187</td>\n",
       "      <td>0.342257</td>\n",
       "      <td>0.726940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbmb</th>\n",
       "      <td>0.837708</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.778259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>0.466649</td>\n",
       "      <td>0.933161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet1</th>\n",
       "      <td>0.643211</td>\n",
       "      <td>0.511538</td>\n",
       "      <td>0.536187</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.639809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet3</th>\n",
       "      <td>0.436315</td>\n",
       "      <td>0.316164</td>\n",
       "      <td>0.342257</td>\n",
       "      <td>0.466649</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_single</th>\n",
       "      <td>0.821308</td>\n",
       "      <td>0.681750</td>\n",
       "      <td>0.726940</td>\n",
       "      <td>0.933161</td>\n",
       "      <td>0.639809</td>\n",
       "      <td>0.456628</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nnet12    nnet13     lgbm1     lgbmb     nnet1     nnet3  \\\n",
       "nnet12      1.000000  0.872521  0.793141  0.837708  0.643211  0.436315   \n",
       "nnet13      0.872521  1.000000  0.871866  0.713111  0.511538  0.316164   \n",
       "lgbm1       0.793141  0.871866  1.000000  0.778259  0.536187  0.342257   \n",
       "lgbmb       0.837708  0.713111  0.778259  1.000000  0.641589  0.466649   \n",
       "nnet1       0.643211  0.511538  0.536187  0.641589  1.000000  0.488963   \n",
       "nnet3       0.436315  0.316164  0.342257  0.466649  0.488963  1.000000   \n",
       "xgb_single  0.821308  0.681750  0.726940  0.933161  0.639809  0.456628   \n",
       "\n",
       "            xgb_single  \n",
       "nnet12        0.821308  \n",
       "nnet13        0.681750  \n",
       "lgbm1         0.726940  \n",
       "lgbmb         0.933161  \n",
       "nnet1         0.639809  \n",
       "nnet3         0.456628  \n",
       "xgb_single    1.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te[meta].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "0 0.698668673061\n",
      "prepare test\n",
      "[ 0.14689715  0.14794698  0.10039477 ...,  0.07865642  0.0795201\n",
      "  0.04324515]\n",
      "fold 1\n",
      "1 0.680282222997\n",
      "prepare test\n",
      "[ 0.14906741  0.14968694  0.10206368 ...,  0.08114234  0.08153055\n",
      "  0.04605674]\n",
      "fold 2\n",
      "2 0.672018064269\n",
      "prepare test\n",
      "[ 0.14804478  0.14921921  0.10076937 ...,  0.07930363  0.07985425\n",
      "  0.04391066]\n",
      "fold 3\n",
      "3 0.688242992587\n",
      "prepare test\n",
      "[ 0.14820545  0.1487872   0.10010541 ...,  0.07902909  0.07945207\n",
      "  0.04392936]\n",
      "fold 4\n",
      "4 0.681608416947\n",
      "prepare test\n",
      "[ 0.14675651  0.14805475  0.10015005 ...,  0.07874264  0.07952918\n",
      "  0.0439385 ]\n",
      "fold 5\n",
      "5 0.683953307708\n",
      "prepare test\n",
      "[ 0.14593103  0.14723824  0.09932246 ...,  0.07791339  0.07879705\n",
      "  0.04330436]\n",
      "fold 6\n",
      "6 0.678530035578\n",
      "prepare test\n",
      "[ 0.15262749  0.15368207  0.10530478 ...,  0.08421273  0.08437358\n",
      "  0.04908429]\n",
      "fold 7\n",
      "7 0.694977085688\n",
      "prepare test\n",
      "[ 0.14813504  0.14974     0.10206602 ...,  0.08021212  0.08106084\n",
      "  0.04498688]\n",
      "fold 8\n",
      "8 0.683650882628\n",
      "prepare test\n",
      "[ 0.14763976  0.14892431  0.10101306 ...,  0.07973616  0.08026111\n",
      "  0.04440297]\n",
      "fold 9\n",
      "9 0.691927886333\n",
      "prepare test\n",
      "[ 0.14971947  0.15133756  0.10406824 ...,  0.0826697   0.08325906\n",
      "  0.04764618]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = meta\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'booster' : 'gbtree',\n",
    "    'n_estimators':20000,\n",
    "    'max_depth':4,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'eval_metric':'auc',\n",
    "    'learning_rate':0.005, \n",
    "    'subsample':.6,\n",
    "    'min_child_weight':10,\n",
    "    'colsample_bytree':.6,\n",
    "    'scale_pos_weight': 19,\n",
    "    'gamma':1,\n",
    "    #'reg_alpha':1,\n",
    "    'reg_lambda':1.3,\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "\n",
    "    \n",
    "    X_tr = X_tr[train_cols].fillna(0).values\n",
    "    X_va = X_va[train_cols].fillna(0).values\n",
    "    X_te = X_te[train_cols].fillna(0).values\n",
    "    \n",
    "       \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    del scaler\n",
    "\n",
    "    y_hat_med = pd.DataFrame(X_va).mean(axis=1)\n",
    "    #print(ifold,'median:',roc_auc_score(y_va,y_hat_med))\n",
    "    \n",
    "    \n",
    "    # Create the LightGBM data containers\n",
    "    model = Ridge(alpha=20)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    \n",
    "    yhat = model.predict(X_va)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    #\n",
    "    ytst = pd.DataFrame(X_te).mean(axis=1)\n",
    "    ytst = model.predict(X_te)\n",
    "    print(minmax_scale(ytst))\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('xgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69866867306089697, 0.68028222299680186, 0.6720180642689908, 0.6882429925869018, 0.6816084169472465, 0.68395330770756191, 0.67853003557793867, 0.69497708568816041, 0.68365088262772289, 0.69192788633314306]\n",
      "0.68538595678 0.00768417125345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68522457782906487"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "roc_auc_score(X.target.values, y_oof.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.246518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.106445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.165956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.074222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.064321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.246518\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.106445\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.165956\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.074222\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.064321"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ridge_e1'\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame()\n",
    "s['s7463'] = minmax_scale(pd.read_csv(results_dir + 'ridge_b1.csv', header=None, names=['v']).v.values)\n",
    "s['s7412'] = minmax_scale(pd.read_csv(results_dir + 'baseline_sparse_10folds.csv', header=None, names=['v']).v.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s7463</th>\n",
       "      <th>s7412</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.811052</td>\n",
       "      <td>0.184876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.570673</td>\n",
       "      <td>0.096343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.726611</td>\n",
       "      <td>0.151491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.429136</td>\n",
       "      <td>0.055863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.389015</td>\n",
       "      <td>0.051491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      s7463     s7412\n",
       "0  0.811052  0.184876\n",
       "1  0.570673  0.096343\n",
       "2  0.726611  0.151491\n",
       "3  0.429136  0.055863\n",
       "4  0.389015  0.051491"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181024"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "0 0.708340402318\n",
      "prepare test\n",
      "[ 0.11833642  0.06954676  0.04558661 ...,  0.05827532  0.05132899\n",
      "  0.02449793]\n",
      "fold 1\n",
      "1 0.688346706663\n",
      "prepare test\n",
      "[ 0.12280386  0.07025359  0.04505599 ...,  0.05958592  0.05212693\n",
      "  0.02435883]\n",
      "fold 2\n",
      "2 0.684819165494\n",
      "prepare test\n",
      "[ 0.12269685  0.06897689  0.04413718 ...,  0.05876828  0.05143197\n",
      "  0.02353197]\n",
      "fold 3\n",
      "3 0.701067555645\n",
      "prepare test\n",
      "[ 0.12503102  0.06917574  0.04570564 ...,  0.05932545  0.05168608\n",
      "  0.02553804]\n",
      "fold 4\n",
      "4 0.691026574999\n",
      "prepare test\n",
      "[ 0.12058299  0.06864695  0.04472805 ...,  0.05722517  0.04967278\n",
      "  0.02260276]\n",
      "fold 5\n",
      "5 0.693334580204\n",
      "prepare test\n",
      "[ 0.12662066  0.06941002  0.04534232 ...,  0.05896353  0.05452349\n",
      "  0.02575778]\n",
      "fold 6\n",
      "6 0.690914034276\n",
      "prepare test\n",
      "[ 0.12218287  0.06798539  0.04394661 ...,  0.05787704  0.05201676\n",
      "  0.02397596]\n",
      "fold 7\n",
      "7 0.705269152806\n",
      "prepare test\n",
      "[ 0.1224833   0.06946139  0.04605142 ...,  0.05979037  0.05210769\n",
      "  0.0248025 ]\n",
      "fold 8\n",
      "8 0.695439919706\n",
      "prepare test\n",
      "[ 0.12474733  0.07133369  0.0451887  ...,  0.06044297  0.05170375\n",
      "  0.02561349]\n",
      "fold 9\n",
      "9 0.695940355797\n",
      "prepare test\n",
      "[ 0.12563466  0.06973907  0.04533891 ...,  0.0594839   0.05142445\n",
      "  0.02442681]\n",
      "[0.70834040231777951, 0.68834670666309106, 0.68481916549374511, 0.7010675556452195, 0.69102657499938402, 0.6933345802038785, 0.69091403427582398, 0.70526915280616354, 0.6954399197058968, 0.69594035579716784]\n",
      "0.695449844791 0.0070884681602\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.322249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.094247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.166798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.073315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.075228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.322249\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.094247\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.166798\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.073315\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.075228"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ridge_st'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['records', 'max_days', 'min_days', \n",
    "              'nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','dt2','dt1','dt3',\n",
    "              'nnet3','nnet1','nnet14','nnet15','timp','nnet19','nnet20','nnet21','nnet22',\n",
    "              'ftrl','vw2','sgd1','nnet16','nnet17','nnet18','tfidf2','tfidf1','deeptree',\n",
    "              'vw','ftrl_50',\n",
    "              'nnet4','nnet5','nnet6','nnet7',\n",
    "              'nnet8','lgbmb'] + ['svd_title_'+str(i+1) for i in range(3)] + ['asvd_'+str(i+1) for i in range(3)]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols].fillna(0).values\n",
    "    X_va = X_va[train_cols].fillna(0).values\n",
    "    X_te = X_te[train_cols].fillna(0).values\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    \n",
    "    model = Ridge()\n",
    "    model.fit(X_tr,y_tr)\n",
    "    yhat = model.predict(X_va)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict(X_te)\n",
    "    print(ytst)\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te\n",
    "    gc.collect()    \n",
    "    \n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "\n",
    "model_name = 'ridge_st'\n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "0 0.705511680265\n",
      "prepare test\n",
      "[ 0.08642063  0.07267376  0.04741563 ...,  0.05219046  0.04414738\n",
      "  0.02277783]\n",
      "fold 1\n",
      "1 0.685630363913\n",
      "prepare test\n",
      "[ 0.0889254   0.07548698  0.0477886  ...,  0.05348164  0.04364017\n",
      "  0.02180306]\n",
      "fold 2\n",
      "2 0.67987137222\n",
      "prepare test\n",
      "[ 0.09477167  0.07239947  0.04755529 ...,  0.05354144  0.04526149\n",
      "  0.02388841]\n",
      "fold 3\n",
      "3 0.697525071504\n",
      "prepare test\n",
      "[ 0.0971157   0.07807275  0.04711992 ...,  0.05378634  0.0472256\n",
      "  0.02222092]\n",
      "fold 4\n",
      "4 0.689512048692\n",
      "prepare test\n",
      "[ 0.09449069  0.07382289  0.04677723 ...,  0.05336513  0.04462568\n",
      "  0.02210347]\n",
      "fold 5\n",
      "5 0.692438852205\n",
      "prepare test\n",
      "[ 0.08735287  0.07449404  0.04748723 ...,  0.05279011  0.04398263\n",
      "  0.02288745]\n",
      "fold 6\n",
      "6 0.686541830548\n",
      "prepare test\n",
      "[ 0.09340419  0.07275115  0.04686383 ...,  0.05253238  0.04410074\n",
      "  0.02276739]\n",
      "fold 7\n",
      "7 0.704164519576\n",
      "prepare test\n",
      "[ 0.0927281   0.0763958   0.04800999 ...,  0.05401471  0.04582339\n",
      "  0.02258885]\n",
      "fold 8\n",
      "8 0.691851131956\n",
      "prepare test\n",
      "[ 0.09485311  0.07455373  0.04858877 ...,  0.05353152  0.04542303\n",
      "  0.02241731]\n",
      "fold 9\n",
      "9 0.693338359596\n",
      "prepare test\n",
      "[ 0.0912804   0.07076584  0.04659129 ...,  0.05318816  0.04761756\n",
      "  0.02246242]\n",
      "[0.70551168026490596, 0.68563036391327226, 0.67987137222004834, 0.69752507150441456, 0.68951204869208349, 0.69243885220509782, 0.68654183054784723, 0.70416451957577442, 0.69185113195595394, 0.69333835959638468]\n",
      "0.692638523048 0.00763317554455\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.325353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.094868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.170970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.054060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.041742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.325353\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.094868\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.170970\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.054060\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.041742"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'extra_trees_st'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['records', 'max_days', 'min_days', \n",
    "              'nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','dt2','dt1','dt3',\n",
    "              'nnet3','nnet1','nnet14','nnet15','timp','nnet19','nnet20','nnet21','nnet22',\n",
    "              'ftrl','vw2','sgd1','nnet16','nnet17','nnet18','tfidf2','tfidf1','deeptree',\n",
    "              'vw','ftrl_50',\n",
    "              'nnet4','nnet5','nnet6','nnet7',\n",
    "              'nnet8','lgbmb'] + ['svd_title_'+str(i+1) for i in range(3)] + ['asvd_'+str(i+1) for i in range(3)]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols].fillna(0).values\n",
    "    X_va = X_va[train_cols].fillna(0).values\n",
    "    X_te = X_te[train_cols].fillna(0).values\n",
    "    \n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    #X_tr = scaler.fit_transform(X_tr)\n",
    "    #X_va = scaler.transform(X_va)\n",
    "    #X_te = scaler.transform(X_te)\n",
    "    \n",
    "    model = ExtraTreesClassifier(n_estimators=110, max_depth=4, min_samples_leaf=10)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    yhat = model.predict_proba(X_va)[:,1]\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict_proba(X_te)[:,1]\n",
    "    print(ytst)\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te\n",
    "    gc.collect()    \n",
    "    \n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "0 0.70804775327\n",
      "prepare test\n",
      "[ 0.116225    0.07927322  0.05347689 ...,  0.05864745  0.05113927\n",
      "  0.0223893 ]\n",
      "fold 1\n",
      "1 0.688951905744\n",
      "prepare test\n",
      "[ 0.12104915  0.08337226  0.05611021 ...,  0.05844343  0.05300218\n",
      "  0.02179187]\n",
      "fold 2\n",
      "2 0.682984219056\n",
      "prepare test\n",
      "[ 0.11450288  0.08221126  0.05529802 ...,  0.05637302  0.0518993\n",
      "  0.02136322]\n",
      "fold 3\n",
      "3 0.700850791707\n",
      "prepare test\n",
      "[ 0.12060298  0.08042467  0.05673899 ...,  0.05704012  0.053041    0.0229039 ]\n",
      "fold 4\n",
      "4 0.692135722238\n",
      "prepare test\n",
      "[ 0.12076868  0.07826234  0.05648222 ...,  0.055417    0.05190627\n",
      "  0.02325941]\n",
      "fold 5\n",
      "5 0.694826201798\n",
      "prepare test\n",
      "[ 0.12203647  0.08352681  0.05279225 ...,  0.05720413  0.05353128\n",
      "  0.02144274]\n",
      "fold 6\n",
      "6 0.689444576416\n",
      "prepare test\n",
      "[ 0.11810649  0.08007047  0.05446343 ...,  0.05510314  0.05291391  0.02166   ]\n",
      "fold 7\n",
      "7 0.705098681343\n",
      "prepare test\n",
      "[ 0.12421809  0.08521453  0.05282894 ...,  0.05790903  0.05262791\n",
      "  0.02173341]\n",
      "fold 8\n",
      "8 0.69417389921\n",
      "prepare test\n",
      "[ 0.1213723   0.08193032  0.05526349 ...,  0.05687396  0.05082249\n",
      "  0.02391575]\n",
      "fold 9\n",
      "9 0.695411500537\n",
      "prepare test\n",
      "[ 0.1226694   0.07765315  0.05460769 ...,  0.05735705  0.05296524\n",
      "  0.02102648]\n",
      "[0.70804775326950886, 0.68895190574429743, 0.68298421905597373, 0.70085079170732256, 0.69213572223827668, 0.69482620179751931, 0.68944457641621459, 0.70509868134335496, 0.69417389921007489, 0.69541150053662038]\n",
      "0.695192525132 0.00725944605138\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.407703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.096418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.201525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.055439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.056374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.407703\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.096418\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.201525\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.055439\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.056374"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'rf_trees_st'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['records', 'max_days', 'min_days', \n",
    "              'nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','dt2','dt1','dt3',\n",
    "              'nnet3','nnet1','nnet14','nnet15','timp','nnet19','nnet20','nnet21','nnet22',\n",
    "              'ftrl','vw2','sgd1','nnet16','nnet17','nnet18','tfidf2','tfidf1','deeptree',\n",
    "              'vw','ftrl_50',\n",
    "              'nnet4','nnet5','nnet6','nnet7',\n",
    "              'nnet8','lgbmb'] + ['svd_title_'+str(i+1) for i in range(3)] + ['asvd_'+str(i+1) for i in range(3)]\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols].fillna(0).values\n",
    "    X_va = X_va[train_cols].fillna(0).values\n",
    "    X_te = X_te[train_cols].fillna(0).values\n",
    "    \n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    #X_tr = scaler.fit_transform(X_tr)\n",
    "    #X_va = scaler.transform(X_va)\n",
    "    #X_te = scaler.transform(X_te)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=110, max_depth=4, min_samples_leaf=10)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    yhat = model.predict_proba(X_va)[:,1]\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict_proba(X_te)[:,1]\n",
    "    print(ytst)\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te\n",
    "    gc.collect()    \n",
    "    \n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = [\n",
    "              'nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','dt2','dt1','dt3',\n",
    "              'nnet3','nnet1','nnet14','nnet15','timp','nnet19','nnet20','nnet21','nnet22',\n",
    "              'ftrl','vw2','sgd1','nnet16','nnet17','nnet18','tfidf2','tfidf1','deeptree',\n",
    "              'vw','ftrl_50',\n",
    "              'nnet4','nnet5','nnet6','nnet7',\n",
    "              'nnet8','lgbmb']\n",
    "for m in ['ridge_st','extra_trees_st','rf_trees_st','xgb_e7','lgb_e9']:\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_' + m + '.npy'))\n",
    "    sample_sub.columns = ['uid',m]\n",
    "    x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "    X[m] = minmax_scale(np.load(results_dir + 'train_'+m+'.npy'))\n",
    "    meta.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.696862\tvalid_1's auc: 0.707882\n",
      "[200]\ttraining's auc: 0.69746\tvalid_1's auc: 0.708169\n",
      "[300]\ttraining's auc: 0.697925\tvalid_1's auc: 0.708068\n",
      "[400]\ttraining's auc: 0.698491\tvalid_1's auc: 0.707993\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's auc: 0.697309\tvalid_1's auc: 0.708271\n",
      "0 0.70827063927\n",
      "prepare test\n",
      "[ 0.64121986  0.55752167  0.49632673 ...,  0.50954521  0.49885458\n",
      "  0.34835354]\n",
      "fold 1\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.699094\tvalid_1's auc: 0.688708\n",
      "[200]\ttraining's auc: 0.699623\tvalid_1's auc: 0.688651\n",
      "[300]\ttraining's auc: 0.70016\tvalid_1's auc: 0.688616\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.698366\tvalid_1's auc: 0.68896\n",
      "1 0.688960350922\n",
      "prepare test\n",
      "[ 0.5272917   0.51023533  0.49676146 ...,  0.50329465  0.49723536\n",
      "  0.47430945]\n",
      "fold 2\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.699623\tvalid_1's auc: 0.683658\n",
      "[200]\ttraining's auc: 0.700098\tvalid_1's auc: 0.683658\n",
      "[300]\ttraining's auc: 0.700658\tvalid_1's auc: 0.683732\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's auc: 0.699539\tvalid_1's auc: 0.683815\n",
      "2 0.683815036753\n",
      "prepare test\n",
      "[ 0.58194585  0.52806397  0.49169609 ...,  0.50863226  0.49305612\n",
      "  0.41865275]\n",
      "fold 3\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.69761\tvalid_1's auc: 0.70162\n",
      "[200]\ttraining's auc: 0.6982\tvalid_1's auc: 0.701813\n",
      "[300]\ttraining's auc: 0.698703\tvalid_1's auc: 0.701832\n",
      "[400]\ttraining's auc: 0.699286\tvalid_1's auc: 0.701919\n",
      "[500]\ttraining's auc: 0.699961\tvalid_1's auc: 0.701954\n",
      "[600]\ttraining's auc: 0.70067\tvalid_1's auc: 0.70194\n",
      "[700]\ttraining's auc: 0.701417\tvalid_1's auc: 0.701932\n",
      "[800]\ttraining's auc: 0.702296\tvalid_1's auc: 0.701866\n",
      "Early stopping, best iteration is:\n",
      "[536]\ttraining's auc: 0.700195\tvalid_1's auc: 0.701977\n",
      "3 0.701977293313\n",
      "prepare test\n",
      "[ 0.73391682  0.58986153  0.48281619 ...,  0.51912798  0.48514622\n",
      "  0.2576781 ]\n",
      "fold 4\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698584\tvalid_1's auc: 0.692405\n",
      "[200]\ttraining's auc: 0.69911\tvalid_1's auc: 0.69243\n",
      "[300]\ttraining's auc: 0.699709\tvalid_1's auc: 0.692125\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.697285\tvalid_1's auc: 0.692814\n",
      "4 0.692814299827\n",
      "prepare test\n",
      "[ 0.50836267  0.50376038  0.49815162 ...,  0.50183523  0.49810327\n",
      "  0.4909471 ]\n",
      "fold 5\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698378\tvalid_1's auc: 0.693942\n",
      "[200]\ttraining's auc: 0.698896\tvalid_1's auc: 0.693997\n",
      "[300]\ttraining's auc: 0.699459\tvalid_1's auc: 0.693913\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's auc: 0.697621\tvalid_1's auc: 0.69445\n",
      "5 0.694450078302\n",
      "prepare test\n",
      "[ 0.51363687  0.50325431  0.49669693 ...,  0.49801231  0.49736014\n",
      "  0.48394011]\n",
      "fold 6\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698837\tvalid_1's auc: 0.691006\n",
      "[200]\ttraining's auc: 0.699367\tvalid_1's auc: 0.690923\n",
      "[300]\ttraining's auc: 0.699899\tvalid_1's auc: 0.690858\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.698057\tvalid_1's auc: 0.691311\n",
      "6 0.691310876131\n",
      "prepare test\n",
      "[ 0.5133173   0.5048913   0.4996208  ...,  0.50165848  0.49962236\n",
      "  0.48826406]\n",
      "fold 7\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.696973\tvalid_1's auc: 0.706406\n",
      "[200]\ttraining's auc: 0.697537\tvalid_1's auc: 0.706298\n",
      "[300]\ttraining's auc: 0.698082\tvalid_1's auc: 0.706359\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's auc: 0.696534\tvalid_1's auc: 0.706498\n",
      "7 0.706498160417\n",
      "prepare test\n",
      "[ 0.5271439   0.50987816  0.49809751 ...,  0.50601372  0.49791475\n",
      "  0.47885451]\n",
      "fold 8\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698196\tvalid_1's auc: 0.695236\n",
      "[200]\ttraining's auc: 0.698801\tvalid_1's auc: 0.695306\n",
      "[300]\ttraining's auc: 0.699374\tvalid_1's auc: 0.695194\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.697182\tvalid_1's auc: 0.695854\n",
      "8 0.695853983901\n",
      "prepare test\n",
      "[ 0.5063045   0.50356028  0.49891853 ...,  0.50115527  0.49891853\n",
      "  0.49402066]\n",
      "fold 9\n",
      "prepare train\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.698104\tvalid_1's auc: 0.697386\n",
      "[200]\ttraining's auc: 0.69853\tvalid_1's auc: 0.697252\n",
      "[300]\ttraining's auc: 0.699058\tvalid_1's auc: 0.697327\n",
      "[400]\ttraining's auc: 0.699601\tvalid_1's auc: 0.697431\n",
      "[500]\ttraining's auc: 0.700217\tvalid_1's auc: 0.69757\n",
      "[600]\ttraining's auc: 0.700898\tvalid_1's auc: 0.697621\n",
      "[700]\ttraining's auc: 0.701587\tvalid_1's auc: 0.697632\n",
      "[800]\ttraining's auc: 0.702384\tvalid_1's auc: 0.697591\n",
      "[900]\ttraining's auc: 0.703179\tvalid_1's auc: 0.697591\n",
      "[1000]\ttraining's auc: 0.703966\tvalid_1's auc: 0.697616\n",
      "[1100]\ttraining's auc: 0.704815\tvalid_1's auc: 0.697652\n",
      "[1200]\ttraining's auc: 0.705662\tvalid_1's auc: 0.697686\n",
      "[1300]\ttraining's auc: 0.706523\tvalid_1's auc: 0.697696\n",
      "[1400]\ttraining's auc: 0.70738\tvalid_1's auc: 0.69777\n",
      "[1500]\ttraining's auc: 0.708236\tvalid_1's auc: 0.697831\n",
      "[1600]\ttraining's auc: 0.709087\tvalid_1's auc: 0.697822\n",
      "[1700]\ttraining's auc: 0.709968\tvalid_1's auc: 0.697872\n",
      "[1800]\ttraining's auc: 0.710862\tvalid_1's auc: 0.697866\n",
      "[1900]\ttraining's auc: 0.711734\tvalid_1's auc: 0.697898\n",
      "[2000]\ttraining's auc: 0.712569\tvalid_1's auc: 0.697826\n",
      "Early stopping, best iteration is:\n",
      "[1739]\ttraining's auc: 0.710336\tvalid_1's auc: 0.697905\n",
      "9 0.697904735211\n",
      "prepare test\n",
      "[ 0.74042706  0.59764313  0.48313988 ...,  0.52514939  0.47829879\n",
      "  0.21907179]\n",
      "[0.70827063927013822, 0.68896035092219887, 0.68381503675343314, 0.70197729331266445, 0.69281429982728371, 0.69445007830215633, 0.69131087613113251, 0.70649816041668556, 0.6958539839006852, 0.69790473521056762]\n",
      "0.696185545405 0.00729645033907\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.942088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.511105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.762535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.378906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.376711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.942088\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.511105\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.762535\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.378906\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.376711"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'lgb_e10'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "              'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days','max_f1','max_f2','max_f3',\n",
    "              'svd_description_1','svd_description_2','svd_description_3','svd_description_4','svd_description_5',\n",
    "              'svd_description_6','svd_description_7','svd_description_8','svd_description_9','svd_description_10',\n",
    "              'most_freq_cat_te'] +\\\n",
    "             ['svd_title_'+str(i+1) for i in range(10)] +\\\n",
    "             ['asvd_'+str(i+1) for i in range(10)] +\\\n",
    "             meta[-5:]\n",
    "\n",
    "\n",
    "#,'most_freq_cat_te','nnet4','nnet5','nnet6','nnet7','nnet10','nnet11','nnet8',\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 16,\n",
    "    'max_depth' : 4,\n",
    "    #'min_data' : 30,\n",
    "    'lambda_l1' : 1.5,\n",
    "    'lambda_l2' : 30.2,\n",
    "    'bagging_freq' : 2,\n",
    "    'bagging_fraction' : 0.8,\n",
    "    'is_unbalance': True,\n",
    "    'learning_rate': 0.005,\n",
    "    'feature_fraction': 0.5,\n",
    "    'min_data_in_leaf':100,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr().astype(np.bool).astype(np.float32)\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr().astype(np.bool).astype(np.float32)\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols]\n",
    "    X_va = X_va[train_cols]\n",
    "    X_te = X_te[train_cols]\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=0.1)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([     X_tr, sp_train_mat   ]).tocsr() #, train_mat_pcat[train_index] \n",
    "    #print(X_tr.shape)\n",
    "    #print('prepare valid')\n",
    "    X_va = sp.hstack([        X_va, sp_val_mat    ]).tocsr()     #, train_mat_pcat[test_index]\n",
    "    #print('prepare test')\n",
    "    X_te = sp.hstack([       X_te, sp_test_mat    ]).tocsr()      #, test_mat_pcat\n",
    "\n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = lgb.Dataset(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = lgb.Dataset(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "\n",
    "    model = lgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      valid_sets=[tr_data,va_data],\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=300,\n",
    "                      verbose_eval=100)\n",
    "    \n",
    "    yhat = model.predict(X_va, model.best_iteration)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict(X_te, model.best_iteration)\n",
    "    print(ytst)\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,tr_data,va_data, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))    \n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
