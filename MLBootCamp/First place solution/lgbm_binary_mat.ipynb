{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/mlboot_dataset/'\n",
    "model_name = 'lgbm_st'\n",
    "results_dir = './results/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 2053602) (609018, 2812610) (609018, 1057788)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir + 'preprocessed_new.csv') \n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index\n",
    "\n",
    "mat1 = sp.load_npz(data_dir+'dmat1.npz').tolil().astype(np.bool)\n",
    "mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil().astype(np.bool)\n",
    "mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil().astype(np.bool)\n",
    "print(mat1.shape, mat2.shape, mat3.shape)\n",
    "\n",
    "df['max_f1'] = mat1.tocsr().max(axis=1).todense()\n",
    "df['max_f2'] = mat2.tocsr().max(axis=1).todense()\n",
    "df['max_f3'] = mat3.tocsr().max(axis=1).todense()\n",
    "\n",
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "limit = 10\n",
    "mat1 = mat1.tocsc()[:, np.where((train_mat1.getnnz(axis=0) > limit) & (test_mat1.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat2 = mat2.tocsc()[:, np.where((train_mat2.getnnz(axis=0) > limit) & (test_mat2.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat3 = mat3.tocsc()[:, np.where((train_mat3.getnnz(axis=0) > limit) & (test_mat3.getnnz(axis=0) > 0))[0]].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 204775) (609018, 20268) (609018, 10296)\n"
     ]
    }
   ],
   "source": [
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = mat1[0].todense()\n",
    "a[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'pca_cat10.npy'), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd.columns = ['svd_description_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat1 = mat1[df_train_index.tolist()].astype(np.int8)\n",
    "test_mat1 = mat1[df_test_index.tolist()].astype(np.int8)\n",
    "train_mat2 = mat2[df_train_index.tolist()].astype(np.int8)\n",
    "test_mat2 = mat2[df_test_index.tolist()].astype(np.int8)\n",
    "train_mat3 = mat3[df_train_index.tolist()].astype(np.int8)\n",
    "test_mat3 = mat3[df_test_index.tolist()].astype(np.int8)\n",
    "\n",
    "del mat1,mat2,mat3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
       "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
       "       'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
       "       'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
       "       'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
       "       'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
       "       'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
       "       'mean_day_cntr', 'nuniq_keys_f1_cat0', 'nuniq_keys_f2_cat0',\n",
       "       'nuniq_keys_f3_cat0', 'nuniq_keys_f1_cat1', 'nuniq_keys_f2_cat1',\n",
       "       'nuniq_keys_f3_cat1', 'nuniq_keys_f1_cat2', 'nuniq_keys_f2_cat2',\n",
       "       'nuniq_keys_f3_cat2', 'nuniq_keys_f1_cat3', 'nuniq_keys_f2_cat3',\n",
       "       'nuniq_keys_f3_cat3', 'nuniq_keys_f1_cat4', 'nuniq_keys_f2_cat4',\n",
       "       'nuniq_keys_f3_cat4', 'nuniq_keys_f1_cat5', 'nuniq_keys_f2_cat5',\n",
       "       'nuniq_keys_f3_cat5', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
       "       'nuniq_keys_f1.2', 'sumval_keys_f1_cat0', 'sumval_keys_f2_cat0',\n",
       "       'sumval_keys_f3_cat0', 'sumval_keys_f1_cat1', 'sumval_keys_f2_cat1',\n",
       "       'sumval_keys_f3_cat1', 'sumval_keys_f1_cat2', 'sumval_keys_f2_cat2',\n",
       "       'sumval_keys_f3_cat2', 'sumval_keys_f1_cat3', 'sumval_keys_f2_cat3',\n",
       "       'sumval_keys_f3_cat3', 'sumval_keys_f1_cat4', 'sumval_keys_f2_cat4',\n",
       "       'sumval_keys_f3_cat4', 'sumval_keys_f1_cat5', 'sumval_keys_f2_cat5',\n",
       "       'sumval_keys_f3_cat5', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
       "       'sumval_keys_f1.2', 'most_freq_cat', 'diff_num_cats', 'unique_days',\n",
       "       'target', 'max_f1', 'max_f2', 'max_f3', 'svd_description_1',\n",
       "       'svd_description_2', 'svd_description_3', 'svd_description_4',\n",
       "       'svd_description_5', 'svd_description_6', 'svd_description_7',\n",
       "       'svd_description_8', 'svd_description_9', 'svd_description_10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_base_model.npy'))\n",
    "sample_sub.columns = ['uid','nnet']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_baseline_sparse_10folds.npy'))\n",
    "sample_sub.columns = ['uid','lgbm1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_all_in_focal_loss.npy'))\n",
    "sample_sub.columns = ['uid','nnet2']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m3.npy'))\n",
    "sample_sub.columns = ['uid','nnet3']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m1.npy'))\n",
    "sample_sub.columns = ['uid','nnet1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_ftrl.npy'))\n",
    "sample_sub.columns = ['uid','ftrl']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model.npy'))\n",
    "sample_sub.columns = ['uid','nnet4']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_catpca.npy'))\n",
    "sample_sub.columns = ['uid','nnet5']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br.npy'))\n",
    "sample_sub.columns = ['uid','nnet6']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['lgbm1'] = minmax_scale(np.load(results_dir + 'train_baseline_sparse_10folds.npy'))\n",
    "X['nnet'] = minmax_scale(np.load(results_dir + 'train_nn_base_model.npy'))\n",
    "X['nnet2'] = minmax_scale(np.load(results_dir + 'train_all_in_focal_loss.npy'))\n",
    "X['nnet1'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m1.npy'))\n",
    "X['nnet3'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m3.npy'))\n",
    "X['nnet4'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model.npy'))\n",
    "X['nnet5'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_catpca.npy'))\n",
    "X['nnet6'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br.npy'))\n",
    "X['ftrl'] = minmax_scale(np.load(results_dir + 'train_ftrl.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet</th>\n",
       "      <th>lgbm1</th>\n",
       "      <th>nnet2</th>\n",
       "      <th>nnet1</th>\n",
       "      <th>nnet3</th>\n",
       "      <th>nnet4</th>\n",
       "      <th>nnet5</th>\n",
       "      <th>nnet6</th>\n",
       "      <th>ftrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681589</td>\n",
       "      <td>0.753490</td>\n",
       "      <td>0.488332</td>\n",
       "      <td>0.330658</td>\n",
       "      <td>0.615233</td>\n",
       "      <td>0.632314</td>\n",
       "      <td>0.620995</td>\n",
       "      <td>0.389290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm1</th>\n",
       "      <td>0.681589</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.593723</td>\n",
       "      <td>0.536187</td>\n",
       "      <td>0.342257</td>\n",
       "      <td>0.661794</td>\n",
       "      <td>0.685686</td>\n",
       "      <td>0.730914</td>\n",
       "      <td>0.318644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet2</th>\n",
       "      <td>0.753490</td>\n",
       "      <td>0.593723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.582909</td>\n",
       "      <td>0.353475</td>\n",
       "      <td>0.615273</td>\n",
       "      <td>0.622576</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>0.329572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet1</th>\n",
       "      <td>0.488332</td>\n",
       "      <td>0.536187</td>\n",
       "      <td>0.582909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.697361</td>\n",
       "      <td>0.685702</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.260184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet3</th>\n",
       "      <td>0.330658</td>\n",
       "      <td>0.342257</td>\n",
       "      <td>0.353475</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541269</td>\n",
       "      <td>0.477470</td>\n",
       "      <td>0.262547</td>\n",
       "      <td>0.199443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet4</th>\n",
       "      <td>0.615233</td>\n",
       "      <td>0.661794</td>\n",
       "      <td>0.615273</td>\n",
       "      <td>0.697361</td>\n",
       "      <td>0.541269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942206</td>\n",
       "      <td>0.624456</td>\n",
       "      <td>0.311540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet5</th>\n",
       "      <td>0.632314</td>\n",
       "      <td>0.685686</td>\n",
       "      <td>0.622576</td>\n",
       "      <td>0.685702</td>\n",
       "      <td>0.477470</td>\n",
       "      <td>0.942206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644069</td>\n",
       "      <td>0.313207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet6</th>\n",
       "      <td>0.620995</td>\n",
       "      <td>0.730914</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.262547</td>\n",
       "      <td>0.624456</td>\n",
       "      <td>0.644069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ftrl</th>\n",
       "      <td>0.389290</td>\n",
       "      <td>0.318644</td>\n",
       "      <td>0.329572</td>\n",
       "      <td>0.260184</td>\n",
       "      <td>0.199443</td>\n",
       "      <td>0.311540</td>\n",
       "      <td>0.313207</td>\n",
       "      <td>0.323725</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nnet     lgbm1     nnet2     nnet1     nnet3     nnet4     nnet5  \\\n",
       "nnet   1.000000  0.681589  0.753490  0.488332  0.330658  0.615233  0.632314   \n",
       "lgbm1  0.681589  1.000000  0.593723  0.536187  0.342257  0.661794  0.685686   \n",
       "nnet2  0.753490  0.593723  1.000000  0.582909  0.353475  0.615273  0.622576   \n",
       "nnet1  0.488332  0.536187  0.582909  1.000000  0.488963  0.697361  0.685702   \n",
       "nnet3  0.330658  0.342257  0.353475  0.488963  1.000000  0.541269  0.477470   \n",
       "nnet4  0.615233  0.661794  0.615273  0.697361  0.541269  1.000000  0.942206   \n",
       "nnet5  0.632314  0.685686  0.622576  0.685702  0.477470  0.942206  1.000000   \n",
       "nnet6  0.620995  0.730914  0.501894  0.429881  0.262547  0.624456  0.644069   \n",
       "ftrl   0.389290  0.318644  0.329572  0.260184  0.199443  0.311540  0.313207   \n",
       "\n",
       "          nnet6      ftrl  \n",
       "nnet   0.620995  0.389290  \n",
       "lgbm1  0.730914  0.318644  \n",
       "nnet2  0.501894  0.329572  \n",
       "nnet1  0.429881  0.260184  \n",
       "nnet3  0.262547  0.199443  \n",
       "nnet4  0.624456  0.311540  \n",
       "nnet5  0.644069  0.313207  \n",
       "nnet6  1.000000  0.323725  \n",
       "ftrl   0.323725  1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te[['nnet','lgbm1','nnet2','nnet1','nnet3','nnet4','nnet5','nnet6','ftrl']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm1</th>\n",
       "      <th>nnet</th>\n",
       "      <th>nnet2</th>\n",
       "      <th>nnet1</th>\n",
       "      <th>nnet3</th>\n",
       "      <th>nnet4</th>\n",
       "      <th>nnet5</th>\n",
       "      <th>nnet6</th>\n",
       "      <th>ftrl</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.664970</td>\n",
       "      <td>0.570985</td>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.639325</td>\n",
       "      <td>0.649844</td>\n",
       "      <td>0.749904</td>\n",
       "      <td>0.574672</td>\n",
       "      <td>0.170409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet</th>\n",
       "      <td>0.664970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659126</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>0.289962</td>\n",
       "      <td>0.617057</td>\n",
       "      <td>0.621315</td>\n",
       "      <td>0.640988</td>\n",
       "      <td>0.616108</td>\n",
       "      <td>0.130658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet2</th>\n",
       "      <td>0.570985</td>\n",
       "      <td>0.659126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.548694</td>\n",
       "      <td>0.312146</td>\n",
       "      <td>0.627571</td>\n",
       "      <td>0.621176</td>\n",
       "      <td>0.527951</td>\n",
       "      <td>0.531092</td>\n",
       "      <td>0.111062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet1</th>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.460005</td>\n",
       "      <td>0.548694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.487426</td>\n",
       "      <td>0.694869</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>0.438476</td>\n",
       "      <td>0.461213</td>\n",
       "      <td>0.091230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet3</th>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.289962</td>\n",
       "      <td>0.312146</td>\n",
       "      <td>0.487426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419134</td>\n",
       "      <td>0.409703</td>\n",
       "      <td>0.274057</td>\n",
       "      <td>0.316298</td>\n",
       "      <td>0.049012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet4</th>\n",
       "      <td>0.639325</td>\n",
       "      <td>0.617057</td>\n",
       "      <td>0.627571</td>\n",
       "      <td>0.694869</td>\n",
       "      <td>0.419134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923874</td>\n",
       "      <td>0.603938</td>\n",
       "      <td>0.566767</td>\n",
       "      <td>0.120073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet5</th>\n",
       "      <td>0.649844</td>\n",
       "      <td>0.621315</td>\n",
       "      <td>0.621176</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>0.409703</td>\n",
       "      <td>0.923874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606204</td>\n",
       "      <td>0.565838</td>\n",
       "      <td>0.122909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet6</th>\n",
       "      <td>0.749904</td>\n",
       "      <td>0.640988</td>\n",
       "      <td>0.527951</td>\n",
       "      <td>0.438476</td>\n",
       "      <td>0.274057</td>\n",
       "      <td>0.603938</td>\n",
       "      <td>0.606204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.603479</td>\n",
       "      <td>0.141818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ftrl</th>\n",
       "      <td>0.574672</td>\n",
       "      <td>0.616108</td>\n",
       "      <td>0.531092</td>\n",
       "      <td>0.461213</td>\n",
       "      <td>0.316298</td>\n",
       "      <td>0.566767</td>\n",
       "      <td>0.565838</td>\n",
       "      <td>0.603479</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.170409</td>\n",
       "      <td>0.130658</td>\n",
       "      <td>0.111062</td>\n",
       "      <td>0.091230</td>\n",
       "      <td>0.049012</td>\n",
       "      <td>0.120073</td>\n",
       "      <td>0.122909</td>\n",
       "      <td>0.141818</td>\n",
       "      <td>0.102134</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lgbm1      nnet     nnet2     nnet1     nnet3     nnet4     nnet5  \\\n",
       "lgbm1   1.000000  0.664970  0.570985  0.494446  0.295207  0.639325  0.649844   \n",
       "nnet    0.664970  1.000000  0.659126  0.460005  0.289962  0.617057  0.621315   \n",
       "nnet2   0.570985  0.659126  1.000000  0.548694  0.312146  0.627571  0.621176   \n",
       "nnet1   0.494446  0.460005  0.548694  1.000000  0.487426  0.694869  0.676564   \n",
       "nnet3   0.295207  0.289962  0.312146  0.487426  1.000000  0.419134  0.409703   \n",
       "nnet4   0.639325  0.617057  0.627571  0.694869  0.419134  1.000000  0.923874   \n",
       "nnet5   0.649844  0.621315  0.621176  0.676564  0.409703  0.923874  1.000000   \n",
       "nnet6   0.749904  0.640988  0.527951  0.438476  0.274057  0.603938  0.606204   \n",
       "ftrl    0.574672  0.616108  0.531092  0.461213  0.316298  0.566767  0.565838   \n",
       "target  0.170409  0.130658  0.111062  0.091230  0.049012  0.120073  0.122909   \n",
       "\n",
       "           nnet6      ftrl    target  \n",
       "lgbm1   0.749904  0.574672  0.170409  \n",
       "nnet    0.640988  0.616108  0.130658  \n",
       "nnet2   0.527951  0.531092  0.111062  \n",
       "nnet1   0.438476  0.461213  0.091230  \n",
       "nnet3   0.274057  0.316298  0.049012  \n",
       "nnet4   0.603938  0.566767  0.120073  \n",
       "nnet5   0.606204  0.565838  0.122909  \n",
       "nnet6   1.000000  0.603479  0.141818  \n",
       "ftrl    0.603479  1.000000  0.102134  \n",
       "target  0.141818  0.102134  1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['lgbm1','nnet','nnet2','nnet1','nnet3','nnet4','nnet5','nnet6','ftrl','target']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682562603224\n",
      "0.648537598744\n",
      "0.649222662589\n",
      "0.623698784984\n",
      "0.574520607881\n",
      "0.647936920552\n",
      "0.648281500429\n",
      "0.628310959446\n",
      "0.658849449887\n"
     ]
    }
   ],
   "source": [
    "for f in ['lgbm1','nnet','nnet2','nnet1','nnet3','nnet4','nnet5','ftrl','nnet6']:\n",
    "    print(roc_auc_score(X.target, X[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['ridge1'] = np.load(results_dir + 'train_ridge1.npy')\n",
    "x_te['ridge1'] = np.load(results_dir + 'test_ridge1.npy')\n",
    "X['ridge2'] = np.load(results_dir + 'train_ridge2.npy')\n",
    "x_te['ridge2'] = np.load(results_dir + 'test_ridge2.npy')\n",
    "X['ridge3'] = np.load(results_dir + 'train_ridge3.npy')\n",
    "x_te['ridge3'] = np.load(results_dir + 'test_ridge3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<181024x11850 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 96203785 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "(385194, 11848)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.737146\tvalid_1's auc: 0.662712\n",
      "[200]\ttraining's auc: 0.766301\tvalid_1's auc: 0.671903\n",
      "[300]\ttraining's auc: 0.787981\tvalid_1's auc: 0.676771\n",
      "[400]\ttraining's auc: 0.804843\tvalid_1's auc: 0.679156\n",
      "[500]\ttraining's auc: 0.818574\tvalid_1's auc: 0.679994\n",
      "[600]\ttraining's auc: 0.830146\tvalid_1's auc: 0.680625\n",
      "[700]\ttraining's auc: 0.839768\tvalid_1's auc: 0.681031\n",
      "[800]\ttraining's auc: 0.84748\tvalid_1's auc: 0.680824\n",
      "[900]\ttraining's auc: 0.85395\tvalid_1's auc: 0.680532\n",
      "[1000]\ttraining's auc: 0.859728\tvalid_1's auc: 0.680158\n",
      "Early stopping, best iteration is:\n",
      "[714]\ttraining's auc: 0.841015\tvalid_1's auc: 0.681113\n",
      "0 0.681113011371\n",
      "prepare test\n",
      "fold 1\n",
      "prepare train\n",
      "(385194, 11848)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.737615\tvalid_1's auc: 0.65384\n",
      "[200]\ttraining's auc: 0.766517\tvalid_1's auc: 0.660893\n",
      "[300]\ttraining's auc: 0.787908\tvalid_1's auc: 0.664733\n",
      "[400]\ttraining's auc: 0.804794\tvalid_1's auc: 0.666151\n",
      "[500]\ttraining's auc: 0.819208\tvalid_1's auc: 0.66691\n",
      "[600]\ttraining's auc: 0.830582\tvalid_1's auc: 0.666958\n",
      "[700]\ttraining's auc: 0.839439\tvalid_1's auc: 0.666868\n",
      "[800]\ttraining's auc: 0.846756\tvalid_1's auc: 0.666425\n",
      "Early stopping, best iteration is:\n",
      "[552]\ttraining's auc: 0.825464\tvalid_1's auc: 0.667297\n",
      "1 0.667296706326\n",
      "prepare test\n",
      "fold 2\n",
      "prepare train\n",
      "(385194, 11847)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.738643\tvalid_1's auc: 0.642824\n",
      "[200]\ttraining's auc: 0.767459\tvalid_1's auc: 0.649458\n",
      "[300]\ttraining's auc: 0.789228\tvalid_1's auc: 0.652438\n",
      "[400]\ttraining's auc: 0.806046\tvalid_1's auc: 0.653969\n",
      "[500]\ttraining's auc: 0.819424\tvalid_1's auc: 0.654367\n",
      "[600]\ttraining's auc: 0.830273\tvalid_1's auc: 0.653642\n",
      "[700]\ttraining's auc: 0.839345\tvalid_1's auc: 0.653102\n",
      "[800]\ttraining's auc: 0.846547\tvalid_1's auc: 0.652717\n",
      "Early stopping, best iteration is:\n",
      "[506]\ttraining's auc: 0.820191\tvalid_1's auc: 0.654434\n",
      "2 0.654434387584\n",
      "prepare test\n",
      "fold 3\n",
      "prepare train\n",
      "(385194, 11787)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.73768\tvalid_1's auc: 0.656654\n",
      "[200]\ttraining's auc: 0.766179\tvalid_1's auc: 0.664211\n",
      "[300]\ttraining's auc: 0.787709\tvalid_1's auc: 0.667584\n",
      "[400]\ttraining's auc: 0.804903\tvalid_1's auc: 0.669026\n",
      "[500]\ttraining's auc: 0.818704\tvalid_1's auc: 0.669856\n",
      "[600]\ttraining's auc: 0.830048\tvalid_1's auc: 0.67021\n",
      "[700]\ttraining's auc: 0.838953\tvalid_1's auc: 0.670431\n",
      "[800]\ttraining's auc: 0.846445\tvalid_1's auc: 0.670868\n",
      "[900]\ttraining's auc: 0.85335\tvalid_1's auc: 0.67122\n",
      "[1000]\ttraining's auc: 0.859592\tvalid_1's auc: 0.670986\n",
      "[1100]\ttraining's auc: 0.865341\tvalid_1's auc: 0.670828\n",
      "[1200]\ttraining's auc: 0.871007\tvalid_1's auc: 0.670572\n",
      "Early stopping, best iteration is:\n",
      "[901]\ttraining's auc: 0.853432\tvalid_1's auc: 0.671259\n",
      "3 0.671259248854\n",
      "prepare test\n",
      "fold 4\n",
      "prepare train\n",
      "(385195, 11848)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.73888\tvalid_1's auc: 0.650631\n",
      "[200]\ttraining's auc: 0.767159\tvalid_1's auc: 0.65864\n",
      "[300]\ttraining's auc: 0.788883\tvalid_1's auc: 0.662537\n",
      "[400]\ttraining's auc: 0.805982\tvalid_1's auc: 0.664909\n",
      "[500]\ttraining's auc: 0.819897\tvalid_1's auc: 0.665666\n",
      "[600]\ttraining's auc: 0.831301\tvalid_1's auc: 0.665376\n",
      "[700]\ttraining's auc: 0.840255\tvalid_1's auc: 0.665121\n",
      "[800]\ttraining's auc: 0.847682\tvalid_1's auc: 0.664633\n",
      "Early stopping, best iteration is:\n",
      "[508]\ttraining's auc: 0.82085\tvalid_1's auc: 0.665731\n",
      "4 0.665730818083\n",
      "prepare test\n",
      "fold 5\n",
      "prepare train\n",
      "(385195, 11848)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.737983\tvalid_1's auc: 0.653968\n",
      "[200]\ttraining's auc: 0.767044\tvalid_1's auc: 0.663211\n",
      "[300]\ttraining's auc: 0.789158\tvalid_1's auc: 0.667842\n",
      "[400]\ttraining's auc: 0.805857\tvalid_1's auc: 0.670261\n",
      "[500]\ttraining's auc: 0.819981\tvalid_1's auc: 0.671254\n",
      "[600]\ttraining's auc: 0.831509\tvalid_1's auc: 0.671642\n",
      "[700]\ttraining's auc: 0.840607\tvalid_1's auc: 0.67157\n",
      "[800]\ttraining's auc: 0.847329\tvalid_1's auc: 0.671677\n",
      "[900]\ttraining's auc: 0.853286\tvalid_1's auc: 0.671274\n",
      "[1000]\ttraining's auc: 0.858799\tvalid_1's auc: 0.67075\n",
      "Early stopping, best iteration is:\n",
      "[768]\ttraining's auc: 0.845499\tvalid_1's auc: 0.67178\n",
      "5 0.671780268077\n",
      "prepare test\n",
      "fold 6\n",
      "prepare train\n",
      "(385195, 11848)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.738177\tvalid_1's auc: 0.652408\n",
      "[200]\ttraining's auc: 0.766463\tvalid_1's auc: 0.658147\n",
      "[300]\ttraining's auc: 0.788654\tvalid_1's auc: 0.661467\n",
      "[400]\ttraining's auc: 0.806015\tvalid_1's auc: 0.663265\n",
      "[500]\ttraining's auc: 0.819658\tvalid_1's auc: 0.66363\n",
      "[600]\ttraining's auc: 0.829815\tvalid_1's auc: 0.6639\n",
      "[700]\ttraining's auc: 0.83783\tvalid_1's auc: 0.66377\n",
      "[800]\ttraining's auc: 0.844975\tvalid_1's auc: 0.663737\n",
      "Early stopping, best iteration is:\n",
      "[577]\ttraining's auc: 0.828012\tvalid_1's auc: 0.66406\n",
      "6 0.664060030695\n",
      "prepare test\n",
      "fold 7\n",
      "prepare train\n",
      "(385195, 11848)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.737817\tvalid_1's auc: 0.663269\n",
      "[200]\ttraining's auc: 0.76626\tvalid_1's auc: 0.67031\n",
      "[300]\ttraining's auc: 0.78838\tvalid_1's auc: 0.673288\n",
      "[400]\ttraining's auc: 0.804929\tvalid_1's auc: 0.674683\n",
      "[500]\ttraining's auc: 0.818941\tvalid_1's auc: 0.674961\n",
      "[600]\ttraining's auc: 0.829957\tvalid_1's auc: 0.675285\n",
      "[700]\ttraining's auc: 0.838353\tvalid_1's auc: 0.675256\n",
      "[800]\ttraining's auc: 0.845423\tvalid_1's auc: 0.675125\n",
      "Early stopping, best iteration is:\n",
      "[572]\ttraining's auc: 0.827247\tvalid_1's auc: 0.675432\n",
      "7 0.675432079525\n",
      "prepare test\n",
      "fold 8\n",
      "prepare train\n",
      "(385195, 11848)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.738591\tvalid_1's auc: 0.65288\n",
      "[200]\ttraining's auc: 0.766363\tvalid_1's auc: 0.657319\n",
      "[300]\ttraining's auc: 0.788124\tvalid_1's auc: 0.659228\n",
      "[400]\ttraining's auc: 0.805317\tvalid_1's auc: 0.659886\n",
      "[500]\ttraining's auc: 0.819311\tvalid_1's auc: 0.659433\n",
      "[600]\ttraining's auc: 0.830709\tvalid_1's auc: 0.659194\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttraining's auc: 0.798412\tvalid_1's auc: 0.659968\n",
      "8 0.65996783608\n",
      "prepare test\n",
      "fold 9\n",
      "prepare train\n",
      "(385195, 11847)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.738783\tvalid_1's auc: 0.657194\n",
      "[200]\ttraining's auc: 0.766612\tvalid_1's auc: 0.663038\n",
      "[300]\ttraining's auc: 0.788002\tvalid_1's auc: 0.66585\n",
      "[400]\ttraining's auc: 0.804698\tvalid_1's auc: 0.667726\n",
      "[500]\ttraining's auc: 0.818473\tvalid_1's auc: 0.668372\n",
      "[600]\ttraining's auc: 0.830012\tvalid_1's auc: 0.668753\n",
      "[700]\ttraining's auc: 0.83921\tvalid_1's auc: 0.668124\n",
      "[800]\ttraining's auc: 0.846524\tvalid_1's auc: 0.667592\n",
      "Early stopping, best iteration is:\n",
      "[598]\ttraining's auc: 0.82976\tvalid_1's auc: 0.668808\n",
      "9 0.66880812976\n",
      "prepare test\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "       'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "       'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "       'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "       'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "       'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "       'mean_day_cntr', 'nuniq_keys_f1_cat0', 'nuniq_keys_f2_cat0',\n",
    "       'nuniq_keys_f3_cat0', 'nuniq_keys_f1_cat1', 'nuniq_keys_f2_cat1',\n",
    "       'nuniq_keys_f3_cat1', 'nuniq_keys_f1_cat2', 'nuniq_keys_f2_cat2',\n",
    "       'nuniq_keys_f3_cat2', 'nuniq_keys_f1_cat3', 'nuniq_keys_f2_cat3',\n",
    "       'nuniq_keys_f3_cat3', 'nuniq_keys_f1_cat4', 'nuniq_keys_f2_cat4',\n",
    "       'nuniq_keys_f3_cat4', 'nuniq_keys_f1_cat5', 'nuniq_keys_f2_cat5',\n",
    "       'nuniq_keys_f3_cat5', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "       'nuniq_keys_f1.2', 'sumval_keys_f1_cat0', 'sumval_keys_f2_cat0',\n",
    "       'sumval_keys_f3_cat0', 'sumval_keys_f1_cat1', 'sumval_keys_f2_cat1',\n",
    "       'sumval_keys_f3_cat1', 'sumval_keys_f1_cat2', 'sumval_keys_f2_cat2',\n",
    "       'sumval_keys_f3_cat2', 'sumval_keys_f1_cat3', 'sumval_keys_f2_cat3',\n",
    "       'sumval_keys_f3_cat3', 'sumval_keys_f1_cat4', 'sumval_keys_f2_cat4',\n",
    "       'sumval_keys_f3_cat4', 'sumval_keys_f1_cat5', 'sumval_keys_f2_cat5',\n",
    "       'sumval_keys_f3_cat5', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "       'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days',\n",
    "       'max_f1', 'max_f2', 'max_f3', \n",
    "       'svd_description_1',\n",
    "       'svd_description_2', 'svd_description_3', 'svd_description_4',\n",
    "       'svd_description_5', 'svd_description_6', 'svd_description_7',\n",
    "       'svd_description_8', 'svd_description_9', 'svd_description_10']\n",
    "\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 128,\n",
    "    'max_depth' : 10,\n",
    "    'min_data' : 50,\n",
    "    #'lambda_l2' : 15.5,\n",
    "    'min_sum_hessian_in_leaf' : 0.5,\n",
    "    #'lambda_l1' : 0.2,\n",
    "    'is_unbalance': True,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.7,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr()\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr()\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols]\n",
    "    X_va = X_va[train_cols]\n",
    "    X_te = X_te[train_cols]\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=5)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([\n",
    "        X_tr.astype(np.float32), sp_train_mat.astype(np.float32)\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    print('prepare valid')\n",
    "    X_va = sp.hstack([\n",
    "        X_va.astype(np.float32), sp_val_mat.astype(np.float32)\n",
    "    ]).tocsr()    \n",
    "    print('prepare test')\n",
    "    X_te = sp.hstack([\n",
    "        X_te.astype(np.float32), sp_test_mat.astype(np.float32)\n",
    "    ]).tocsr()     \n",
    "\n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = lgb.Dataset(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = lgb.Dataset(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "\n",
    "    model = lgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      valid_sets=[tr_data,va_data],\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=300,\n",
    "                      verbose_eval=100)\n",
    "    \n",
    "    yhat = model.predict(X_va, model.best_iteration)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict(X_te, model.best_iteration)\n",
    "    y_pred += ytst*0.1\n",
    "    \n",
    "    del X_tr,X_va,tr_data,va_data, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('lgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68111301137079538, 0.66729670632609595, 0.65443438758439576, 0.67125924885411026, 0.6657308180834548, 0.67178026807655611, 0.66406003069547004, 0.67543207952522499, 0.659967836079704, 0.66880812975959825]\n",
      "0.667988251636 0.00722814195148\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.574572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.582298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.643829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.411411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.399608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.574572\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.582298\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.643829\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.411411\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.399608"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'binary_lgbm'\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66792997872031157"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(X.target.values, y_oof.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
