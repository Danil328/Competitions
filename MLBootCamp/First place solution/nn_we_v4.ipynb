{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/mlboot_dataset/'\n",
    "results_dir = './results/'\n",
    "model_name = 'nn_we_v4'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir + 'preprocessed_new.csv') \n",
    "q = pd.read_csv(data_dir + 'sessions.csv')\n",
    "df = df.merge(q, on='uid', how='left')\n",
    "del q\n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'pca_cat10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "mat1 = sp.load_npz(data_dir+'dmat1.npz').tolil()\n",
    "mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil()\n",
    "mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil()\n",
    "\n",
    "mat = sp.hstack([mat1,mat2,mat3]).tolil()\n",
    "del mat1,mat2,mat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = mat[df_train_index.tolist()]\n",
    "test_mat = mat[df_test_index.tolist()]\n",
    "mat = mat.tocsc()[:, np.where((train_mat.getnnz(axis=0) > 1) & (test_mat.getnnz(axis=0) > 1))[0]].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 609018/609018 [01:22<00:00, 7342.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "idcs = []\n",
    "for q in tqdm.tqdm(range(len(df))):\n",
    "    idcs.append(mat[q].nonzero()[1] + 1)\n",
    "df['idcs'] = np.array(idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_idcs'] = df.idcs.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    609018.000000\n",
       "mean        749.315358\n",
       "std         903.071800\n",
       "min           0.000000\n",
       "25%         156.000000\n",
       "50%         450.000000\n",
       "75%        1001.000000\n",
       "max       17519.000000\n",
       "Name: len_idcs, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.len_idcs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378668"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['max_idc'] = df.idcs.apply(lambda r : np.max(r) if len(r) > 0 else 0)\n",
    "np.max(df.max_idc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mat,train_mat,test_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(r):\n",
    "    if len(r) > 750:\n",
    "        return(r[:750])\n",
    "    return r+[0]*(750-len(r))\n",
    "df['idcs'] = df['idcs'].apply(lambda r : np.array(padding(r.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['uid','idcs']].to_csv(data_dir + 'indices.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_pca = np.load(data_dir + 'pca_cat100.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "scaler_mat = MaxAbsScaler()\n",
    "mat_pca = scaler_mat.fit_transform(mat_pca)\n",
    "train_mat_pcat = mat_pca[df_train_index.tolist()]\n",
    "test_mat_pcat = mat_pca[df_test_index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import Sequence\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
       "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
       "       'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
       "       'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
       "       'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
       "       'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
       "       'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
       "       'mean_day_cntr', 'nuniq_keys_f1_cat0', 'nuniq_keys_f2_cat0',\n",
       "       'nuniq_keys_f3_cat0', 'nuniq_keys_f1_cat1', 'nuniq_keys_f2_cat1',\n",
       "       'nuniq_keys_f3_cat1', 'nuniq_keys_f1_cat2', 'nuniq_keys_f2_cat2',\n",
       "       'nuniq_keys_f3_cat2', 'nuniq_keys_f1_cat3', 'nuniq_keys_f2_cat3',\n",
       "       'nuniq_keys_f3_cat3', 'nuniq_keys_f1_cat4', 'nuniq_keys_f2_cat4',\n",
       "       'nuniq_keys_f3_cat4', 'nuniq_keys_f1_cat5', 'nuniq_keys_f2_cat5',\n",
       "       'nuniq_keys_f3_cat5', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
       "       'nuniq_keys_f1.2', 'sumval_keys_f1_cat0', 'sumval_keys_f2_cat0',\n",
       "       'sumval_keys_f3_cat0', 'sumval_keys_f1_cat1', 'sumval_keys_f2_cat1',\n",
       "       'sumval_keys_f3_cat1', 'sumval_keys_f1_cat2', 'sumval_keys_f2_cat2',\n",
       "       'sumval_keys_f3_cat2', 'sumval_keys_f1_cat3', 'sumval_keys_f2_cat3',\n",
       "       'sumval_keys_f3_cat3', 'sumval_keys_f1_cat4', 'sumval_keys_f2_cat4',\n",
       "       'sumval_keys_f3_cat4', 'sumval_keys_f1_cat5', 'sumval_keys_f2_cat5',\n",
       "       'sumval_keys_f3_cat5', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
       "       'sumval_keys_f1.2', 'most_freq_cat', 'diff_num_cats', 'unique_days',\n",
       "       'sess_keys_mean', 'sess_keys_max', 'diff_key1_mean', 'diff_key1_max',\n",
       "       'diff_key2_mean', 'diff_key2_max', 'diff_key3_mean', 'diff_key3_max',\n",
       "       'quot_key1_mean', 'quot_key1_max', 'quot_key2_mean', 'quot_key2_max',\n",
       "       'quot_key3_mean', 'quot_key3_max', 'target', 'svd_1', 'svd_2', 'svd_3',\n",
       "       'svd_4', 'svd_5', 'svd_6', 'svd_7', 'svd_8', 'svd_9', 'svd_10', 'idcs',\n",
       "       'len_idcs', 'max_idc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "       'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "       'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "       'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "       'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "       'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "       'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "       'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "       'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "       'mean_day_cntr', 'nuniq_keys_f1_cat0', 'nuniq_keys_f2_cat0',\n",
    "       'nuniq_keys_f3_cat0', 'nuniq_keys_f1_cat1', 'nuniq_keys_f2_cat1',\n",
    "       'nuniq_keys_f3_cat1', 'nuniq_keys_f1_cat2', 'nuniq_keys_f2_cat2',\n",
    "       'nuniq_keys_f3_cat2', 'nuniq_keys_f1_cat3', 'nuniq_keys_f2_cat3',\n",
    "       'nuniq_keys_f3_cat3', 'nuniq_keys_f1_cat4', 'nuniq_keys_f2_cat4',\n",
    "       'nuniq_keys_f3_cat4', 'nuniq_keys_f1_cat5', 'nuniq_keys_f2_cat5',\n",
    "       'nuniq_keys_f3_cat5', 'nuniq_keys_f1', 'sumval_keys_f1_cat0', 'sumval_keys_f2_cat0',\n",
    "       'sumval_keys_f3_cat0', 'sumval_keys_f1_cat1', 'sumval_keys_f2_cat1',\n",
    "       'sumval_keys_f3_cat1', 'sumval_keys_f1_cat2', 'sumval_keys_f2_cat2',\n",
    "       'sumval_keys_f3_cat2', 'sumval_keys_f1_cat3', 'sumval_keys_f2_cat3',\n",
    "       'sumval_keys_f3_cat3', 'sumval_keys_f1_cat4', 'sumval_keys_f2_cat4',\n",
    "       'sumval_keys_f3_cat4', 'sumval_keys_f1_cat5', 'sumval_keys_f2_cat5',\n",
    "       'sumval_keys_f3_cat5', 'sumval_keys_f1', 'most_freq_cat', 'diff_num_cats', 'unique_days'] + ['svd_'+str(i+1) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X[train_cols].fillna(0).values)\n",
    "X[train_cols] = scaler.transform(X[train_cols].fillna(0).values)\n",
    "x_te[train_cols] = scaler.transform(x_te[train_cols].fillna(0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import log_loss\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "            \n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, X_seq, y, name, interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.X_seq, self.y = X_seq, y\n",
    "        self.name = name\n",
    "        self.interval = interval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_generator(self.X_seq, steps=len(self.X_seq), \n",
    "                                                          use_multiprocessing=False, workers=1, \n",
    "                                                          max_queue_size=4*cpu_cores).ravel()\n",
    "            auc = roc_auc_score(self.y, y_pred)\n",
    "            logloss = log_loss(self.y, y_pred)\n",
    "            logs[self.name+\"_auc\"] = auc\n",
    "            logs[self.name+\"_logloss\"] = logloss\n",
    "            print((self.name+\"_auc: {:.8f}; \"+\"_logloss: {:.8f}; \").format(auc,logloss))\n",
    "            \n",
    "class FeatureSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, X, y, inx, batch_size, shuffle=False):\n",
    "        \n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.inx = inx\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.inx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.inx.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        batch_inx = self.inx[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        \n",
    "        batch = [x[batch_inx] for x in self.X[:2]] +  [x[batch_inx] for x in [self.X[-1]]]\n",
    "        return batch, self.y[batch_inx]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.inx)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Hash embedding layer. Note that the zero word index is always used for masking.\n",
    "    # Properties\n",
    "        max_word_idx: maximum word index (e.g. the maximum dictionary value).\n",
    "        num_buckets: number of buckets\n",
    "        embedding_size: size of embedding\n",
    "        num_hash_functions: number of hash functions\n",
    "        W_trainable = True, if the embedding should be trainable\n",
    "        p_trainable = True, if the importance parameters should be trainable\n",
    "        append_weight, True if the importance parameters should be appended\n",
    "        aggregation_mode: either 'sum' or 'concatenate' depending on whether\n",
    "                        the component vectors should be summed or concatenated\n",
    "\"\"\"\n",
    "class HashEmbedding(Layer):\n",
    "    def __init__(self, max_word_idx = 378669, num_buckets = 40000, embedding_size = 37, num_hash_functions=2,\n",
    "                 W_trainable=True, p_trainable = True, append_weight= True, aggregation_mode = 'sum', seed=3, **kwargs):\n",
    "        super(HashEmbedding, self).__init__(**kwargs)\n",
    "        np.random.seed(seed)\n",
    "        self.word_count = max_word_idx\n",
    "        W = np.random.normal(0, 0.1, (num_buckets, embedding_size))\n",
    "        self.num_buckets = W.shape[0]\n",
    "        self.mask_zero = True\n",
    "        self.append_weight = append_weight\n",
    "        self.p = None\n",
    "        self.trainable_weights = []\n",
    "        self.p_trainable = p_trainable\n",
    "        self.num_hashes = num_hash_functions\n",
    "        self.p_init_std = 0.0005\n",
    "\n",
    "        self.num_hash_functions = num_hash_functions\n",
    "        self.hashing_vals = []\n",
    "        self.hashing_offset_vals = []\n",
    "\n",
    "\n",
    "        # Initialize hash table. Note that this could easily be implemented by a modulo operation\n",
    "        tab = (np.random.randint(0, 2 ** 30, size=(self.word_count, self.num_hash_functions)) % self.num_buckets) + 1\n",
    "        self.hash_tables = K.variable(tab, dtype='int32')\n",
    "\n",
    "        # Initialize word importance parameters\n",
    "        p_init = np.random.normal(0, self.p_init_std, (self.word_count, self.num_hashes))\n",
    "        self.p = K.variable(p_init,name='p_hash')\n",
    "        if self.p_trainable:\n",
    "            self.trainable_weights.append(self.p)\n",
    "\n",
    "\n",
    "        #Initialize the embedding matrix\n",
    "        # add zero vector for nulls (for masking)\n",
    "        W = np.row_stack((np.zeros((1, W.shape[1])), W)).astype('float32')\n",
    "        self.embedding_size = W.shape[1]\n",
    "        W_shared = K.variable(W, name='W_hash')\n",
    "        self.W = W_shared\n",
    "        if W_trainable:\n",
    "            self.trainable_weights.append(self.W)\n",
    "\n",
    "        if aggregation_mode == 'sum':\n",
    "            self.aggregation_function = sum\n",
    "        else:\n",
    "            if aggregation_mode == 'concatenate':\n",
    "                self.aggregation_function = lambda x: K.concatenate(x,axis = -1)\n",
    "            else:\n",
    "                raise('unknown aggregation function')\n",
    "        self.aggregation_mode = aggregation_mode\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "\n",
    "    def call(self, input, mask=None):\n",
    "        W = self.W\n",
    "        pvals = []\n",
    "        retvals = []\n",
    "        input_w = input%self.word_count\n",
    "        input_p = (3+input)%self.word_count\n",
    "        idx_bucket_all = K.gather(self.hash_tables, input_w)\n",
    "        for hash_fun_num in range(self.num_hash_functions):\n",
    "            W0 = K.gather(W, idx_bucket_all[:,:,hash_fun_num]*(1-K.cast(K.equal(0, input_w), 'int32')))\n",
    "            p_0 = K.gather(self.p[:,hash_fun_num], input_p)\n",
    "            p = K.expand_dims(p_0, -1)\n",
    "            pvals.append(p)\n",
    "            retvals.append(W0*p)\n",
    "        retval = self.aggregation_function(retvals)\n",
    "        if self.append_weight:\n",
    "            retval = K.concatenate([retval]+pvals,axis=-1)\n",
    "        return retval\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        weight_addition = 0\n",
    "        if self.append_weight:\n",
    "            weight_addition = self.num_hash_functions\n",
    "        if self.aggregation_mode == 'sum':\n",
    "            return (input_shape[0], input_shape[1], self.embedding_size+weight_addition)\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1], self.embedding_size * self.num_hash_functions + weight_addition)\n",
    "\n",
    "\n",
    "class ReduceSum(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "\n",
    "        super(ReduceSum, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x, m = x\n",
    "        x = x * K.cast(K.expand_dims(K.not_equal(m,0), -1), 'float32')\n",
    "        x = K.cast(x, 'float32')\n",
    "        return K.sum(x, axis=1,keepdims=False)\n",
    "\n",
    "    def compute_mask(self, input, mask=None):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "num_inp (InputLayer)            (None, 88)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_words (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 88)           352         num_inp[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hash_embedding_2 (HashEmbedding (None, None, 39)     2237375     input_words[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          11392       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_inp (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reduce_sum_2 (ReduceSum)        (None, 39)           0           hash_embedding_2[0][0]           \n",
      "                                                                 input_words[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 128)          512         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 100)          400         dense_inp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 39)           156         reduce_sum_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 100)          0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 39)           0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 267)          0           dropout_9[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         274432      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1024)         4096        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1024)         0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          524800      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 512)          2048        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 512)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          131328      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 256)          1024        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 256)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          32896       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128)          512         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 128)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           8256        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64)           256         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 64)           0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            65          dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,229,900\n",
      "Trainable params: 3,225,222\n",
      "Non-trainable params: 4,678\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def split_inputs(X):\n",
    "    return np.split(X, X.shape[-1], axis=-1)\n",
    "\n",
    "def buildBaseModel():   \n",
    "    num_inp = Input((len(train_cols),), name='num_inp')\n",
    "    num_x = BatchNormalization()(num_inp)\n",
    "    num_x = Dense(128, activation=\"relu\")(num_x)\n",
    "    num_x = BatchNormalization()(num_x)\n",
    "    num_x = Dropout(0.7)(num_x)\n",
    "    \n",
    "    dense_inp = Input((train_mat_pcat.shape[1],), name='dense_inp')\n",
    "    dense_x = BatchNormalization()(dense_inp)\n",
    "    dense_x = Dropout(0.7)(dense_x)\n",
    "\n",
    "    embedding = HashEmbedding()    \n",
    "    input_words = Input([None], dtype='int32', name='input_words')\n",
    "    sparse_x = embedding(input_words)\n",
    "    sparse_x = ReduceSum()([sparse_x, input_words])\n",
    "    sparse_x = BatchNormalization()(sparse_x)\n",
    "    #sparse_x = Dense(32, activation=\"relu\")(sparse_x)\n",
    "    #sparse_x = BatchNormalization()(sparse_x)\n",
    "    sparse_x = Dropout(0.7)(sparse_x)\n",
    "    \n",
    "    x = concatenate([num_x, dense_x, sparse_x]) #\n",
    "    \n",
    "    x1 = Dense(1024, activation=\"relu\")(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.6)(x1)\n",
    "    \n",
    "    x1 = Dense(512, activation=\"relu\")(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.6)(x1)\n",
    "    \n",
    "    x1 = Dense(256, activation=\"relu\")(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.6)(x1)\n",
    "    \n",
    "    x4 = Dense(128, activation=\"relu\")(x1)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Dropout(0.6)(x4)\n",
    "\n",
    "    x4 = Dense(64, activation=\"relu\")(x4)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Dropout(0.5)(x4)\n",
    "    \n",
    "    x_output = Dense(1, activation=\"sigmoid\", name=\"output\")(x4)\n",
    "    return Model(inputs = [num_inp, dense_inp, input_words], outputs=x_output)\n",
    "\n",
    "model = buildBaseModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "Epoch 1/60\n",
      " - 37s - loss: 0.0975\n",
      "val_auc: 0.48553972; _logloss: 0.55264953; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.48554, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 2/60\n",
      " - 33s - loss: 0.0807\n",
      "val_auc: 0.52631309; _logloss: 0.57766869; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.48554 to 0.52631, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 3/60\n",
      " - 33s - loss: 0.0747\n",
      "val_auc: 0.55541822; _logloss: 0.60882694; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.52631 to 0.55542, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 4/60\n",
      " - 34s - loss: 0.0730\n",
      "val_auc: 0.56135020; _logloss: 0.60781251; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.55542 to 0.56135, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 5/60\n",
      " - 34s - loss: 0.0727\n",
      "val_auc: 0.57104963; _logloss: 0.60567294; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.56135 to 0.57105, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 6/60\n",
      " - 34s - loss: 0.0723\n",
      "val_auc: 0.58177708; _logloss: 0.60545672; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.57105 to 0.58178, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 7/60\n",
      " - 34s - loss: 0.0718\n",
      "val_auc: 0.58827884; _logloss: 0.60608577; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.58178 to 0.58828, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 8/60\n",
      " - 34s - loss: 0.0715\n",
      "val_auc: 0.60576141; _logloss: 0.59821705; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.58828 to 0.60576, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 9/60\n",
      " - 34s - loss: 0.0709\n",
      "val_auc: 0.65023763; _logloss: 0.57635956; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.60576 to 0.65024, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 10/60\n",
      " - 33s - loss: 0.0690\n",
      "val_auc: 0.68399143; _logloss: 0.53167540; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.65024 to 0.68399, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 11/60\n",
      " - 33s - loss: 0.0665\n",
      "val_auc: 0.68844838; _logloss: 0.49531411; \n",
      "\n",
      "Epoch 00011: val_auc improved from 0.68399 to 0.68845, saving model to ./results/nn_we_v3__f0.h5\n",
      "Epoch 12/60\n",
      " - 34s - loss: 0.0638\n",
      "val_auc: 0.68548266; _logloss: 0.49640899; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 34s - loss: 0.0617\n",
      "val_auc: 0.67912120; _logloss: 0.47937230; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 14/60\n",
      " - 33s - loss: 0.0596\n",
      "val_auc: 0.67010816; _logloss: 0.44810133; \n",
      "\n",
      "Epoch 00014: val_auc did not improve\n",
      "Epoch 00014: early stopping\n",
      "\n",
      "Predicting fold 0\n",
      "fold: 0, auc: 0.6884483791716599\n",
      "fold: 0, logloss: 0.4953141053945848\n",
      "\n",
      "Training fold 1\n",
      "Epoch 1/60\n",
      " - 37s - loss: 0.0973\n",
      "val_auc: 0.51838652; _logloss: 0.56037640; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.51839, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 2/60\n",
      " - 33s - loss: 0.0807\n",
      "val_auc: 0.53860566; _logloss: 0.60498209; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.51839 to 0.53861, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 3/60\n",
      " - 33s - loss: 0.0746\n",
      "val_auc: 0.55967601; _logloss: 0.60573163; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.53861 to 0.55968, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 4/60\n",
      " - 34s - loss: 0.0728\n",
      "val_auc: 0.56178095; _logloss: 0.60849072; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.55968 to 0.56178, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 5/60\n",
      " - 34s - loss: 0.0726\n",
      "val_auc: 0.56649381; _logloss: 0.60556845; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.56178 to 0.56649, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 6/60\n",
      " - 34s - loss: 0.0721\n",
      "val_auc: 0.57603735; _logloss: 0.60177995; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.56649 to 0.57604, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 7/60\n",
      " - 34s - loss: 0.0712\n",
      "val_auc: 0.62834047; _logloss: 0.56884947; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.57604 to 0.62834, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 8/60\n",
      " - 33s - loss: 0.0694\n",
      "val_auc: 0.66301084; _logloss: 0.53293869; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.62834 to 0.66301, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 9/60\n",
      " - 33s - loss: 0.0678\n",
      "val_auc: 0.67314593; _logloss: 0.51923205; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.66301 to 0.67315, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 10/60\n",
      " - 33s - loss: 0.0663\n",
      "val_auc: 0.67650598; _logloss: 0.49947677; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.67315 to 0.67651, saving model to ./results/nn_we_v3__f1.h5\n",
      "Epoch 11/60\n",
      " - 33s - loss: 0.0633\n",
      "val_auc: 0.67024727; _logloss: 0.47838039; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 33s - loss: 0.0600\n",
      "val_auc: 0.66249664; _logloss: 0.45753543; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 33s - loss: 0.0573\n",
      "val_auc: 0.65695191; _logloss: 0.44223530; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 1\n",
      "fold: 1, auc: 0.6765059807695386\n",
      "fold: 1, logloss: 0.4994767707482527\n",
      "\n",
      "Training fold 2\n",
      "Epoch 1/60\n",
      " - 37s - loss: 0.0978\n",
      "val_auc: 0.52405206; _logloss: 0.49696996; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.52405, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 2/60\n",
      " - 33s - loss: 0.0810\n",
      "val_auc: 0.56159170; _logloss: 0.60364444; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.52405 to 0.56159, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 3/60\n",
      " - 33s - loss: 0.0745\n",
      "val_auc: 0.56512623; _logloss: 0.61350666; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.56159 to 0.56513, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 4/60\n",
      " - 34s - loss: 0.0729\n",
      "val_auc: 0.56687612; _logloss: 0.61566593; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.56513 to 0.56688, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 5/60\n",
      " - 34s - loss: 0.0725\n",
      "val_auc: 0.57010985; _logloss: 0.61560341; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.56688 to 0.57011, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 6/60\n",
      " - 33s - loss: 0.0722\n",
      "val_auc: 0.57953348; _logloss: 0.61246114; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.57011 to 0.57953, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 7/60\n",
      " - 33s - loss: 0.0712\n",
      "val_auc: 0.63358880; _logloss: 0.58075099; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.57953 to 0.63359, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 8/60\n",
      " - 34s - loss: 0.0693\n",
      "val_auc: 0.66478215; _logloss: 0.56275124; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.63359 to 0.66478, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 9/60\n",
      " - 33s - loss: 0.0677\n",
      "val_auc: 0.67382011; _logloss: 0.54193115; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.66478 to 0.67382, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 10/60\n",
      " - 34s - loss: 0.0662\n",
      "val_auc: 0.67525259; _logloss: 0.51408845; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.67382 to 0.67525, saving model to ./results/nn_we_v3__f2.h5\n",
      "Epoch 11/60\n",
      " - 34s - loss: 0.0633\n",
      "val_auc: 0.66677686; _logloss: 0.48490264; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 33s - loss: 0.0597\n",
      "val_auc: 0.65980008; _logloss: 0.46493562; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 33s - loss: 0.0571\n",
      "val_auc: 0.65266909; _logloss: 0.45955448; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 2\n",
      "fold: 2, auc: 0.6752525930048094\n",
      "fold: 2, logloss: 0.5140884445846333\n",
      "\n",
      "Training fold 3\n",
      "Epoch 1/60\n",
      " - 37s - loss: 0.0970\n",
      "val_auc: 0.53205959; _logloss: 0.55378150; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.53206, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 2/60\n",
      " - 33s - loss: 0.0809\n",
      "val_auc: 0.53802318; _logloss: 0.56560973; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.53206 to 0.53802, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 3/60\n",
      " - 34s - loss: 0.0745\n",
      "val_auc: 0.55792367; _logloss: 0.60499215; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.53802 to 0.55792, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 4/60\n",
      " - 34s - loss: 0.0732\n",
      "val_auc: 0.56414163; _logloss: 0.61355700; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.55792 to 0.56414, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 5/60\n",
      " - 33s - loss: 0.0728\n",
      "val_auc: 0.56544359; _logloss: 0.61488919; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.56414 to 0.56544, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 6/60\n",
      " - 33s - loss: 0.0724\n",
      "val_auc: 0.57156456; _logloss: 0.60800049; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.56544 to 0.57156, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 7/60\n",
      " - 33s - loss: 0.0718\n",
      "val_auc: 0.58821794; _logloss: 0.58226355; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.57156 to 0.58822, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 8/60\n",
      " - 34s - loss: 0.0710\n",
      "val_auc: 0.62792676; _logloss: 0.55630754; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.58822 to 0.62793, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 34s - loss: 0.0699\n",
      "val_auc: 0.65886940; _logloss: 0.52320862; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.62793 to 0.65887, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 10/60\n",
      " - 34s - loss: 0.0679\n",
      "val_auc: 0.67017801; _logloss: 0.51557934; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.65887 to 0.67018, saving model to ./results/nn_we_v3__f3.h5\n",
      "Epoch 11/60\n",
      " - 34s - loss: 0.0652\n",
      "val_auc: 0.66940031; _logloss: 0.48615981; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 34s - loss: 0.0621\n",
      "val_auc: 0.66449533; _logloss: 0.46041997; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 34s - loss: 0.0597\n",
      "val_auc: 0.65647097; _logloss: 0.44019393; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 3\n",
      "fold: 3, auc: 0.6701780072958545\n",
      "fold: 3, logloss: 0.5155793344661259\n",
      "\n",
      "Training fold 4\n",
      "Epoch 1/60\n",
      " - 38s - loss: 0.0971\n",
      "val_auc: 0.55346655; _logloss: 0.49942147; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.55347, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 2/60\n",
      " - 34s - loss: 0.0803\n",
      "val_auc: 0.54441421; _logloss: 0.57858655; \n",
      "\n",
      "Epoch 00002: val_auc did not improve\n",
      "Epoch 3/60\n",
      " - 34s - loss: 0.0745\n",
      "val_auc: 0.57536556; _logloss: 0.61130885; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.55347 to 0.57537, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 4/60\n",
      " - 35s - loss: 0.0730\n",
      "val_auc: 0.57859989; _logloss: 0.61279034; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.57537 to 0.57860, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 5/60\n",
      " - 34s - loss: 0.0725\n",
      "val_auc: 0.58226674; _logloss: 0.61444536; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.57860 to 0.58227, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 6/60\n",
      " - 35s - loss: 0.0721\n",
      "val_auc: 0.59107767; _logloss: 0.60004846; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.58227 to 0.59108, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 7/60\n",
      " - 35s - loss: 0.0715\n",
      "val_auc: 0.61844539; _logloss: 0.57783815; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.59108 to 0.61845, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 8/60\n",
      " - 35s - loss: 0.0702\n",
      "val_auc: 0.66128803; _logloss: 0.54944357; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.61845 to 0.66129, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 9/60\n",
      " - 35s - loss: 0.0689\n",
      "val_auc: 0.67767570; _logloss: 0.53754329; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.66129 to 0.67768, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 10/60\n",
      " - 35s - loss: 0.0671\n",
      "val_auc: 0.68378522; _logloss: 0.51792098; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.67768 to 0.68379, saving model to ./results/nn_we_v3__f4.h5\n",
      "Epoch 11/60\n",
      " - 39s - loss: 0.0644\n",
      "val_auc: 0.67559158; _logloss: 0.51569782; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 42s - loss: 0.0610\n",
      "val_auc: 0.66931642; _logloss: 0.46993266; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 42s - loss: 0.0585\n",
      "val_auc: 0.66488364; _logloss: 0.45633446; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 4\n",
      "fold: 4, auc: 0.6837852182793608\n",
      "fold: 4, logloss: 0.5179209761985342\n",
      "\n",
      "Training fold 5\n",
      "Epoch 1/60\n",
      " - 48s - loss: 0.0972\n",
      "val_auc: 0.52209732; _logloss: 0.52111584; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.52210, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 2/60\n",
      " - 42s - loss: 0.0806\n",
      "val_auc: 0.55172607; _logloss: 0.59467836; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.52210 to 0.55173, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 3/60\n",
      " - 39s - loss: 0.0745\n",
      "val_auc: 0.56637138; _logloss: 0.60199335; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.55173 to 0.56637, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 4/60\n",
      " - 42s - loss: 0.0728\n",
      "val_auc: 0.57836885; _logloss: 0.60726042; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.56637 to 0.57837, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 5/60\n",
      " - 42s - loss: 0.0726\n",
      "val_auc: 0.58138695; _logloss: 0.60336455; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.57837 to 0.58139, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 6/60\n",
      " - 42s - loss: 0.0722\n",
      "val_auc: 0.58771482; _logloss: 0.59899093; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.58139 to 0.58771, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 7/60\n",
      " - 42s - loss: 0.0711\n",
      "val_auc: 0.63888706; _logloss: 0.56811980; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.58771 to 0.63889, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 8/60\n",
      " - 42s - loss: 0.0689\n",
      "val_auc: 0.66004884; _logloss: 0.54085836; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.63889 to 0.66005, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 9/60\n",
      " - 42s - loss: 0.0675\n",
      "val_auc: 0.66933904; _logloss: 0.52596067; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.66005 to 0.66934, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 10/60\n",
      " - 41s - loss: 0.0657\n",
      "val_auc: 0.67084569; _logloss: 0.50453929; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.66934 to 0.67085, saving model to ./results/nn_we_v3__f5.h5\n",
      "Epoch 11/60\n",
      " - 41s - loss: 0.0628\n",
      "val_auc: 0.66427057; _logloss: 0.48600418; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 42s - loss: 0.0591\n",
      "val_auc: 0.65691599; _logloss: 0.45987563; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 42s - loss: 0.0564\n",
      "val_auc: 0.65071711; _logloss: 0.44805222; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 5\n",
      "fold: 5, auc: 0.6708456903818651\n",
      "fold: 5, logloss: 0.5045392855757959\n",
      "\n",
      "Training fold 6\n",
      "Epoch 1/60\n",
      " - 46s - loss: 0.0974\n",
      "val_auc: 0.50197145; _logloss: 0.54820064; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.50197, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 2/60\n",
      " - 42s - loss: 0.0804\n",
      "val_auc: 0.54674425; _logloss: 0.57797958; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.50197 to 0.54674, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 3/60\n",
      " - 42s - loss: 0.0744\n",
      "val_auc: 0.55943744; _logloss: 0.60273230; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.54674 to 0.55944, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 4/60\n",
      " - 43s - loss: 0.0729\n",
      "val_auc: 0.56640218; _logloss: 0.60870649; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.55944 to 0.56640, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 5/60\n",
      " - 39s - loss: 0.0724\n",
      "val_auc: 0.56658162; _logloss: 0.60649837; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.56640 to 0.56658, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 6/60\n",
      " - 42s - loss: 0.0719\n",
      "val_auc: 0.57147533; _logloss: 0.59464570; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.56658 to 0.57148, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 7/60\n",
      " - 42s - loss: 0.0714\n",
      "val_auc: 0.60598615; _logloss: 0.57806513; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.57148 to 0.60599, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 8/60\n",
      " - 40s - loss: 0.0700\n",
      "val_auc: 0.65135485; _logloss: 0.53814726; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.60599 to 0.65135, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 9/60\n",
      " - 42s - loss: 0.0686\n",
      "val_auc: 0.66957816; _logloss: 0.52082426; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.65135 to 0.66958, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 10/60\n",
      " - 42s - loss: 0.0669\n",
      "val_auc: 0.67685253; _logloss: 0.49175455; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.66958 to 0.67685, saving model to ./results/nn_we_v3__f6.h5\n",
      "Epoch 11/60\n",
      " - 40s - loss: 0.0642\n",
      "val_auc: 0.67659493; _logloss: 0.46511390; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 41s - loss: 0.0609\n",
      "val_auc: 0.66872290; _logloss: 0.43887276; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 42s - loss: 0.0587\n",
      "val_auc: 0.66213639; _logloss: 0.43041301; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 6\n",
      "fold: 6, auc: 0.6768525326111063\n",
      "fold: 6, logloss: 0.4917545516324726\n",
      "\n",
      "Training fold 7\n",
      "Epoch 1/60\n",
      " - 48s - loss: 0.0968\n",
      "val_auc: 0.53670986; _logloss: 0.52067671; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.53671, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 2/60\n",
      " - 39s - loss: 0.0807\n",
      "val_auc: 0.54434869; _logloss: 0.56362838; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.53671 to 0.54435, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 3/60\n",
      " - 41s - loss: 0.0744\n",
      "val_auc: 0.57987247; _logloss: 0.59702102; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.54435 to 0.57987, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 4/60\n",
      " - 41s - loss: 0.0728\n",
      "val_auc: 0.58108589; _logloss: 0.59908735; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.57987 to 0.58109, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 5/60\n",
      " - 42s - loss: 0.0725\n",
      "val_auc: 0.58241688; _logloss: 0.60217988; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.58109 to 0.58242, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 6/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 42s - loss: 0.0721\n",
      "val_auc: 0.59419158; _logloss: 0.58562540; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.58242 to 0.59419, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 7/60\n",
      " - 42s - loss: 0.0707\n",
      "val_auc: 0.65777308; _logloss: 0.55159588; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.59419 to 0.65777, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 8/60\n",
      " - 39s - loss: 0.0684\n",
      "val_auc: 0.67409644; _logloss: 0.52884824; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.65777 to 0.67410, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 9/60\n",
      " - 42s - loss: 0.0668\n",
      "val_auc: 0.68150503; _logloss: 0.51606579; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.67410 to 0.68151, saving model to ./results/nn_we_v3__f7.h5\n",
      "Epoch 10/60\n",
      " - 41s - loss: 0.0652\n",
      "val_auc: 0.68061563; _logloss: 0.47347408; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/60\n",
      " - 42s - loss: 0.0623\n",
      "val_auc: 0.67508321; _logloss: 0.45743295; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 39s - loss: 0.0585\n",
      "val_auc: 0.66546037; _logloss: 0.43475477; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Predicting fold 7\n",
      "fold: 7, auc: 0.6815050317054565\n",
      "fold: 7, logloss: 0.5160657889349063\n",
      "\n",
      "Training fold 8\n",
      "Epoch 1/60\n",
      " - 48s - loss: 0.0973\n",
      "val_auc: 0.53078177; _logloss: 0.50371435; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.53078, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 2/60\n",
      " - 42s - loss: 0.0807\n",
      "val_auc: 0.54159406; _logloss: 0.61384594; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.53078 to 0.54159, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 3/60\n",
      " - 43s - loss: 0.0745\n",
      "val_auc: 0.55256695; _logloss: 0.62609077; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.54159 to 0.55257, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 4/60\n",
      " - 33s - loss: 0.0729\n",
      "val_auc: 0.56051266; _logloss: 0.62041407; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.55257 to 0.56051, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 5/60\n",
      " - 34s - loss: 0.0725\n",
      "val_auc: 0.56395345; _logloss: 0.61385933; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.56051 to 0.56395, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 6/60\n",
      " - 34s - loss: 0.0720\n",
      "val_auc: 0.57359935; _logloss: 0.59722614; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.56395 to 0.57360, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 7/60\n",
      " - 34s - loss: 0.0710\n",
      "val_auc: 0.64395202; _logloss: 0.55839113; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.57360 to 0.64395, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 8/60\n",
      " - 34s - loss: 0.0690\n",
      "val_auc: 0.66401150; _logloss: 0.53721353; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.64395 to 0.66401, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 9/60\n",
      " - 34s - loss: 0.0673\n",
      "val_auc: 0.67434804; _logloss: 0.51825569; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.66401 to 0.67435, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 10/60\n",
      " - 34s - loss: 0.0655\n",
      "val_auc: 0.67459304; _logloss: 0.49601740; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.67435 to 0.67459, saving model to ./results/nn_we_v3__f8.h5\n",
      "Epoch 11/60\n",
      " - 34s - loss: 0.0626\n",
      "val_auc: 0.66469024; _logloss: 0.46027089; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 34s - loss: 0.0591\n",
      "val_auc: 0.66087423; _logloss: 0.46811849; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 34s - loss: 0.0564\n",
      "val_auc: 0.65411347; _logloss: 0.47025269; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 8\n",
      "fold: 8, auc: 0.6745930364993737\n",
      "fold: 8, logloss: 0.4960173965917634\n",
      "\n",
      "Training fold 9\n",
      "Epoch 1/60\n",
      " - 37s - loss: 0.0969\n",
      "val_auc: 0.51425888; _logloss: 0.50675047; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.51426, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 2/60\n",
      " - 33s - loss: 0.0808\n",
      "val_auc: 0.52229053; _logloss: 0.58526822; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.51426 to 0.52229, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 3/60\n",
      " - 33s - loss: 0.0746\n",
      "val_auc: 0.55225799; _logloss: 0.60402875; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.52229 to 0.55226, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 4/60\n",
      " - 34s - loss: 0.0730\n",
      "val_auc: 0.56439653; _logloss: 0.60257736; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.55226 to 0.56440, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 5/60\n",
      " - 34s - loss: 0.0727\n",
      "val_auc: 0.57335242; _logloss: 0.59704721; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.56440 to 0.57335, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 6/60\n",
      " - 34s - loss: 0.0722\n",
      "val_auc: 0.58344717; _logloss: 0.59394026; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.57335 to 0.58345, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 7/60\n",
      " - 34s - loss: 0.0715\n",
      "val_auc: 0.61457027; _logloss: 0.58077785; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.58345 to 0.61457, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 8/60\n",
      " - 34s - loss: 0.0702\n",
      "val_auc: 0.65824073; _logloss: 0.54768320; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.61457 to 0.65824, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 9/60\n",
      " - 34s - loss: 0.0687\n",
      "val_auc: 0.67118609; _logloss: 0.52505888; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.65824 to 0.67119, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 10/60\n",
      " - 34s - loss: 0.0668\n",
      "val_auc: 0.67577252; _logloss: 0.48614794; \n",
      "\n",
      "Epoch 00010: val_auc improved from 0.67119 to 0.67577, saving model to ./results/nn_we_v3__f9.h5\n",
      "Epoch 11/60\n",
      " - 34s - loss: 0.0640\n",
      "val_auc: 0.67272785; _logloss: 0.48264771; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/60\n",
      " - 34s - loss: 0.0608\n",
      "val_auc: 0.66682176; _logloss: 0.45258700; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/60\n",
      " - 34s - loss: 0.0584\n",
      "val_auc: 0.66212324; _logloss: 0.44930775; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 9\n",
      "fold: 9, auc: 0.6757725215751085\n",
      "fold: 9, logloss: 0.4861479443392487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "TRN_BATCH_SIZE = 512\n",
    "INF_BATCH_SIZE = 512\n",
    "\n",
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=31239)\n",
    "epochs = 60\n",
    "pred = np.zeros(y.shape)\n",
    "test_pred = 0\n",
    "ifold = 0\n",
    "\n",
    "fold_auc = []\n",
    "\n",
    "import gc\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "cpu_cores = 4\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha, gamma=0.5):\n",
    "    alpha = K.variable(alpha)\n",
    "    pt = K.abs(1. - y_true - y_pred)\n",
    "    pt = K.clip(pt, K.epsilon(), 1. - K.epsilon())\n",
    "    return K.mean(-alpha * K.pow(1. - pt, gamma) * K.log(pt), axis=-1)\n",
    "\n",
    "for trn_inx, val_inx in kf.split(y):\n",
    "    print(\"Training fold {}\".format(ifold))\n",
    "    K.clear_session()\n",
    "    \n",
    "    model_file_name = model_name+\"__f\"+str(ifold)\n",
    "    model_file = results_dir+model_file_name+'.h5'   \n",
    "        \n",
    "    model = buildBaseModel()\n",
    "    \n",
    "    trn_seq = FeatureSequence([X.loc[trn_inx,train_cols].values, \n",
    "                               train_mat_pcat,\n",
    "                               np.vstack(X.loc[trn_inx,'idcs'].values)\n",
    "                              ], \n",
    "                              y[trn_inx], np.array(list(range(len(trn_inx)))), TRN_BATCH_SIZE, shuffle=True)\n",
    "    val_seq = FeatureSequence([X.loc[val_inx,train_cols].values, \n",
    "                               train_mat_pcat[val_inx],\n",
    "                               np.vstack(X.loc[val_inx,'idcs'].values)\n",
    "                              ], \n",
    "                              y[val_inx], np.array(list(range(len(val_inx)))), INF_BATCH_SIZE, shuffle=False)\n",
    "    te_seq = FeatureSequence([x_te[train_cols].values, \n",
    "                               test_mat_pcat,\n",
    "                               np.vstack(x_te.idcs.values)\n",
    "                             ], \n",
    "                              y, np.array(list(range(len(x_te)))), INF_BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "    # Callbacks\n",
    "    model_checkpoint = ModelCheckpoint(model_file, monitor='val_auc', verbose=1, mode='max',\n",
    "                                       save_best_only=True, save_weights_only=False, period=1)\n",
    "    clr = CyclicLR(base_lr=0.00005, max_lr=0.0005, step_size=2*math.ceil(len(trn_seq)), mode='triangular2')\n",
    "    early_stop = EarlyStopping(monitor='val_auc', min_delta=0, patience=3, verbose=1, mode='max')\n",
    "    mse_eval = RocAucEvaluation(val_seq, y[val_inx], 'val')\n",
    "    \n",
    "    alpha = 10\n",
    "    gamma = 0.05\n",
    "    #alpha = 0.25\n",
    "    #gamma = 2\n",
    "    # Training\n",
    "    opt=optimizers.Nadam()\n",
    "    #model.compile(optimizer=opt, loss=lambda y_true, y_pred: focal_loss(y_true, y_pred, alpha=alpha, gamma=gamma))\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "\n",
    "    model.fit_generator(\n",
    "        generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "        initial_epoch=0, epochs=epochs, shuffle=False, verbose=2,\n",
    "        callbacks=[mse_eval, model_checkpoint, early_stop, clr], #\n",
    "        class_weight={0:0.06,1:0.94},\n",
    "        use_multiprocessing=False, workers=1, max_queue_size=4*cpu_cores)\n",
    "    \n",
    "     \n",
    "    # Predicting\n",
    "    print(\"\\nPredicting fold {}\".format(ifold))\n",
    "    del model  \n",
    "    model = load_model(model_file, compile=True, custom_objects={'HashEmbedding':HashEmbedding,'ReduceSum':ReduceSum})\n",
    "    pred[val_inx] = model.predict_generator(val_seq, steps=len(val_seq), \n",
    "                                                    use_multiprocessing=False, workers=1, \n",
    "                                                    max_queue_size=4*cpu_cores).ravel()\n",
    "    \n",
    "    auc = roc_auc_score(y[val_inx], pred[val_inx])\n",
    "    logloss = log_loss(y[val_inx], pred[val_inx])\n",
    "    fold_auc.append(auc)\n",
    "    print(\"fold: {}, auc: {}\".format(ifold, auc))\n",
    "    print(\"fold: {}, logloss: {}\".format(ifold, logloss))\n",
    "    print()\n",
    "    \n",
    "    test_pred += model.predict_generator(te_seq, steps=len(te_seq), \n",
    "                                                    use_multiprocessing=False, workers=1, \n",
    "                                                    max_queue_size=4*cpu_cores).ravel()/n_folds\n",
    "    ifold += 1\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68844837917165991, 0.67650598076953861, 0.67525259300480944, 0.67017800729585453, 0.68378521827936078, 0.67084569038186515, 0.67685253261110634, 0.68150503170545651, 0.67459303649937374, 0.67577252157510848]\n",
      "0.677373899129 0.00539477894431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67658314668821706"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fold_auc)\n",
    "print(np.mean(fold_auc), np.std(fold_auc))\n",
    "roc_auc_score(X.target.values, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(results_dir + 'train_' + model_name +'.npy', pred)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.514980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.468628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.559089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.362585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.217259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.514980\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.468628\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.559089\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.362585\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.217259"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = test_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
