{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sparse_tfidf'\n",
    "data_dir = './data/mlboot_dataset/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 609018/609018 [01:15<00:00, 8047.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "df = pd.read_csv(data_dir + 'preprocessed.csv') \n",
    "q = pd.read_csv(data_dir + 'sessions.csv')\n",
    "df = df.merge(q, on='uid', how='left')\n",
    "del q\n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index\n",
    "\n",
    "mat = sp.load_npz(data_dir+'dmat1.npz').tolil()\n",
    "#mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil()\n",
    "#mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil()\n",
    "\n",
    "#mat = sp.hstack([mat1,mat2,mat3]).tolil()\n",
    "#del mat1,mat2,mat3\n",
    "\n",
    "train_mat = mat[df_train_index.tolist()]\n",
    "test_mat = mat[df_test_index.tolist()]\n",
    "mat = mat.tocsc()[:, np.where((train_mat.getnnz(axis=0) > 0) & (test_mat.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "\n",
    "import tqdm\n",
    "idcs = []\n",
    "for q in tqdm.tqdm(range(len(df))):\n",
    "    idcs.append(mat[q].nonzero()[1] + 1)\n",
    "df['idcs'] = np.array(idcs)\n",
    "del idcs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "del mat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['idcs'] = df.idcs.apply(lambda x : ' '.join([str(q) for q in x.tolist()]))\n",
    "mat = TfidfVectorizer(max_features=5000).fit_transform(df.idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.53 s, sys: 140 ms, total: 2.67 s\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_mat = mat[df_train_index.tolist()]\n",
    "test_mat = mat[df_test_index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "(385194, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "0 0.667640045102\n",
      "[ 0.07349721  0.12928352  0.05150488 ...,  0.07837062  0.06115719\n",
      "  0.02823173]\n",
      "[ 0.40577469  0.45913654  0.38473815 ...,  0.41043631  0.39397097\n",
      "  0.36247645]\n",
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "/root/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare train\n",
      "(385194, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "1 0.659310694371\n",
      "[ 0.06992129  0.12683005  0.05159513 ...,  0.0388825   0.07637881\n",
      "  0.03104474]\n",
      "[ 0.39352646  0.44440091  0.37714351 ...,  0.36577887  0.39929926\n",
      "  0.35877218]\n",
      "fold 2\n",
      "prepare train\n",
      "(385194, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "2 0.656572926728\n",
      "[ 0.07236681  0.14677482  0.05314057 ...,  0.08369351  0.07338431\n",
      "  0.0324171 ]\n",
      "[ 0.38279577  0.45369657  0.36447576 ...,  0.39358859  0.38376531\n",
      "  0.34472909]\n",
      "fold 3\n",
      "prepare train\n",
      "(385194, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "3 0.668256747525\n",
      "[ 0.07156164  0.12614479  0.0488369  ...,  0.03938583  0.07105935\n",
      "  0.00964939]\n",
      "[ 0.41778379  0.46814681  0.39681602 ...,  0.38809567  0.41732033\n",
      "  0.36065833]\n",
      "fold 4\n",
      "prepare train\n",
      "(385195, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "4 0.66267845102\n",
      "[ 0.07003006  0.13374442  0.04710925 ...,  0.03885763  0.06714329\n",
      "  0.02437874]\n",
      "[ 0.28232381  0.34860959  0.25847794 ...,  0.2498933   0.27932053\n",
      "  0.23483007]\n",
      "fold 5\n",
      "prepare train\n",
      "(385195, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "5 0.665823806926\n",
      "[ 0.08287504  0.12985324  0.0465488  ...,  0.040325    0.06746377\n",
      "  0.02069513]\n",
      "[ 0.41367271  0.45726669  0.37996334 ...,  0.37418789  0.39937164\n",
      "  0.35597212]\n",
      "fold 6\n",
      "prepare train\n",
      "(385195, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "6 0.666189478762\n",
      "[ 0.07891533  0.13716836  0.0503414  ...,  0.0706005   0.07715259\n",
      "  0.02866818]\n",
      "[ 0.42094426  0.47277293  0.39552156 ...,  0.41354642  0.41937592\n",
      "  0.37623855]\n",
      "fold 7\n",
      "prepare train\n",
      "(385195, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "7 0.670473240914\n",
      "[ 0.07793691  0.13099295  0.05026191 ...,  0.07356912  0.07071578\n",
      "  0.02727443]\n",
      "[ 0.41900986  0.4688844   0.39299439 ...,  0.41490398  0.41222175\n",
      "  0.37138535]\n",
      "fold 8\n",
      "prepare train\n",
      "(385195, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "8 0.667492891831\n",
      "[ 0.07165995  0.12804233  0.04623725 ...,  0.04145821  0.0670204\n",
      "  0.01872816]\n",
      "[ 0.42582287  0.474337    0.40394795 ...,  0.39983583  0.42183078\n",
      "  0.3802778 ]\n",
      "fold 9\n",
      "prepare train\n",
      "(385195, 1047)\n",
      "prepare valid\n",
      "prepare test\n",
      "9 0.669649682197\n",
      "[ 0.07218745  0.13300571  0.04314722 ...,  0.07385309  0.07092262\n",
      "  0.02467743]\n",
      "[ 0.30045853  0.36582037  0.26924879 ...,  0.30224861  0.29909922\n",
      "  0.24939917]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier,VWRegressor\n",
    "from sklearn.linear_model import SGDRegressor,SGDClassifier,Ridge\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = [\n",
    "       'sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "       'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "       'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "       u'num_keys_f1',\n",
    "       u'sum_values_f1', u'num_keys_f2', u'sum_values_f2', u'num_keys_f3',\n",
    "       u'sum_values_f3', u'num_times_cat_eq_0', u'num_times_cat_eq_1',\n",
    "       u'num_times_cat_eq_2', u'num_times_cat_eq_3', u'num_times_cat_eq_4',\n",
    "       u'num_times_cat_eq_5', u'records', u'max_days', u'min_days',\n",
    "       u'sum_values_f1_std', u'num_keys_f1_std', u'sum_values_f2_std',\n",
    "       u'num_keys_f2_std', u'sum_values_f3_std', u'num_keys_f3_std',\n",
    "       u'sum_values_f1_max', u'num_keys_f1_max', u'sum_values_f2_max',\n",
    "       u'num_keys_f2_max', u'sum_values_f3_max', u'num_keys_f3_max',\n",
    "       u'sum_values_f1_mean', u'num_keys_f1_mean', u'sum_values_f2_mean',\n",
    "       u'num_keys_f2_mean', u'sum_values_f3_mean', u'num_keys_f3_mean'] \n",
    "    \n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    \n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols].fillna(0)\n",
    "    X_va = X_va[train_cols].fillna(0)\n",
    "    X_te = X_te[train_cols].fillna(0)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=20)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([\n",
    "        X_tr, sp_train_mat\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    print('prepare valid')\n",
    "    X_va = sp.hstack([\n",
    "        X_va, sp_val_mat\n",
    "    ]).tocsr()    \n",
    "    print('prepare test')\n",
    "    X_te = sp.hstack([\n",
    "        X_te, sp_test_mat\n",
    "    ]).tocsr()     \n",
    "    \n",
    "    model = Ridge()\n",
    "    model.fit(X_tr,y_tr)\n",
    "    \n",
    "    yhat = model.predict(X_va)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "   \n",
    "    ytst = model.predict(X_te)\n",
    "    print(ytst)\n",
    "    print(minmax_scale(ytst))\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    ifold += 1 \n",
    "#model = VWRegressor(power_t=0.99, ftrl=True, l1=15, l2=0.1)\n",
    "#0.678657863271\n",
    "#0.654927206221\n",
    "\n",
    "#0.8 perc\n",
    "# 0 0.681140205086\n",
    "# 1 0.664159676552\n",
    "# 2 0.661224538511\n",
    "# 3 0.671735521827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66764004510150232, 0.65931069437122747, 0.65657292672814449, 0.66825674752497444, 0.66267845102005618, 0.6658238069261444, 0.6661894787624616, 0.67047324091370886, 0.66749289183146332, 0.66964968219749332] 0.665408796538\n",
      "0.6653782259\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.567283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.375874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.436236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.363009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.351171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.567283\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.375874\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.436236\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.363009\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.351171"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'tfidfr_5folds_1'\n",
    "\n",
    "print(scores,np.mean(scores))\n",
    "print(roc_auc_score(X.target.values, y_oof.target.values))\n",
    "\n",
    "results_dir = './results/'\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = TfidfVectorizer().fit_transform(df.idcs)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "pca = TruncatedSVD(n_components = 300, algorithm='arpack')\n",
    "mat = pca.fit_transform(mat.astype(np.float32))\n",
    "train_mat = mat[df_train_index.tolist()]\n",
    "test_mat = mat[df_test_index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_dir+'svd_tfidf300.npy',mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "prepare valid\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's auc: 0.609881\n",
      "[100]\tvalid_0's auc: 0.626785\n",
      "[150]\tvalid_0's auc: 0.636711\n",
      "[200]\tvalid_0's auc: 0.645291\n",
      "[250]\tvalid_0's auc: 0.653108\n",
      "[300]\tvalid_0's auc: 0.659199\n",
      "[350]\tvalid_0's auc: 0.66384\n",
      "[400]\tvalid_0's auc: 0.667295\n",
      "[450]\tvalid_0's auc: 0.670719\n",
      "[500]\tvalid_0's auc: 0.67305\n",
      "[550]\tvalid_0's auc: 0.674532\n",
      "[600]\tvalid_0's auc: 0.676042\n",
      "[650]\tvalid_0's auc: 0.677376\n",
      "[700]\tvalid_0's auc: 0.678354\n",
      "[750]\tvalid_0's auc: 0.679278\n",
      "[800]\tvalid_0's auc: 0.679786\n",
      "[850]\tvalid_0's auc: 0.680553\n",
      "[900]\tvalid_0's auc: 0.681362\n",
      "[950]\tvalid_0's auc: 0.681892\n",
      "[1000]\tvalid_0's auc: 0.68238\n",
      "[1050]\tvalid_0's auc: 0.682756\n",
      "[1100]\tvalid_0's auc: 0.683065\n",
      "[1150]\tvalid_0's auc: 0.68333\n",
      "[1200]\tvalid_0's auc: 0.683592\n",
      "[1250]\tvalid_0's auc: 0.683563\n",
      "[1300]\tvalid_0's auc: 0.683724\n",
      "[1350]\tvalid_0's auc: 0.683775\n",
      "[1400]\tvalid_0's auc: 0.683777\n",
      "[1450]\tvalid_0's auc: 0.683716\n",
      "[1500]\tvalid_0's auc: 0.683929\n",
      "[1550]\tvalid_0's auc: 0.684021\n",
      "[1600]\tvalid_0's auc: 0.683991\n",
      "[1650]\tvalid_0's auc: 0.683928\n",
      "[1700]\tvalid_0's auc: 0.684055\n",
      "[1750]\tvalid_0's auc: 0.683843\n",
      "[1800]\tvalid_0's auc: 0.683592\n",
      "[1850]\tvalid_0's auc: 0.683317\n",
      "Early stopping, best iteration is:\n",
      "[1690]\tvalid_0's auc: 0.684101\n",
      "0 0.684033816489\n",
      "prepare test\n",
      "fold 1\n",
      "prepare train\n",
      "prepare valid\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's auc: 0.612161\n",
      "[100]\tvalid_0's auc: 0.623688\n",
      "[150]\tvalid_0's auc: 0.631552\n",
      "[200]\tvalid_0's auc: 0.639341\n",
      "[250]\tvalid_0's auc: 0.646148\n",
      "[300]\tvalid_0's auc: 0.652009\n",
      "[350]\tvalid_0's auc: 0.656283\n",
      "[400]\tvalid_0's auc: 0.660432\n",
      "[450]\tvalid_0's auc: 0.664152\n",
      "[500]\tvalid_0's auc: 0.666632\n",
      "[550]\tvalid_0's auc: 0.668476\n",
      "[600]\tvalid_0's auc: 0.670109\n",
      "[650]\tvalid_0's auc: 0.671744\n",
      "[700]\tvalid_0's auc: 0.673073\n",
      "[750]\tvalid_0's auc: 0.674357\n",
      "[800]\tvalid_0's auc: 0.675468\n",
      "[850]\tvalid_0's auc: 0.67623\n",
      "[900]\tvalid_0's auc: 0.677069\n",
      "[950]\tvalid_0's auc: 0.677711\n",
      "[1000]\tvalid_0's auc: 0.678265\n",
      "[1050]\tvalid_0's auc: 0.678711\n",
      "[1100]\tvalid_0's auc: 0.67923\n",
      "[1150]\tvalid_0's auc: 0.679469\n",
      "[1200]\tvalid_0's auc: 0.679888\n",
      "[1250]\tvalid_0's auc: 0.680132\n",
      "[1300]\tvalid_0's auc: 0.680532\n",
      "[1350]\tvalid_0's auc: 0.681052\n",
      "[1400]\tvalid_0's auc: 0.681291\n",
      "[1450]\tvalid_0's auc: 0.681327\n",
      "[1500]\tvalid_0's auc: 0.681588\n",
      "[1550]\tvalid_0's auc: 0.681533\n",
      "[1600]\tvalid_0's auc: 0.68169\n",
      "[1650]\tvalid_0's auc: 0.681626\n",
      "[1700]\tvalid_0's auc: 0.681682\n",
      "[1750]\tvalid_0's auc: 0.681592\n",
      "[1800]\tvalid_0's auc: 0.681577\n",
      "[1850]\tvalid_0's auc: 0.681487\n",
      "Early stopping, best iteration is:\n",
      "[1693]\tvalid_0's auc: 0.68175\n",
      "1 0.681675032089\n",
      "prepare test\n",
      "fold 2\n",
      "prepare train\n",
      "prepare valid\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's auc: 0.604192\n",
      "[100]\tvalid_0's auc: 0.617348\n",
      "[150]\tvalid_0's auc: 0.627984\n",
      "[200]\tvalid_0's auc: 0.638907\n",
      "[250]\tvalid_0's auc: 0.646968\n",
      "[300]\tvalid_0's auc: 0.653531\n",
      "[350]\tvalid_0's auc: 0.658234\n",
      "[400]\tvalid_0's auc: 0.662888\n",
      "[450]\tvalid_0's auc: 0.666797\n",
      "[500]\tvalid_0's auc: 0.66924\n",
      "[550]\tvalid_0's auc: 0.671544\n",
      "[600]\tvalid_0's auc: 0.67292\n",
      "[650]\tvalid_0's auc: 0.67467\n",
      "[700]\tvalid_0's auc: 0.675768\n",
      "[750]\tvalid_0's auc: 0.67663\n",
      "[800]\tvalid_0's auc: 0.677476\n",
      "[850]\tvalid_0's auc: 0.678056\n",
      "[900]\tvalid_0's auc: 0.678453\n",
      "[950]\tvalid_0's auc: 0.679012\n",
      "[1000]\tvalid_0's auc: 0.679239\n",
      "[1050]\tvalid_0's auc: 0.679645\n",
      "[1100]\tvalid_0's auc: 0.679989\n",
      "[1150]\tvalid_0's auc: 0.680199\n",
      "[1200]\tvalid_0's auc: 0.680583\n",
      "[1250]\tvalid_0's auc: 0.68078\n",
      "[1300]\tvalid_0's auc: 0.68092\n",
      "[1350]\tvalid_0's auc: 0.68095\n",
      "[1400]\tvalid_0's auc: 0.681151\n",
      "[1450]\tvalid_0's auc: 0.681191\n",
      "[1500]\tvalid_0's auc: 0.681282\n",
      "[1550]\tvalid_0's auc: 0.681333\n",
      "[1600]\tvalid_0's auc: 0.681378\n",
      "[1650]\tvalid_0's auc: 0.681561\n",
      "[1700]\tvalid_0's auc: 0.681496\n",
      "[1750]\tvalid_0's auc: 0.681467\n",
      "[1800]\tvalid_0's auc: 0.681467\n",
      "[1850]\tvalid_0's auc: 0.681459\n",
      "[1900]\tvalid_0's auc: 0.681368\n",
      "Early stopping, best iteration is:\n",
      "[1718]\tvalid_0's auc: 0.681612\n",
      "2 0.681526385384\n",
      "prepare test\n",
      "fold 3\n",
      "prepare train\n",
      "prepare valid\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's auc: 0.622087\n",
      "[100]\tvalid_0's auc: 0.630289\n",
      "[150]\tvalid_0's auc: 0.639584\n",
      "[200]\tvalid_0's auc: 0.648033\n",
      "[250]\tvalid_0's auc: 0.653934\n",
      "[300]\tvalid_0's auc: 0.65925\n",
      "[350]\tvalid_0's auc: 0.663326\n",
      "[400]\tvalid_0's auc: 0.666937\n",
      "[450]\tvalid_0's auc: 0.67021\n",
      "[500]\tvalid_0's auc: 0.672366\n",
      "[550]\tvalid_0's auc: 0.673956\n",
      "[600]\tvalid_0's auc: 0.67543\n",
      "[650]\tvalid_0's auc: 0.677268\n",
      "[700]\tvalid_0's auc: 0.678471\n",
      "[750]\tvalid_0's auc: 0.679367\n",
      "[800]\tvalid_0's auc: 0.680156\n",
      "[850]\tvalid_0's auc: 0.681019\n",
      "[900]\tvalid_0's auc: 0.6815\n",
      "[950]\tvalid_0's auc: 0.682075\n",
      "[1000]\tvalid_0's auc: 0.682695\n",
      "[1050]\tvalid_0's auc: 0.683075\n",
      "[1100]\tvalid_0's auc: 0.683457\n",
      "[1150]\tvalid_0's auc: 0.683695\n",
      "[1200]\tvalid_0's auc: 0.684035\n",
      "[1250]\tvalid_0's auc: 0.684065\n",
      "[1300]\tvalid_0's auc: 0.684429\n",
      "[1350]\tvalid_0's auc: 0.684634\n",
      "[1400]\tvalid_0's auc: 0.684758\n",
      "[1450]\tvalid_0's auc: 0.684872\n",
      "[1500]\tvalid_0's auc: 0.685045\n",
      "[1550]\tvalid_0's auc: 0.685207\n",
      "[1600]\tvalid_0's auc: 0.685088\n",
      "[1650]\tvalid_0's auc: 0.685186\n",
      "[1700]\tvalid_0's auc: 0.68521\n",
      "[1750]\tvalid_0's auc: 0.685285\n",
      "[1800]\tvalid_0's auc: 0.684946\n",
      "[1850]\tvalid_0's auc: 0.684836\n",
      "[1900]\tvalid_0's auc: 0.684792\n",
      "Early stopping, best iteration is:\n",
      "[1726]\tvalid_0's auc: 0.685344\n",
      "3 0.685236206659\n",
      "prepare test\n",
      "fold 4\n",
      "prepare train\n",
      "prepare valid\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's auc: 0.614981\n",
      "[100]\tvalid_0's auc: 0.627679\n",
      "[150]\tvalid_0's auc: 0.634555\n",
      "[200]\tvalid_0's auc: 0.642154\n",
      "[250]\tvalid_0's auc: 0.648785\n",
      "[300]\tvalid_0's auc: 0.654052\n",
      "[350]\tvalid_0's auc: 0.658355\n",
      "[400]\tvalid_0's auc: 0.661697\n",
      "[450]\tvalid_0's auc: 0.664684\n",
      "[500]\tvalid_0's auc: 0.666987\n",
      "[550]\tvalid_0's auc: 0.669214\n",
      "[600]\tvalid_0's auc: 0.671097\n",
      "[650]\tvalid_0's auc: 0.672565\n",
      "[700]\tvalid_0's auc: 0.673638\n",
      "[750]\tvalid_0's auc: 0.674373\n",
      "[800]\tvalid_0's auc: 0.675325\n",
      "[850]\tvalid_0's auc: 0.676325\n",
      "[900]\tvalid_0's auc: 0.676888\n",
      "[950]\tvalid_0's auc: 0.677592\n",
      "[1000]\tvalid_0's auc: 0.677912\n",
      "[1050]\tvalid_0's auc: 0.678139\n",
      "[1100]\tvalid_0's auc: 0.678398\n",
      "[1150]\tvalid_0's auc: 0.678742\n",
      "[1200]\tvalid_0's auc: 0.678971\n",
      "[1250]\tvalid_0's auc: 0.679133\n",
      "[1300]\tvalid_0's auc: 0.679118\n",
      "[1350]\tvalid_0's auc: 0.679256\n",
      "[1400]\tvalid_0's auc: 0.679347\n",
      "[1450]\tvalid_0's auc: 0.679449\n",
      "[1500]\tvalid_0's auc: 0.679451\n",
      "[1550]\tvalid_0's auc: 0.679551\n",
      "[1600]\tvalid_0's auc: 0.679625\n",
      "[1650]\tvalid_0's auc: 0.679738\n",
      "[1700]\tvalid_0's auc: 0.679611\n",
      "[1750]\tvalid_0's auc: 0.679755\n",
      "[1800]\tvalid_0's auc: 0.679838\n",
      "[1850]\tvalid_0's auc: 0.6798\n",
      "[1900]\tvalid_0's auc: 0.679804\n"
     ]
    }
   ],
   "source": [
    "train_cols = [\n",
    "       'sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "       'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "       'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "       u'num_keys_f1',\n",
    "       u'sum_values_f1', u'num_keys_f2', u'sum_values_f2', u'num_keys_f3',\n",
    "       u'sum_values_f3', u'num_times_cat_eq_0', u'num_times_cat_eq_1',\n",
    "       u'num_times_cat_eq_2', u'num_times_cat_eq_3', u'num_times_cat_eq_4',\n",
    "       u'num_times_cat_eq_5', u'records', u'max_days', u'min_days',\n",
    "       u'sum_values_f1_std', u'num_keys_f1_std', u'sum_values_f2_std',\n",
    "       u'num_keys_f2_std', u'sum_values_f3_std', u'num_keys_f3_std',\n",
    "       u'sum_values_f1_max', u'num_keys_f1_max', u'sum_values_f2_max',\n",
    "       u'num_keys_f2_max', u'sum_values_f3_max', u'num_keys_f3_max',\n",
    "       u'sum_values_f1_mean', u'num_keys_f1_mean', u'sum_values_f2_mean',\n",
    "       u'num_keys_f2_mean', u'sum_values_f3_mean', u'num_keys_f3_mean']\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 128,\n",
    "    #'max_depth' : 12,\n",
    "    #'min_data' : 30,\n",
    "    'lambda_l2' : 41.5,\n",
    "    #'min_sum_hessian_in_leaf' : 0.3,\n",
    "    'max_bin': 64,\n",
    "    #'max_drop' : 1,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.27,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):       \n",
    "    print('fold', ifold)\n",
    "    \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va = X.loc[train_index, train_cols].values,X.loc[test_index, train_cols].values\n",
    "    X_te = x_te[train_cols]\n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = np.hstack([\n",
    "        X_tr, train_mat[train_index]\n",
    "    ])\n",
    "    print('prepare valid')\n",
    "    X_va = np.hstack([\n",
    "        X_va, train_mat[test_index]\n",
    "    ])\n",
    "    \n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = lgb.Dataset(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = lgb.Dataset(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "\n",
    "    model = lgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      valid_sets=va_data,\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=200,\n",
    "                      verbose_eval=50)\n",
    "    \n",
    "    yhat = model.predict(X_va, model.best_iteration)\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    X_te = np.hstack([\n",
    "        X_te, test_mat\n",
    "    ])\n",
    "    \n",
    "    ytst = model.predict(X_te, model.best_iteration)\n",
    "    y_pred += ytst*0.1\n",
    "    \n",
    "    ifold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tfifg_svd_5folds'\n",
    "\n",
    "results_dir = './results/'\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
