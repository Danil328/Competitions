{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/mlboot_dataset/'\n",
    "model_name = 'j1'\n",
    "results_dir = './results/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 2053602) (609018, 20275) (609018, 1057788)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir + 'preprocessed_new.csv') \n",
    "q = pd.read_csv(data_dir + 'sessions.csv')\n",
    "df = df.merge(q, on='uid', how='left')\n",
    "del q\n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index\n",
    "\n",
    "mat1 = sp.load_npz(data_dir+'dmat1.npz').tolil()\n",
    "mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil()\n",
    "mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil()\n",
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_f1'] = mat1.tocsr().max(axis=1).todense()\n",
    "df['max_f2'] = mat2.tocsr().max(axis=1).todense()\n",
    "df['max_f3'] = mat3.tocsr().max(axis=1).todense()\n",
    "\n",
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "limit = 11\n",
    "mat1 = mat1.tocsc()[:, np.where((train_mat1.getnnz(axis=0) > limit) & (test_mat1.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat2 = mat2.tocsc()[:, np.where((train_mat2.getnnz(axis=0) > limit) & (test_mat2.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat3 = mat3.tocsc()[:, np.where((train_mat3.getnnz(axis=0) > limit) & (test_mat3.getnnz(axis=0) > 0))[0]].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 195734) (609018, 20268) (609018, 9415)\n"
     ]
    }
   ],
   "source": [
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'pca_cat10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_description_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'bin_pca_dim10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_title_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "del mat1,mat2,mat3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
       "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
       "       'records', 'max_days', 'min_days',\n",
       "       ...\n",
       "       'svd_title_1', 'svd_title_2', 'svd_title_3', 'svd_title_4',\n",
       "       'svd_title_5', 'svd_title_6', 'svd_title_7', 'svd_title_8',\n",
       "       'svd_title_9', 'svd_title_10'],\n",
       "      dtype='object', length=107)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(data_dir + 'train_meta.csv')\n",
    "X = X.merge(meta, on='uid', how='left')\n",
    "\n",
    "meta = pd.read_csv(data_dir + 'test_meta.csv')\n",
    "x_te = x_te.merge(meta, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr()\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = test_mat.sum(axis=0)\n",
    "ixs = np.asarray(mat)[0].argsort()[-750:][::-1]\n",
    "train_mat = train_mat[:,ixs]\n",
    "test_mat = test_mat[:,ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    478079\n",
       "2    120192\n",
       "3     10459\n",
       "4       285\n",
       "5         3\n",
       "Name: diff_num_cats, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diff_num_cats.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    329708\n",
       "2    166519\n",
       "1     47177\n",
       "0     38653\n",
       "3     22501\n",
       "4      4460\n",
       "Name: most_freq_cat, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.most_freq_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "(385194, 345)\n",
      "[0]\ttrain-auc:0.689925\tvalid-auc:0.701331\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.697817\tvalid-auc:0.708615\n",
      "[200]\ttrain-auc:0.698516\tvalid-auc:0.708747\n",
      "[300]\ttrain-auc:0.699302\tvalid-auc:0.708698\n",
      "Stopping. Best iteration:\n",
      "[13]\ttrain-auc:0.696628\tvalid-auc:0.709409\n",
      "\n",
      "0 0.708711190124\n",
      "[ 0.6627304   0.26411811  0.01822019 ...,  0.09749877  0.04546159\n",
      " -0.6171779 ]\n",
      "[ 0.73890769  0.57077128  0.46705046 ...,  0.50049049  0.47854099\n",
      "  0.19903675]\n",
      "fold 1\n",
      "(385194, 345)\n",
      "[0]\ttrain-auc:0.693322\tvalid-auc:0.680544\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.700037\tvalid-auc:0.687988\n",
      "[200]\ttrain-auc:0.700823\tvalid-auc:0.688118\n",
      "[300]\ttrain-auc:0.701623\tvalid-auc:0.688279\n",
      "[400]\ttrain-auc:0.702404\tvalid-auc:0.688349\n",
      "[500]\ttrain-auc:0.703187\tvalid-auc:0.688367\n",
      "[600]\ttrain-auc:0.704024\tvalid-auc:0.688499\n",
      "[700]\ttrain-auc:0.704901\tvalid-auc:0.688512\n",
      "[800]\ttrain-auc:0.705845\tvalid-auc:0.688514\n",
      "[900]\ttrain-auc:0.706772\tvalid-auc:0.688549\n",
      "[1000]\ttrain-auc:0.707726\tvalid-auc:0.68861\n",
      "[1100]\ttrain-auc:0.708667\tvalid-auc:0.688638\n",
      "[1200]\ttrain-auc:0.709576\tvalid-auc:0.688661\n",
      "[1300]\ttrain-auc:0.710461\tvalid-auc:0.68864\n",
      "Stopping. Best iteration:\n",
      "[1086]\ttrain-auc:0.708531\tvalid-auc:0.688677\n",
      "\n",
      "1 0.688640750821\n",
      "[ 0.95508385  0.36766687 -0.0109221  ...,  0.13382483  0.06891146\n",
      " -1.03245449]\n",
      "[ 0.70695513  0.57882303  0.4962422  ...,  0.52781558  0.51365614\n",
      "  0.27341741]\n",
      "fold 2\n",
      "(385194, 345)\n",
      "[0]\ttrain-auc:0.692289\tvalid-auc:0.674281\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.70046\tvalid-auc:0.682706\n",
      "[200]\ttrain-auc:0.701238\tvalid-auc:0.683111\n",
      "[300]\ttrain-auc:0.702034\tvalid-auc:0.68354\n",
      "[400]\ttrain-auc:0.702817\tvalid-auc:0.683724\n",
      "[500]\ttrain-auc:0.703631\tvalid-auc:0.68376\n",
      "[600]\ttrain-auc:0.704475\tvalid-auc:0.683826\n",
      "[700]\ttrain-auc:0.705335\tvalid-auc:0.68389\n",
      "[800]\ttrain-auc:0.706288\tvalid-auc:0.683887\n",
      "[900]\ttrain-auc:0.707217\tvalid-auc:0.683815\n",
      "Stopping. Best iteration:\n",
      "[685]\ttrain-auc:0.705187\tvalid-auc:0.683929\n",
      "\n",
      "2 0.683823328903\n",
      "[ 0.95279831  0.37428552 -0.04263382 ...,  0.08652858  0.02143381\n",
      " -0.98846495]\n",
      "[ 0.69463187  0.55759209  0.45883104 ...,  0.48942742  0.47400758\n",
      "  0.23477986]\n",
      "fold 3\n",
      "(385194, 345)\n",
      "[0]\ttrain-auc:0.690825\tvalid-auc:0.696006\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.698632\tvalid-auc:0.701661\n",
      "[200]\ttrain-auc:0.699327\tvalid-auc:0.701869\n",
      "[300]\ttrain-auc:0.700099\tvalid-auc:0.70204\n",
      "[400]\ttrain-auc:0.700879\tvalid-auc:0.702239\n",
      "[500]\ttrain-auc:0.701668\tvalid-auc:0.702428\n",
      "[600]\ttrain-auc:0.70246\tvalid-auc:0.702498\n",
      "[700]\ttrain-auc:0.703285\tvalid-auc:0.702606\n",
      "[800]\ttrain-auc:0.704194\tvalid-auc:0.702671\n",
      "[900]\ttrain-auc:0.705115\tvalid-auc:0.702779\n",
      "[1000]\ttrain-auc:0.706024\tvalid-auc:0.702825\n",
      "[1100]\ttrain-auc:0.706915\tvalid-auc:0.702856\n",
      "[1200]\ttrain-auc:0.707865\tvalid-auc:0.702879\n",
      "[1300]\ttrain-auc:0.708731\tvalid-auc:0.702862\n",
      "[1400]\ttrain-auc:0.709606\tvalid-auc:0.702901\n",
      "[1500]\ttrain-auc:0.710563\tvalid-auc:0.702909\n",
      "[1600]\ttrain-auc:0.711517\tvalid-auc:0.702902\n",
      "[1700]\ttrain-auc:0.712392\tvalid-auc:0.702871\n",
      "Stopping. Best iteration:\n",
      "[1436]\ttrain-auc:0.70995\tvalid-auc:0.702941\n",
      "\n",
      "3 0.702838903975\n",
      "[ 1.02682805  0.44374809 -0.01645126 ...,  0.13556175 -0.15445027\n",
      " -0.97829163]\n",
      "[ 0.73201036  0.61195838  0.51720655 ...,  0.54850495  0.48879355\n",
      "  0.31917045]\n",
      "fold 4\n",
      "(385195, 345)\n",
      "[0]\ttrain-auc:0.691273\tvalid-auc:0.684086\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.699549\tvalid-auc:0.691222\n",
      "[200]\ttrain-auc:0.700344\tvalid-auc:0.691565\n",
      "[300]\ttrain-auc:0.701112\tvalid-auc:0.691799\n",
      "[400]\ttrain-auc:0.701924\tvalid-auc:0.691862\n",
      "[500]\ttrain-auc:0.702745\tvalid-auc:0.691988\n",
      "[600]\ttrain-auc:0.703593\tvalid-auc:0.692035\n",
      "[700]\ttrain-auc:0.70446\tvalid-auc:0.692056\n",
      "[800]\ttrain-auc:0.705396\tvalid-auc:0.692014\n",
      "[900]\ttrain-auc:0.706358\tvalid-auc:0.69193\n",
      "[1000]\ttrain-auc:0.707265\tvalid-auc:0.691848\n",
      "Stopping. Best iteration:\n",
      "[721]\ttrain-auc:0.70466\tvalid-auc:0.692078\n",
      "\n",
      "4 0.691813245833\n",
      "[ 0.93353808  0.35305449  0.07365058 ...,  0.02084151  0.03981818\n",
      " -0.96681195]\n",
      "[ 0.69613063  0.56539863  0.50247341 ...,  0.4905802   0.49485397\n",
      "  0.26814848]\n",
      "fold 5\n",
      "(385195, 345)\n",
      "[0]\ttrain-auc:0.691765\tvalid-auc:0.686847\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.699093\tvalid-auc:0.694507\n",
      "[200]\ttrain-auc:0.699917\tvalid-auc:0.694742\n",
      "[300]\ttrain-auc:0.700801\tvalid-auc:0.695043\n",
      "[400]\ttrain-auc:0.701577\tvalid-auc:0.695121\n",
      "[500]\ttrain-auc:0.70242\tvalid-auc:0.695143\n",
      "[600]\ttrain-auc:0.703287\tvalid-auc:0.69511\n",
      "[700]\ttrain-auc:0.704163\tvalid-auc:0.695114\n",
      "Stopping. Best iteration:\n",
      "[430]\ttrain-auc:0.701833\tvalid-auc:0.695172\n",
      "\n",
      "5 0.695114521515\n",
      "[ 0.95342177  0.36382523  0.00152878 ...,  0.13141811  0.08975889\n",
      " -0.88190299]\n",
      "[ 0.70227581  0.54984814  0.45618409 ...,  0.48976421  0.4789941\n",
      "  0.22779156]\n",
      "fold 6\n",
      "(385195, 345)\n",
      "[0]\ttrain-auc:0.691726\tvalid-auc:0.684007\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.699718\tvalid-auc:0.690069\n",
      "[200]\ttrain-auc:0.70053\tvalid-auc:0.690396\n",
      "[300]\ttrain-auc:0.70135\tvalid-auc:0.690571\n",
      "[400]\ttrain-auc:0.702172\tvalid-auc:0.69063\n",
      "[500]\ttrain-auc:0.70299\tvalid-auc:0.690696\n",
      "[600]\ttrain-auc:0.703812\tvalid-auc:0.690773\n",
      "[700]\ttrain-auc:0.704649\tvalid-auc:0.690851\n",
      "[800]\ttrain-auc:0.705559\tvalid-auc:0.69087\n",
      "[900]\ttrain-auc:0.706523\tvalid-auc:0.69095\n",
      "[1000]\ttrain-auc:0.707427\tvalid-auc:0.690998\n",
      "[1100]\ttrain-auc:0.708312\tvalid-auc:0.691101\n",
      "[1200]\ttrain-auc:0.709213\tvalid-auc:0.691136\n",
      "[1300]\ttrain-auc:0.710138\tvalid-auc:0.691209\n",
      "[1400]\ttrain-auc:0.711045\tvalid-auc:0.691261\n",
      "[1500]\ttrain-auc:0.711965\tvalid-auc:0.691267\n",
      "[1600]\ttrain-auc:0.712862\tvalid-auc:0.691245\n",
      "[1700]\ttrain-auc:0.713779\tvalid-auc:0.691197\n",
      "Stopping. Best iteration:\n",
      "[1438]\ttrain-auc:0.711393\tvalid-auc:0.691287\n",
      "\n",
      "6 0.691233842192\n",
      "[ 1.03362322  0.35405198 -0.05492963 ...,  0.09053959 -0.11928685\n",
      " -1.10073864]\n",
      "[ 0.73309815  0.58878022  0.50192642 ...,  0.53281915  0.48825911\n",
      "  0.27983201]\n",
      "fold 7\n",
      "(385195, 345)\n",
      "[0]\ttrain-auc:0.690253\tvalid-auc:0.696472\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.698018\tvalid-auc:0.70432\n",
      "[200]\ttrain-auc:0.698816\tvalid-auc:0.704621\n",
      "[300]\ttrain-auc:0.699637\tvalid-auc:0.704773\n",
      "[400]\ttrain-auc:0.700502\tvalid-auc:0.704995\n",
      "[500]\ttrain-auc:0.701303\tvalid-auc:0.705156\n",
      "[600]\ttrain-auc:0.702156\tvalid-auc:0.705253\n",
      "[700]\ttrain-auc:0.703016\tvalid-auc:0.705306\n",
      "[800]\ttrain-auc:0.703959\tvalid-auc:0.705353\n",
      "[900]\ttrain-auc:0.704937\tvalid-auc:0.705399\n",
      "[1000]\ttrain-auc:0.705885\tvalid-auc:0.705423\n",
      "[1100]\ttrain-auc:0.706795\tvalid-auc:0.705465\n",
      "[1200]\ttrain-auc:0.707698\tvalid-auc:0.705484\n",
      "[1300]\ttrain-auc:0.70863\tvalid-auc:0.705449\n",
      "[1400]\ttrain-auc:0.709547\tvalid-auc:0.7054\n",
      "Stopping. Best iteration:\n",
      "[1193]\ttrain-auc:0.70764\tvalid-auc:0.705504\n",
      "\n",
      "7 0.705417130659\n",
      "[ 0.97577631  0.36884111  0.03400956 ...,  0.13483916  0.02207641\n",
      " -1.05334997]\n",
      "[ 0.71113133  0.57954627  0.50695395 ...,  0.52881408  0.50436682\n",
      "  0.27121171]\n",
      "fold 8\n",
      "(385195, 345)\n",
      "[0]\ttrain-auc:0.690881\tvalid-auc:0.686215\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.699044\tvalid-auc:0.693484\n",
      "[200]\ttrain-auc:0.699872\tvalid-auc:0.694103\n",
      "[300]\ttrain-auc:0.700695\tvalid-auc:0.694435\n",
      "[400]\ttrain-auc:0.701507\tvalid-auc:0.694719\n",
      "[500]\ttrain-auc:0.702325\tvalid-auc:0.694851\n",
      "[600]\ttrain-auc:0.703128\tvalid-auc:0.695009\n",
      "[700]\ttrain-auc:0.703967\tvalid-auc:0.695074\n",
      "[800]\ttrain-auc:0.704884\tvalid-auc:0.69522\n",
      "[900]\ttrain-auc:0.705839\tvalid-auc:0.695363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain-auc:0.706755\tvalid-auc:0.695369\n",
      "[1100]\ttrain-auc:0.707606\tvalid-auc:0.695418\n",
      "[1200]\ttrain-auc:0.708499\tvalid-auc:0.695383\n",
      "[1300]\ttrain-auc:0.709373\tvalid-auc:0.695371\n",
      "Stopping. Best iteration:\n",
      "[1061]\ttrain-auc:0.707269\tvalid-auc:0.695438\n",
      "\n",
      "8 0.695348593841\n",
      "[ 1.01451731  0.38997546  0.03274715 ...,  0.0912283  -0.06479736\n",
      " -1.0743525 ]\n",
      "[ 0.71518785  0.5774399   0.49865022 ...,  0.51154876  0.47713596\n",
      "  0.25447014]\n",
      "fold 9\n",
      "(385195, 345)\n",
      "[0]\ttrain-auc:0.691836\tvalid-auc:0.69\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 300 rounds.\n",
      "[100]\ttrain-auc:0.698889\tvalid-auc:0.695608\n",
      "[200]\ttrain-auc:0.699688\tvalid-auc:0.696183\n",
      "[300]\ttrain-auc:0.700487\tvalid-auc:0.696538\n",
      "[400]\ttrain-auc:0.701281\tvalid-auc:0.696862\n",
      "[500]\ttrain-auc:0.702058\tvalid-auc:0.696989\n",
      "[600]\ttrain-auc:0.702873\tvalid-auc:0.697149\n",
      "[700]\ttrain-auc:0.703735\tvalid-auc:0.697296\n",
      "[800]\ttrain-auc:0.704634\tvalid-auc:0.697382\n",
      "[900]\ttrain-auc:0.705536\tvalid-auc:0.697486\n",
      "[1000]\ttrain-auc:0.706453\tvalid-auc:0.697449\n",
      "[1100]\ttrain-auc:0.707328\tvalid-auc:0.69748\n",
      "[1200]\ttrain-auc:0.708232\tvalid-auc:0.697486\n",
      "[1300]\ttrain-auc:0.709124\tvalid-auc:0.697442\n",
      "[1400]\ttrain-auc:0.709983\tvalid-auc:0.697372\n",
      "Stopping. Best iteration:\n",
      "[1127]\ttrain-auc:0.707565\tvalid-auc:0.697509\n",
      "\n",
      "9 0.697354289629\n",
      "[ 0.99839067  0.40337887 -0.0017067  ...,  0.10375144  0.01403016\n",
      " -0.99558449]\n",
      "[ 0.72182035  0.59612811  0.51055646 ...,  0.53283381  0.51388079\n",
      "  0.30060643]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat','diff_num_cats']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_2', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'unique_days','max_f1','max_f2',\n",
    "              'svd_description_1','svd_description_2','svd_description_3',\n",
    "              'svd_description_4','svd_description_5','svd_description_6',\n",
    "              'svd_description_7','svd_description_8','svd_description_9',\n",
    "              'most_freq_cat_te'] + ['svd_title_'+str(i+1) for i in range(9)] + meta.drop(['uid'], axis=1).columns.tolist()\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'booster' : 'gbtree',\n",
    "    'n_estimators':20000,\n",
    "    'max_depth':4,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'eval_metric':'auc',\n",
    "    'learning_rate':0.004, \n",
    "    'subsample':.6,\n",
    "    'min_child_weight':10,\n",
    "    'colsample_bytree':.6,\n",
    "    'scale_pos_weight': 19,\n",
    "    'gamma':1,\n",
    "    'reg_lambda' : 41.3,\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols].fillna(0).values\n",
    "    X_va = X_va[train_cols].fillna(0).values\n",
    "    X_te = X_te[train_cols].fillna(0).values\n",
    "    \n",
    "    #yy = y_tr\n",
    "    #ssp = SelectPercentile(percentile=0.1)  \n",
    "    #ssp.fit(train_mat[train_index], yy)\n",
    "    #sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    #sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    #sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(X_tr)\n",
    "    #X_tr = scaler.transform(X_tr)\n",
    "    #X_va = scaler.transform(X_va)\n",
    "    #X_te = scaler.transform(X_te)\n",
    "    #del scaler\n",
    "    \n",
    "    sp_train_mat = train_mat[train_index]\n",
    "    sp_test_mat = test_mat\n",
    "    sp_val_mat = train_mat[test_index]\n",
    "    \n",
    "    from sklearn.preprocessing import MaxAbsScaler \n",
    "    scaler = MaxAbsScaler()\n",
    "    scaler.fit(sp_train_mat)\n",
    "    sp_train_mat = scaler.transform(sp_train_mat)\n",
    "    sp_val_mat = scaler.transform(sp_val_mat)\n",
    "    sp_test_mat = scaler.transform(sp_test_mat)\n",
    "    del scaler\n",
    "    \n",
    "    X_tr = sp.hstack([\n",
    "        X_tr, sp_train_mat\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    X_va = sp.hstack([\n",
    "        X_va, sp_val_mat\n",
    "    ]).tocsr()    \n",
    "    X_te = sp.hstack([\n",
    "        X_te, sp_test_mat\n",
    "    ]).tocsr()     \n",
    "\n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = xgb.DMatrix(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = xgb.DMatrix(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "    te_data = xgb.DMatrix(X_te, label=y_va)\n",
    "    model = xgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      evals=[(tr_data,'train'),(va_data,'valid')],\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=300,\n",
    "                      #maximize = True,\n",
    "                      verbose_eval=100)\n",
    "    \n",
    "    yhat = model.predict(va_data, model.best_iteration)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "   \n",
    "    ytst = model.predict(te_data, model.best_iteration)\n",
    "    print(ytst)\n",
    "    print(minmax_scale(ytst))\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,tr_data,va_data,te_data, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('xgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7087111901240899, 0.6886407508208614, 0.68382332890308606, 0.70283890397526994, 0.69181324583274528, 0.6951145215154142, 0.6912338421917712, 0.70541713065910994, 0.69534859384084058, 0.6973542896287005]\n",
      "0.696029579749 0.00736706252798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.69487836601595887"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "roc_auc_score(X.target.values, y_oof.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.828121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.520657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.702572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.409549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.421917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.828121\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.520657\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.702572\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.409549\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.421917"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'xgb_j1'\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70266956283189674, 0.68534367448460842, 0.67659762699023662, 0.69533404351538208, 0.68507270994204705, 0.68726859777613714, 0.68473580622851282, 0.70047125013335787, 0.68772433900540597, 0.69305728895477903]\n",
      "0.689827489986 0.0075736412952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68967107272739692"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "roc_auc_score(X.target.values, y_oof.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "(385194, 302)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.686688\tvalid_1's auc: 0.697801\n",
      "[200]\ttraining's auc: 0.688095\tvalid_1's auc: 0.699308\n",
      "[300]\ttraining's auc: 0.688965\tvalid_1's auc: 0.699675\n",
      "[400]\ttraining's auc: 0.689871\tvalid_1's auc: 0.69988\n",
      "[500]\ttraining's auc: 0.690854\tvalid_1's auc: 0.699993\n",
      "[600]\ttraining's auc: 0.691808\tvalid_1's auc: 0.700054\n",
      "[700]\ttraining's auc: 0.692743\tvalid_1's auc: 0.700072\n",
      "[800]\ttraining's auc: 0.693726\tvalid_1's auc: 0.700011\n",
      "[900]\ttraining's auc: 0.694608\tvalid_1's auc: 0.699987\n",
      "Early stopping, best iteration is:\n",
      "[648]\ttraining's auc: 0.692249\tvalid_1's auc: 0.700121\n",
      "0 0.700120848549\n",
      "prepare test\n",
      "[ 0.68733739  0.66464567  0.56854747 ...,  0.49842045  0.53676299\n",
      "  0.28347787]\n",
      "fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d161811821bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0myy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mssp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectPercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mssp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0msp_train_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0msp_val_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mf_classif\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msafe_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf_oneway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mf_oneway\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mn_samples_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mss_alldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0msums_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0msquare_of_sums_alldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msums_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mn_samples_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mss_alldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0msums_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0msquare_of_sums_alldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msums_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self, axis, dtype, out)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;31m# is in {None, -1, 0, 1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0msum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self, axis, dtype, out)\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0;31m# sum over columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             ret = np.asmatrix(np.ones(\n\u001b[0;32m-> 1011\u001b[0;31m                 (1, m), dtype=res_dtype)) * self\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# sum over rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__rmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                 \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;31m#####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;31m# csr_matvec or csc_matvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = 'lgb_e1'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "              'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days','max_f1','max_f2','max_f3',\n",
    "              'svd_description_1','svd_description_2','svd_description_3','svd_description_4','svd_description_5',\n",
    "              'svd_description_6','svd_description_7','svd_description_8','svd_description_9','svd_description_10',\n",
    "              'nnet4','nnet5','nnet6','nnet7','nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','nnet3','nnet1',\n",
    "              'vw','ftrl_50','ftrl','vw2','sgd1',\n",
    "              'nnet8','lgbmb','most_freq_cat_te'] + ['svd_title_'+str(i+1) for i in range(10)]\n",
    "\n",
    "#,'most_freq_cat_te','nnet4','nnet5','nnet6','nnet7','nnet10','nnet11','nnet8',\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 16,\n",
    "    'max_depth' : 4,\n",
    "    #'min_data' : 30,\n",
    "    #'lambda_l2' : 15.5,\n",
    "    #'min_sum_hessian_in_leaf' : 0.5,\n",
    "    'lambda_l1' : 5.2,\n",
    "    'is_unbalance': True,\n",
    "    'learning_rate': 0.005,\n",
    "    'feature_fraction': 0.7,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr().astype(np.bool).astype(np.float32)\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr().astype(np.bool).astype(np.float32)\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols]\n",
    "    X_va = X_va[train_cols]\n",
    "    X_te = X_te[train_cols]\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=0.1)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([\n",
    "        X_tr, sp_train_mat\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    print('prepare valid')\n",
    "    X_va = sp.hstack([\n",
    "        X_va, sp_val_mat\n",
    "    ]).tocsr()    \n",
    "    print('prepare test')\n",
    "    X_te = sp.hstack([\n",
    "        X_te, sp_test_mat\n",
    "    ]).tocsr()     \n",
    "\n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = lgb.Dataset(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = lgb.Dataset(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "\n",
    "    model = lgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      valid_sets=[tr_data,va_data],\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=300,\n",
    "                      verbose_eval=100)\n",
    "    \n",
    "    yhat = model.predict(X_va, model.best_iteration)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict(X_te, model.best_iteration)\n",
    "    print(ytst)\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,tr_data,va_data, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('lgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))    \n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688782\tvalid_1's auc: 0.680434\n",
      "[200]\ttraining's auc: 0.689369\tvalid_1's auc: 0.680718\n",
      "[300]\ttraining's auc: 0.689748\tvalid_1's auc: 0.681187\n",
      "[400]\ttraining's auc: 0.690082\tvalid_1's auc: 0.681541\n",
      "[500]\ttraining's auc: 0.690511\tvalid_1's auc: 0.681837\n",
      "[600]\ttraining's auc: 0.690828\tvalid_1's auc: 0.682075\n",
      "[700]\ttraining's auc: 0.691088\tvalid_1's auc: 0.682264\n",
      "[800]\ttraining's auc: 0.691347\tvalid_1's auc: 0.682386\n",
      "[900]\ttraining's auc: 0.691583\tvalid_1's auc: 0.682475\n",
      "[1000]\ttraining's auc: 0.691845\tvalid_1's auc: 0.682563\n",
      "[1100]\ttraining's auc: 0.692089\tvalid_1's auc: 0.682652\n",
      "[1200]\ttraining's auc: 0.692325\tvalid_1's auc: 0.68269\n",
      "[1300]\ttraining's auc: 0.692538\tvalid_1's auc: 0.682712\n",
      "[1400]\ttraining's auc: 0.692752\tvalid_1's auc: 0.682727\n",
      "[1500]\ttraining's auc: 0.692951\tvalid_1's auc: 0.68275\n",
      "[1600]\ttraining's auc: 0.693164\tvalid_1's auc: 0.682802\n",
      "[1700]\ttraining's auc: 0.693368\tvalid_1's auc: 0.682826\n",
      "[1800]\ttraining's auc: 0.693588\tvalid_1's auc: 0.682832\n",
      "[1900]\ttraining's auc: 0.693813\tvalid_1's auc: 0.682817\n",
      "[2000]\ttraining's auc: 0.694056\tvalid_1's auc: 0.682845\n",
      "[2100]\ttraining's auc: 0.694308\tvalid_1's auc: 0.682856\n",
      "[2200]\ttraining's auc: 0.694573\tvalid_1's auc: 0.682807\n",
      "[2300]\ttraining's auc: 0.694823\tvalid_1's auc: 0.682801\n",
      "[2400]\ttraining's auc: 0.69508\tvalid_1's auc: 0.682783\n",
      "Early stopping, best iteration is:\n",
      "[2100]\ttraining's auc: 0.694308\tvalid_1's auc: 0.682856\n",
      "0 0.682855654011\n",
      "prepare test\n",
      "fold 1\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.68803\tvalid_1's auc: 0.686672\n",
      "[200]\ttraining's auc: 0.68849\tvalid_1's auc: 0.686788\n",
      "[300]\ttraining's auc: 0.688778\tvalid_1's auc: 0.687\n",
      "[400]\ttraining's auc: 0.689064\tvalid_1's auc: 0.687205\n",
      "[500]\ttraining's auc: 0.689408\tvalid_1's auc: 0.687485\n",
      "[600]\ttraining's auc: 0.689794\tvalid_1's auc: 0.687926\n",
      "[700]\ttraining's auc: 0.690118\tvalid_1's auc: 0.688263\n",
      "[800]\ttraining's auc: 0.690398\tvalid_1's auc: 0.688464\n",
      "[900]\ttraining's auc: 0.690656\tvalid_1's auc: 0.688628\n",
      "[1000]\ttraining's auc: 0.690914\tvalid_1's auc: 0.688776\n",
      "[1100]\ttraining's auc: 0.691179\tvalid_1's auc: 0.688871\n",
      "[1200]\ttraining's auc: 0.691431\tvalid_1's auc: 0.688967\n",
      "[1300]\ttraining's auc: 0.691684\tvalid_1's auc: 0.689033\n",
      "[1400]\ttraining's auc: 0.691929\tvalid_1's auc: 0.689101\n",
      "[1500]\ttraining's auc: 0.692172\tvalid_1's auc: 0.689192\n",
      "[1600]\ttraining's auc: 0.692409\tvalid_1's auc: 0.689297\n",
      "[1700]\ttraining's auc: 0.692653\tvalid_1's auc: 0.689369\n",
      "[1800]\ttraining's auc: 0.692899\tvalid_1's auc: 0.689434\n",
      "[1900]\ttraining's auc: 0.693131\tvalid_1's auc: 0.689488\n",
      "[2000]\ttraining's auc: 0.693359\tvalid_1's auc: 0.68953\n",
      "[2100]\ttraining's auc: 0.693598\tvalid_1's auc: 0.689583\n",
      "[2200]\ttraining's auc: 0.693838\tvalid_1's auc: 0.689637\n",
      "[2300]\ttraining's auc: 0.694105\tvalid_1's auc: 0.689704\n",
      "[2400]\ttraining's auc: 0.69436\tvalid_1's auc: 0.689759\n",
      "[2500]\ttraining's auc: 0.694621\tvalid_1's auc: 0.689814\n",
      "[2600]\ttraining's auc: 0.694878\tvalid_1's auc: 0.689868\n",
      "[2700]\ttraining's auc: 0.695137\tvalid_1's auc: 0.689939\n",
      "[2800]\ttraining's auc: 0.69539\tvalid_1's auc: 0.690003\n",
      "[2900]\ttraining's auc: 0.695635\tvalid_1's auc: 0.690059\n",
      "[3000]\ttraining's auc: 0.695883\tvalid_1's auc: 0.690128\n",
      "[3100]\ttraining's auc: 0.696144\tvalid_1's auc: 0.690181\n",
      "[3200]\ttraining's auc: 0.696426\tvalid_1's auc: 0.690234\n",
      "[3300]\ttraining's auc: 0.696718\tvalid_1's auc: 0.690284\n",
      "[3400]\ttraining's auc: 0.697\tvalid_1's auc: 0.690289\n",
      "[3500]\ttraining's auc: 0.697291\tvalid_1's auc: 0.690315\n",
      "[3600]\ttraining's auc: 0.69759\tvalid_1's auc: 0.69033\n",
      "[3700]\ttraining's auc: 0.697894\tvalid_1's auc: 0.690339\n",
      "[3800]\ttraining's auc: 0.698179\tvalid_1's auc: 0.690344\n",
      "[3900]\ttraining's auc: 0.698455\tvalid_1's auc: 0.69036\n",
      "[4000]\ttraining's auc: 0.698714\tvalid_1's auc: 0.6904\n",
      "[4100]\ttraining's auc: 0.698962\tvalid_1's auc: 0.690442\n",
      "[4200]\ttraining's auc: 0.69923\tvalid_1's auc: 0.690458\n",
      "[4300]\ttraining's auc: 0.699482\tvalid_1's auc: 0.690461\n",
      "[4400]\ttraining's auc: 0.699732\tvalid_1's auc: 0.690457\n",
      "[4500]\ttraining's auc: 0.699988\tvalid_1's auc: 0.690467\n",
      "[4600]\ttraining's auc: 0.700237\tvalid_1's auc: 0.690462\n",
      "[4700]\ttraining's auc: 0.700478\tvalid_1's auc: 0.690453\n",
      "[4800]\ttraining's auc: 0.700723\tvalid_1's auc: 0.690439\n",
      "Early stopping, best iteration is:\n",
      "[4512]\ttraining's auc: 0.700018\tvalid_1's auc: 0.690473\n",
      "1 0.690473482077\n",
      "prepare test\n",
      "fold 2\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.687897\tvalid_1's auc: 0.687679\n",
      "[200]\ttraining's auc: 0.688505\tvalid_1's auc: 0.687971\n",
      "[300]\ttraining's auc: 0.688897\tvalid_1's auc: 0.688232\n",
      "[400]\ttraining's auc: 0.68925\tvalid_1's auc: 0.68838\n",
      "[500]\ttraining's auc: 0.689585\tvalid_1's auc: 0.68848\n",
      "[600]\ttraining's auc: 0.689937\tvalid_1's auc: 0.688536\n",
      "[700]\ttraining's auc: 0.690246\tvalid_1's auc: 0.688574\n",
      "[800]\ttraining's auc: 0.69053\tvalid_1's auc: 0.688574\n",
      "[900]\ttraining's auc: 0.690794\tvalid_1's auc: 0.688556\n",
      "[1000]\ttraining's auc: 0.691036\tvalid_1's auc: 0.688521\n",
      "Early stopping, best iteration is:\n",
      "[767]\ttraining's auc: 0.690435\tvalid_1's auc: 0.688596\n",
      "2 0.688595677634\n",
      "prepare test\n",
      "fold 3\n",
      "prepare train\n",
      "(385194, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688611\tvalid_1's auc: 0.681558\n",
      "[200]\ttraining's auc: 0.688942\tvalid_1's auc: 0.681723\n",
      "[300]\ttraining's auc: 0.689405\tvalid_1's auc: 0.681981\n",
      "[400]\ttraining's auc: 0.689836\tvalid_1's auc: 0.682159\n",
      "[500]\ttraining's auc: 0.69026\tvalid_1's auc: 0.68261\n",
      "[600]\ttraining's auc: 0.690625\tvalid_1's auc: 0.683\n",
      "[700]\ttraining's auc: 0.690936\tvalid_1's auc: 0.683255\n",
      "[800]\ttraining's auc: 0.691192\tvalid_1's auc: 0.683397\n",
      "[900]\ttraining's auc: 0.691395\tvalid_1's auc: 0.683551\n",
      "[1000]\ttraining's auc: 0.691634\tvalid_1's auc: 0.683672\n",
      "[1100]\ttraining's auc: 0.691877\tvalid_1's auc: 0.683843\n",
      "[1200]\ttraining's auc: 0.692116\tvalid_1's auc: 0.683963\n",
      "[1300]\ttraining's auc: 0.692337\tvalid_1's auc: 0.684054\n",
      "[1400]\ttraining's auc: 0.692579\tvalid_1's auc: 0.684135\n",
      "[1500]\ttraining's auc: 0.692796\tvalid_1's auc: 0.68423\n",
      "[1600]\ttraining's auc: 0.693033\tvalid_1's auc: 0.68428\n",
      "[1700]\ttraining's auc: 0.693263\tvalid_1's auc: 0.684328\n",
      "[1800]\ttraining's auc: 0.693501\tvalid_1's auc: 0.684393\n",
      "[1900]\ttraining's auc: 0.693769\tvalid_1's auc: 0.684411\n",
      "[2000]\ttraining's auc: 0.694031\tvalid_1's auc: 0.684399\n",
      "[2100]\ttraining's auc: 0.694289\tvalid_1's auc: 0.684376\n",
      "[2200]\ttraining's auc: 0.694553\tvalid_1's auc: 0.684387\n",
      "Early stopping, best iteration is:\n",
      "[1937]\ttraining's auc: 0.693863\tvalid_1's auc: 0.68442\n",
      "3 0.684419555855\n",
      "prepare test\n",
      "fold 4\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688906\tvalid_1's auc: 0.682953\n",
      "[200]\ttraining's auc: 0.689386\tvalid_1's auc: 0.682946\n",
      "[300]\ttraining's auc: 0.689657\tvalid_1's auc: 0.683262\n",
      "[400]\ttraining's auc: 0.689896\tvalid_1's auc: 0.683462\n",
      "[500]\ttraining's auc: 0.69017\tvalid_1's auc: 0.683643\n",
      "[600]\ttraining's auc: 0.690428\tvalid_1's auc: 0.683747\n",
      "[700]\ttraining's auc: 0.690663\tvalid_1's auc: 0.683901\n",
      "[800]\ttraining's auc: 0.690916\tvalid_1's auc: 0.684049\n",
      "[900]\ttraining's auc: 0.691138\tvalid_1's auc: 0.684172\n",
      "[1000]\ttraining's auc: 0.691358\tvalid_1's auc: 0.684302\n",
      "[1100]\ttraining's auc: 0.691595\tvalid_1's auc: 0.684494\n",
      "[1200]\ttraining's auc: 0.691813\tvalid_1's auc: 0.684628\n",
      "[1300]\ttraining's auc: 0.692018\tvalid_1's auc: 0.684754\n",
      "[1400]\ttraining's auc: 0.692237\tvalid_1's auc: 0.684893\n",
      "[1500]\ttraining's auc: 0.692458\tvalid_1's auc: 0.685019\n",
      "[1600]\ttraining's auc: 0.692668\tvalid_1's auc: 0.685113\n",
      "[1700]\ttraining's auc: 0.692893\tvalid_1's auc: 0.685188\n",
      "[1800]\ttraining's auc: 0.693119\tvalid_1's auc: 0.685269\n",
      "[1900]\ttraining's auc: 0.693338\tvalid_1's auc: 0.685379\n",
      "[2000]\ttraining's auc: 0.693583\tvalid_1's auc: 0.685446\n",
      "[2100]\ttraining's auc: 0.693811\tvalid_1's auc: 0.685476\n",
      "[2200]\ttraining's auc: 0.694037\tvalid_1's auc: 0.685507\n",
      "[2300]\ttraining's auc: 0.694295\tvalid_1's auc: 0.685536\n",
      "[2400]\ttraining's auc: 0.694554\tvalid_1's auc: 0.685556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500]\ttraining's auc: 0.694809\tvalid_1's auc: 0.685585\n",
      "[2600]\ttraining's auc: 0.695072\tvalid_1's auc: 0.685611\n",
      "[2700]\ttraining's auc: 0.695337\tvalid_1's auc: 0.685642\n",
      "[2800]\ttraining's auc: 0.6956\tvalid_1's auc: 0.685645\n",
      "[2900]\ttraining's auc: 0.695853\tvalid_1's auc: 0.685642\n",
      "[3000]\ttraining's auc: 0.696117\tvalid_1's auc: 0.685645\n",
      "[3100]\ttraining's auc: 0.696391\tvalid_1's auc: 0.685631\n",
      "[3200]\ttraining's auc: 0.696669\tvalid_1's auc: 0.685628\n",
      "Early stopping, best iteration is:\n",
      "[2959]\ttraining's auc: 0.696009\tvalid_1's auc: 0.68565\n",
      "4 0.685650472164\n",
      "prepare test\n",
      "fold 5\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.68827\tvalid_1's auc: 0.68582\n",
      "[200]\ttraining's auc: 0.688599\tvalid_1's auc: 0.686177\n",
      "[300]\ttraining's auc: 0.689037\tvalid_1's auc: 0.686457\n",
      "[400]\ttraining's auc: 0.689382\tvalid_1's auc: 0.686671\n",
      "[500]\ttraining's auc: 0.689775\tvalid_1's auc: 0.68678\n",
      "[600]\ttraining's auc: 0.690088\tvalid_1's auc: 0.68699\n",
      "[700]\ttraining's auc: 0.690347\tvalid_1's auc: 0.687086\n",
      "[800]\ttraining's auc: 0.690576\tvalid_1's auc: 0.687199\n",
      "[900]\ttraining's auc: 0.690823\tvalid_1's auc: 0.687286\n",
      "[1000]\ttraining's auc: 0.691055\tvalid_1's auc: 0.687357\n",
      "[1100]\ttraining's auc: 0.691272\tvalid_1's auc: 0.68746\n",
      "[1200]\ttraining's auc: 0.691487\tvalid_1's auc: 0.687606\n",
      "[1300]\ttraining's auc: 0.6917\tvalid_1's auc: 0.687699\n",
      "[1400]\ttraining's auc: 0.691914\tvalid_1's auc: 0.687838\n",
      "[1500]\ttraining's auc: 0.692128\tvalid_1's auc: 0.687971\n",
      "[1600]\ttraining's auc: 0.692348\tvalid_1's auc: 0.688065\n",
      "[1700]\ttraining's auc: 0.692569\tvalid_1's auc: 0.688149\n",
      "[1800]\ttraining's auc: 0.692811\tvalid_1's auc: 0.688252\n",
      "[1900]\ttraining's auc: 0.693063\tvalid_1's auc: 0.688336\n",
      "[2000]\ttraining's auc: 0.693301\tvalid_1's auc: 0.688422\n",
      "[2100]\ttraining's auc: 0.693537\tvalid_1's auc: 0.688522\n",
      "[2200]\ttraining's auc: 0.693767\tvalid_1's auc: 0.688568\n",
      "[2300]\ttraining's auc: 0.694006\tvalid_1's auc: 0.688608\n",
      "[2400]\ttraining's auc: 0.694247\tvalid_1's auc: 0.688667\n",
      "[2500]\ttraining's auc: 0.694487\tvalid_1's auc: 0.688703\n",
      "[2600]\ttraining's auc: 0.694707\tvalid_1's auc: 0.688727\n",
      "[2700]\ttraining's auc: 0.694932\tvalid_1's auc: 0.68874\n",
      "[2800]\ttraining's auc: 0.695158\tvalid_1's auc: 0.688781\n",
      "[2900]\ttraining's auc: 0.695401\tvalid_1's auc: 0.688828\n",
      "[3000]\ttraining's auc: 0.695628\tvalid_1's auc: 0.688872\n",
      "[3100]\ttraining's auc: 0.695883\tvalid_1's auc: 0.688922\n",
      "[3200]\ttraining's auc: 0.69614\tvalid_1's auc: 0.68896\n",
      "[3300]\ttraining's auc: 0.696416\tvalid_1's auc: 0.688976\n",
      "[3400]\ttraining's auc: 0.696699\tvalid_1's auc: 0.689017\n",
      "[3500]\ttraining's auc: 0.696993\tvalid_1's auc: 0.689052\n",
      "[3600]\ttraining's auc: 0.697305\tvalid_1's auc: 0.689092\n",
      "[3700]\ttraining's auc: 0.697616\tvalid_1's auc: 0.68912\n",
      "[3800]\ttraining's auc: 0.697928\tvalid_1's auc: 0.68915\n",
      "[3900]\ttraining's auc: 0.69823\tvalid_1's auc: 0.689183\n",
      "[4000]\ttraining's auc: 0.698497\tvalid_1's auc: 0.689196\n",
      "[4100]\ttraining's auc: 0.698768\tvalid_1's auc: 0.689233\n",
      "[4200]\ttraining's auc: 0.699041\tvalid_1's auc: 0.689245\n",
      "[4300]\ttraining's auc: 0.699319\tvalid_1's auc: 0.689242\n",
      "[4400]\ttraining's auc: 0.699607\tvalid_1's auc: 0.689246\n",
      "[4500]\ttraining's auc: 0.699903\tvalid_1's auc: 0.689249\n",
      "[4600]\ttraining's auc: 0.700185\tvalid_1's auc: 0.689236\n",
      "[4700]\ttraining's auc: 0.700458\tvalid_1's auc: 0.689207\n",
      "Early stopping, best iteration is:\n",
      "[4474]\ttraining's auc: 0.699823\tvalid_1's auc: 0.689256\n",
      "5 0.689256016897\n",
      "prepare test\n",
      "fold 6\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688559\tvalid_1's auc: 0.685646\n",
      "[200]\ttraining's auc: 0.689041\tvalid_1's auc: 0.685962\n",
      "[300]\ttraining's auc: 0.689383\tvalid_1's auc: 0.686052\n",
      "[400]\ttraining's auc: 0.689712\tvalid_1's auc: 0.686178\n",
      "[500]\ttraining's auc: 0.690023\tvalid_1's auc: 0.686434\n",
      "[600]\ttraining's auc: 0.690261\tvalid_1's auc: 0.686589\n",
      "[700]\ttraining's auc: 0.690481\tvalid_1's auc: 0.686681\n",
      "[800]\ttraining's auc: 0.690711\tvalid_1's auc: 0.686873\n",
      "[900]\ttraining's auc: 0.690946\tvalid_1's auc: 0.687074\n",
      "[1000]\ttraining's auc: 0.69117\tvalid_1's auc: 0.687195\n",
      "[1100]\ttraining's auc: 0.691406\tvalid_1's auc: 0.687293\n",
      "[1200]\ttraining's auc: 0.691651\tvalid_1's auc: 0.687404\n",
      "[1300]\ttraining's auc: 0.691886\tvalid_1's auc: 0.687507\n",
      "[1400]\ttraining's auc: 0.692107\tvalid_1's auc: 0.687546\n",
      "[1500]\ttraining's auc: 0.692334\tvalid_1's auc: 0.68758\n",
      "[1600]\ttraining's auc: 0.692578\tvalid_1's auc: 0.687567\n",
      "[1700]\ttraining's auc: 0.692823\tvalid_1's auc: 0.687525\n",
      "[1800]\ttraining's auc: 0.693079\tvalid_1's auc: 0.687533\n",
      "Early stopping, best iteration is:\n",
      "[1522]\ttraining's auc: 0.692392\tvalid_1's auc: 0.687584\n",
      "6 0.687583903933\n",
      "prepare test\n",
      "fold 7\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688485\tvalid_1's auc: 0.683377\n",
      "[200]\ttraining's auc: 0.688764\tvalid_1's auc: 0.683258\n",
      "[300]\ttraining's auc: 0.689203\tvalid_1's auc: 0.683403\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's auc: 0.688508\tvalid_1's auc: 0.684045\n",
      "7 0.684044639416\n",
      "prepare test\n",
      "fold 8\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688397\tvalid_1's auc: 0.691516\n",
      "[200]\ttraining's auc: 0.688981\tvalid_1's auc: 0.69175\n",
      "[300]\ttraining's auc: 0.689236\tvalid_1's auc: 0.691751\n",
      "[400]\ttraining's auc: 0.689467\tvalid_1's auc: 0.691801\n",
      "[500]\ttraining's auc: 0.689665\tvalid_1's auc: 0.691771\n",
      "[600]\ttraining's auc: 0.689858\tvalid_1's auc: 0.69174\n",
      "[700]\ttraining's auc: 0.69011\tvalid_1's auc: 0.691741\n",
      "Early stopping, best iteration is:\n",
      "[402]\ttraining's auc: 0.689478\tvalid_1's auc: 0.691813\n",
      "8 0.691812957391\n",
      "prepare test\n",
      "fold 9\n",
      "prepare train\n",
      "(385195, 410)\n",
      "prepare valid\n",
      "prepare test\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's auc: 0.688583\tvalid_1's auc: 0.684301\n",
      "[200]\ttraining's auc: 0.688993\tvalid_1's auc: 0.684234\n",
      "[300]\ttraining's auc: 0.689373\tvalid_1's auc: 0.68414\n",
      "[400]\ttraining's auc: 0.689836\tvalid_1's auc: 0.684309\n",
      "[500]\ttraining's auc: 0.690274\tvalid_1's auc: 0.68469\n",
      "[600]\ttraining's auc: 0.690595\tvalid_1's auc: 0.684881\n",
      "[700]\ttraining's auc: 0.690874\tvalid_1's auc: 0.685048\n",
      "[800]\ttraining's auc: 0.691094\tvalid_1's auc: 0.685174\n",
      "[900]\ttraining's auc: 0.691297\tvalid_1's auc: 0.685261\n",
      "[1000]\ttraining's auc: 0.691516\tvalid_1's auc: 0.685333\n",
      "[1100]\ttraining's auc: 0.691741\tvalid_1's auc: 0.685351\n",
      "[1200]\ttraining's auc: 0.691971\tvalid_1's auc: 0.685374\n",
      "[1300]\ttraining's auc: 0.692194\tvalid_1's auc: 0.685396\n",
      "[1400]\ttraining's auc: 0.692407\tvalid_1's auc: 0.685437\n",
      "[1500]\ttraining's auc: 0.692627\tvalid_1's auc: 0.685481\n",
      "[1600]\ttraining's auc: 0.692841\tvalid_1's auc: 0.685481\n",
      "[1700]\ttraining's auc: 0.693058\tvalid_1's auc: 0.685471\n",
      "[1800]\ttraining's auc: 0.693262\tvalid_1's auc: 0.685474\n",
      "Early stopping, best iteration is:\n",
      "[1511]\ttraining's auc: 0.692649\tvalid_1's auc: 0.685487\n",
      "9 0.685486562807\n",
      "prepare test\n",
      "[0.68285565401054749, 0.69047348207724357, 0.68859567763395668, 0.6844195558552767, 0.68565047216409147, 0.68925601689664151, 0.68758390393251834, 0.68404463941572935, 0.69181295739052984, 0.68548656280730147]\n",
      "0.687017892218 0.00282636423315\n",
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.727645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.578061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.673377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.476768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.444018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.727645\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.578061\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.673377\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.476768\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.444018"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'lgb_b2'\n",
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "              'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days','max_f1','max_f2','max_f3',\n",
    "              'svd_description_1','svd_description_2','svd_description_3','svd_description_4','svd_description_5',\n",
    "              'svd_description_6','svd_description_7','svd_description_8','svd_description_9','svd_description_10',\n",
    "              'nnet4','nnet5','nnet6','nnet7','nnet10','nnet11','xgb_single','nnet12','nnet13','lgbm1','nnet3','nnet1',\n",
    "              'nnet8','lgbmb','most_freq_cat_te'] + ['svd_title_'+str(i+1) for i in range(10)]\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 16,\n",
    "    'max_depth' : 4,\n",
    "    #'min_data' : 100,\n",
    "    #'lambda_l2' : 15.5,\n",
    "    'min_sum_hessian_in_leaf' : 0.2,\n",
    "    'lambda_l1' : 6.2,\n",
    "    'is_unbalance': True,\n",
    "    'learning_rate': 0.001,\n",
    "    'feature_fraction': 0.7,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=13)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).tocsr()\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).tocsr()\n",
    "mat_spca = np.load(data_dir + 'spca_dim100.npy')\n",
    "train_mat_spca = mat_spca[df_train_index.tolist()]\n",
    "test_mat_spca = mat_spca[df_test_index.tolist()]\n",
    "del mat_spca\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols]\n",
    "    X_va = X_va[train_cols]\n",
    "    X_te = X_te[train_cols]\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=0.1)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([\n",
    "        X_tr, sp_train_mat, train_mat_spca[train_index]\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    print('prepare valid')\n",
    "    X_va = sp.hstack([\n",
    "        X_va, sp_val_mat, train_mat_spca[test_index]\n",
    "    ]).tocsr()    \n",
    "    print('prepare test')\n",
    "    X_te = sp.hstack([\n",
    "        X_te, sp_test_mat, test_mat_spca\n",
    "    ]).tocsr()     \n",
    "\n",
    "    # Create the LightGBM data containers\n",
    "    tr_data = lgb.Dataset(X_tr, label=y_tr) #, categorical_feature=cate_cols\n",
    "    va_data = lgb.Dataset(X_va, label=y_va) #, categorical_feature=cate_cols\n",
    "\n",
    "    model = lgb.train(parameters,\n",
    "                      tr_data,\n",
    "                      valid_sets=[tr_data,va_data],\n",
    "                      num_boost_round=8000,\n",
    "                      early_stopping_rounds=300,\n",
    "                      verbose_eval=100)\n",
    "    \n",
    "    yhat = model.predict(X_va, model.best_iteration)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    ytst = model.predict(X_te, model.best_iteration)\n",
    "    y_pred += ytst*0.1\n",
    "    \n",
    "    del X_tr,X_va,tr_data,va_data, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('lgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1     \n",
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))    \n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.727645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.578061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.673377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.476768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.444018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.727645\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.578061\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.673377\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.476768\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.444018"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_xgb_b1.npy'))\n",
    "sample_sub.columns = ['uid','xgb_sess4']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_lgb_b1.npy'))\n",
    "sample_sub.columns = ['uid','lgb_sess6']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_lgb_b2.npy'))\n",
    "sample_sub.columns = ['uid','lgb_sess7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['xgb_sess4'] = minmax_scale(np.load(results_dir + 'train_xgb_b1.npy'))\n",
    "X['lgb_sess6'] = minmax_scale(np.load(results_dir + 'train_lgb_b1.npy'))\n",
    "X['lgb_sess7'] = minmax_scale(np.load(results_dir + 'train_lgb_b2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnet12 0.67274153339\n",
      "nnet13 0.67264163497\n",
      "lgbm1 0.682562603224\n",
      "lgbmb 0.66792997872\n",
      "nnet1 0.623698784984\n",
      "nnet3 0.574520607881\n",
      "xgb_single 0.660175309563\n"
     ]
    }
   ],
   "source": [
    "meta = ['nnet12','nnet13','lgbm1','lgbmb','nnet1','nnet3','xgb_single'] #\n",
    "for f in meta:\n",
    "    print(f,roc_auc_score(X.target, X[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "      <th>lgbm1</th>\n",
       "      <th>lgbmb</th>\n",
       "      <th>nnet1</th>\n",
       "      <th>nnet3</th>\n",
       "      <th>xgb_single</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815829</td>\n",
       "      <td>0.736898</td>\n",
       "      <td>0.844497</td>\n",
       "      <td>0.612611</td>\n",
       "      <td>0.445006</td>\n",
       "      <td>0.825464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.815829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853592</td>\n",
       "      <td>0.710129</td>\n",
       "      <td>0.463131</td>\n",
       "      <td>0.288266</td>\n",
       "      <td>0.687111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm1</th>\n",
       "      <td>0.736898</td>\n",
       "      <td>0.853592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.752149</td>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.698921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbmb</th>\n",
       "      <td>0.844497</td>\n",
       "      <td>0.710129</td>\n",
       "      <td>0.752149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.668780</td>\n",
       "      <td>0.501951</td>\n",
       "      <td>0.907113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet1</th>\n",
       "      <td>0.612611</td>\n",
       "      <td>0.463131</td>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.668780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.487426</td>\n",
       "      <td>0.661838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet3</th>\n",
       "      <td>0.445006</td>\n",
       "      <td>0.288266</td>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.501951</td>\n",
       "      <td>0.487426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_single</th>\n",
       "      <td>0.825464</td>\n",
       "      <td>0.687111</td>\n",
       "      <td>0.698921</td>\n",
       "      <td>0.907113</td>\n",
       "      <td>0.661838</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nnet12    nnet13     lgbm1     lgbmb     nnet1     nnet3  \\\n",
       "nnet12      1.000000  0.815829  0.736898  0.844497  0.612611  0.445006   \n",
       "nnet13      0.815829  1.000000  0.853592  0.710129  0.463131  0.288266   \n",
       "lgbm1       0.736898  0.853592  1.000000  0.752149  0.494446  0.295207   \n",
       "lgbmb       0.844497  0.710129  0.752149  1.000000  0.668780  0.501951   \n",
       "nnet1       0.612611  0.463131  0.494446  0.668780  1.000000  0.487426   \n",
       "nnet3       0.445006  0.288266  0.295207  0.501951  0.487426  1.000000   \n",
       "xgb_single  0.825464  0.687111  0.698921  0.907113  0.661838  0.490909   \n",
       "\n",
       "            xgb_single  \n",
       "nnet12        0.825464  \n",
       "nnet13        0.687111  \n",
       "lgbm1         0.698921  \n",
       "lgbmb         0.907113  \n",
       "nnet1         0.661838  \n",
       "nnet3         0.490909  \n",
       "xgb_single    1.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[meta].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "      <th>lgbm1</th>\n",
       "      <th>lgbmb</th>\n",
       "      <th>nnet1</th>\n",
       "      <th>nnet3</th>\n",
       "      <th>xgb_single</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>0.793141</td>\n",
       "      <td>0.837708</td>\n",
       "      <td>0.643211</td>\n",
       "      <td>0.436315</td>\n",
       "      <td>0.821308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.872521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871866</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.511538</td>\n",
       "      <td>0.316164</td>\n",
       "      <td>0.681750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm1</th>\n",
       "      <td>0.793141</td>\n",
       "      <td>0.871866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778259</td>\n",
       "      <td>0.536187</td>\n",
       "      <td>0.342257</td>\n",
       "      <td>0.726940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbmb</th>\n",
       "      <td>0.837708</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.778259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>0.466649</td>\n",
       "      <td>0.933161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet1</th>\n",
       "      <td>0.643211</td>\n",
       "      <td>0.511538</td>\n",
       "      <td>0.536187</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.639809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet3</th>\n",
       "      <td>0.436315</td>\n",
       "      <td>0.316164</td>\n",
       "      <td>0.342257</td>\n",
       "      <td>0.466649</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_single</th>\n",
       "      <td>0.821308</td>\n",
       "      <td>0.681750</td>\n",
       "      <td>0.726940</td>\n",
       "      <td>0.933161</td>\n",
       "      <td>0.639809</td>\n",
       "      <td>0.456628</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nnet12    nnet13     lgbm1     lgbmb     nnet1     nnet3  \\\n",
       "nnet12      1.000000  0.872521  0.793141  0.837708  0.643211  0.436315   \n",
       "nnet13      0.872521  1.000000  0.871866  0.713111  0.511538  0.316164   \n",
       "lgbm1       0.793141  0.871866  1.000000  0.778259  0.536187  0.342257   \n",
       "lgbmb       0.837708  0.713111  0.778259  1.000000  0.641589  0.466649   \n",
       "nnet1       0.643211  0.511538  0.536187  0.641589  1.000000  0.488963   \n",
       "nnet3       0.436315  0.316164  0.342257  0.466649  0.488963  1.000000   \n",
       "xgb_single  0.821308  0.681750  0.726940  0.933161  0.639809  0.456628   \n",
       "\n",
       "            xgb_single  \n",
       "nnet12        0.821308  \n",
       "nnet13        0.681750  \n",
       "lgbm1         0.726940  \n",
       "lgbmb         0.933161  \n",
       "nnet1         0.639809  \n",
       "nnet3         0.456628  \n",
       "xgb_single    1.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te[meta].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "0 0.698668673061\n",
      "prepare test\n",
      "[ 0.14689715  0.14794698  0.10039477 ...,  0.07865642  0.0795201\n",
      "  0.04324515]\n",
      "fold 1\n",
      "1 0.680282222997\n",
      "prepare test\n",
      "[ 0.14906741  0.14968694  0.10206368 ...,  0.08114234  0.08153055\n",
      "  0.04605674]\n",
      "fold 2\n",
      "2 0.672018064269\n",
      "prepare test\n",
      "[ 0.14804478  0.14921921  0.10076937 ...,  0.07930363  0.07985425\n",
      "  0.04391066]\n",
      "fold 3\n",
      "3 0.688242992587\n",
      "prepare test\n",
      "[ 0.14820545  0.1487872   0.10010541 ...,  0.07902909  0.07945207\n",
      "  0.04392936]\n",
      "fold 4\n",
      "4 0.681608416947\n",
      "prepare test\n",
      "[ 0.14675651  0.14805475  0.10015005 ...,  0.07874264  0.07952918\n",
      "  0.0439385 ]\n",
      "fold 5\n",
      "5 0.683953307708\n",
      "prepare test\n",
      "[ 0.14593103  0.14723824  0.09932246 ...,  0.07791339  0.07879705\n",
      "  0.04330436]\n",
      "fold 6\n",
      "6 0.678530035578\n",
      "prepare test\n",
      "[ 0.15262749  0.15368207  0.10530478 ...,  0.08421273  0.08437358\n",
      "  0.04908429]\n",
      "fold 7\n",
      "7 0.694977085688\n",
      "prepare test\n",
      "[ 0.14813504  0.14974     0.10206602 ...,  0.08021212  0.08106084\n",
      "  0.04498688]\n",
      "fold 8\n",
      "8 0.683650882628\n",
      "prepare test\n",
      "[ 0.14763976  0.14892431  0.10101306 ...,  0.07973616  0.08026111\n",
      "  0.04440297]\n",
      "fold 9\n",
      "9 0.691927886333\n",
      "prepare test\n",
      "[ 0.14971947  0.15133756  0.10406824 ...,  0.0826697   0.08325906\n",
      "  0.04764618]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "def save_submit(model_name, folds, y_pred):\n",
    "    global x_te\n",
    "    sub = x_te[['uid','target']].copy()\n",
    "    sub['target'] = y_pred\n",
    "    sub.columns = ['cuid','target']\n",
    "    sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "    sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "    sample_sub[['target']].to_csv(results_dir + model_name + '_' + str(folds) + 'folds.csv', header=False, index=False)\n",
    "    del sub,sample_sub\n",
    "    gc.collect()\n",
    "    \n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = meta\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'booster' : 'gbtree',\n",
    "    'n_estimators':20000,\n",
    "    'max_depth':4,\n",
    "    'objective':\"binary:logistic\",\n",
    "    'eval_metric':'auc',\n",
    "    'learning_rate':0.005, \n",
    "    'subsample':.6,\n",
    "    'min_child_weight':10,\n",
    "    'colsample_bytree':.6,\n",
    "    'scale_pos_weight': 19,\n",
    "    'gamma':1,\n",
    "    #'reg_alpha':1,\n",
    "    'reg_lambda':1.3,\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "\n",
    "    \n",
    "    X_tr = X_tr[train_cols].fillna(0).values\n",
    "    X_va = X_va[train_cols].fillna(0).values\n",
    "    X_te = X_te[train_cols].fillna(0).values\n",
    "    \n",
    "       \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    del scaler\n",
    "\n",
    "    y_hat_med = pd.DataFrame(X_va).mean(axis=1)\n",
    "    #print(ifold,'median:',roc_auc_score(y_va,y_hat_med))\n",
    "    \n",
    "    \n",
    "    # Create the LightGBM data containers\n",
    "    model = Ridge(alpha=20)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    \n",
    "    yhat = model.predict(X_va)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "\n",
    "    print('prepare test')\n",
    "    \n",
    "    #\n",
    "    ytst = pd.DataFrame(X_te).mean(axis=1)\n",
    "    ytst = model.predict(X_te)\n",
    "    print(minmax_scale(ytst))\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te\n",
    "    gc.collect()    \n",
    "    \n",
    "    save_submit('xgb_q', ifold, y_pred)\n",
    "\n",
    "    ifold += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69866867306089697, 0.68028222299680186, 0.6720180642689908, 0.6882429925869018, 0.6816084169472465, 0.68395330770756191, 0.67853003557793867, 0.69497708568816041, 0.68365088262772289, 0.69192788633314306]\n",
      "0.68538595678 0.00768417125345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68522457782906487"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "roc_auc_score(X.target.values, y_oof.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.246518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.106445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.165956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.074222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.064321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.246518\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.106445\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.165956\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.074222\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.064321"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ridge_e1'\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame()\n",
    "s['s7463'] = minmax_scale(pd.read_csv(results_dir + 'ridge_b1.csv', header=None, names=['v']).v.values)\n",
    "s['s7412'] = minmax_scale(pd.read_csv(results_dir + 'baseline_sparse_10folds.csv', header=None, names=['v']).v.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s7463</th>\n",
       "      <th>s7412</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.811052</td>\n",
       "      <td>0.184876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.570673</td>\n",
       "      <td>0.096343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.726611</td>\n",
       "      <td>0.151491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.429136</td>\n",
       "      <td>0.055863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.389015</td>\n",
       "      <td>0.051491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      s7463     s7412\n",
       "0  0.811052  0.184876\n",
       "1  0.570673  0.096343\n",
       "2  0.726611  0.151491\n",
       "3  0.429136  0.055863\n",
       "4  0.389015  0.051491"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181024"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
