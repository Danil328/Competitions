{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/mlboot_dataset/'\n",
    "model_name = 'xgb_b1'\n",
    "results_dir = './results/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 2053602) (609018, 2812610) (609018, 1057788)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir + 'preprocessed_new.csv') \n",
    "q = pd.read_csv(data_dir + 'sessions.csv')\n",
    "df = df.merge(q, on='uid', how='left')\n",
    "del q\n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index\n",
    "\n",
    "mat1 = sp.load_npz(data_dir+'dmat1.npz').tolil()\n",
    "mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil()\n",
    "mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil()\n",
    "print(mat1.shape, mat2.shape, mat3.shape)\n",
    "\n",
    "df['max_f1'] = mat1.tocsr().max(axis=1).todense()\n",
    "df['max_f2'] = mat2.tocsr().max(axis=1).todense()\n",
    "df['max_f3'] = mat3.tocsr().max(axis=1).todense()\n",
    "\n",
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "limit = 11\n",
    "mat1 = mat1.tocsc()[:, np.where((train_mat1.getnnz(axis=0) > limit) & (test_mat1.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat2 = mat2.tocsc()[:, np.where((train_mat2.getnnz(axis=0) > limit) & (test_mat2.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat3 = mat3.tocsc()[:, np.where((train_mat3.getnnz(axis=0) > limit) & (test_mat3.getnnz(axis=0) > 0))[0]].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 195733) (609018, 20268) (609018, 9415)\n"
     ]
    }
   ],
   "source": [
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'pca_cat10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_description_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'bin_pca_dim10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_title_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "del mat1,mat2,mat3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
       "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
       "       'records', 'max_days', 'min_days',\n",
       "       ...\n",
       "       'svd_title_1', 'svd_title_2', 'svd_title_3', 'svd_title_4',\n",
       "       'svd_title_5', 'svd_title_6', 'svd_title_7', 'svd_title_8',\n",
       "       'svd_title_9', 'svd_title_10'],\n",
       "      dtype='object', length=107)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_baseline_sparse_10folds.npy'))\n",
    "sample_sub.columns = ['uid','lgbm1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_all_in_focal_loss.npy'))\n",
    "sample_sub.columns = ['uid','nnet2']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m3.npy'))\n",
    "sample_sub.columns = ['uid','nnet3']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m1.npy'))\n",
    "sample_sub.columns = ['uid','nnet1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_ftrl_50.npy'))\n",
    "sample_sub.columns = ['uid','ftrl_50']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_ftrl_70.npy'))\n",
    "sample_sub.columns = ['uid','ftrl']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model.npy'))\n",
    "sample_sub.columns = ['uid','nnet4']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_catpca.npy'))\n",
    "sample_sub.columns = ['uid','nnet5']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br.npy'))\n",
    "sample_sub.columns = ['uid','nnet6']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_wcc.npy'))\n",
    "sample_sub.columns = ['uid','nnet7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_tanh.npy'))\n",
    "sample_sub.columns = ['uid','nnet8']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_binary_lgbm.npy'))\n",
    "sample_sub.columns = ['uid','lgbmb']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_v4.npy'))\n",
    "sample_sub.columns = ['uid','nnet10']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_v5.npy'))\n",
    "sample_sub.columns = ['uid','nnet11']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_xgb_single.npy'))\n",
    "sample_sub.columns = ['uid','xgb_single']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_3br_bin_v2.npy'))\n",
    "sample_sub.columns = ['uid','nnet12']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_bin.npy'))\n",
    "sample_sub.columns = ['uid','nnet13']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['lgbm1'] = minmax_scale(np.load(results_dir + 'train_baseline_sparse_10folds.npy'))\n",
    "X['lgbmb'] = minmax_scale(np.load(results_dir + 'train_binary_lgbm.npy'))\n",
    "X['nnet2'] = minmax_scale(np.load(results_dir + 'train_all_in_focal_loss.npy'))\n",
    "X['nnet1'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m1.npy'))\n",
    "X['nnet3'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m3.npy'))\n",
    "X['nnet4'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model.npy'))\n",
    "X['nnet5'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_catpca.npy'))\n",
    "X['nnet6'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br.npy'))\n",
    "X['nnet7'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_wcc.npy'))\n",
    "X['nnet8'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_tanh.npy'))\n",
    "X['ftrl_50']  = minmax_scale(np.load(results_dir + 'train_ftrl_50.npy'))\n",
    "X['ftrl']  = minmax_scale(np.load(results_dir + 'train_ftrl_70.npy'))\n",
    "X['nnet10'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_v4.npy'))\n",
    "X['nnet11'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_v5.npy'))\n",
    "X['xgb_single']  = minmax_scale(np.load(results_dir + 'train_xgb_single.npy'))\n",
    "X['nnet12'] = minmax_scale(np.load(results_dir + 'train_nn_3br_bin_v2.npy'))\n",
    "X['nnet13'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_bin.npy'))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm1 0.682562603224\n",
      "nnet2 0.649222662589\n",
      "nnet1 0.623698784984\n",
      "nnet3 0.574520607881\n",
      "nnet4 0.647936920552\n",
      "nnet5 0.648281500429\n",
      "ftrl 0.622924063126\n",
      "nnet6 0.658849449887\n",
      "nnet8 0.657409030233\n",
      "ftrl_50 0.623440994459\n",
      "nnet11 0.660053076044\n",
      "xgb_single 0.660175309563\n",
      "lgbmb 0.66792997872\n",
      "nnet12 0.67274153339\n",
      "nnet13 0.67264163497\n"
     ]
    }
   ],
   "source": [
    "for f in ['lgbm1','nnet2','nnet1','nnet3','nnet4','nnet5','ftrl','nnet6','nnet8','ftrl_50','nnet11','xgb_single','lgbmb','nnet12','nnet13']:\n",
    "    print(f,roc_auc_score(X.target, X[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.815829</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nnet12    nnet13\n",
       "nnet12  1.000000  0.815829\n",
       "nnet13  0.815829  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['nnet12','nnet13']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.872521</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nnet12    nnet13\n",
       "nnet12  1.000000  0.872521\n",
       "nnet13  0.872521  1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te[['nnet12','nnet13']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).astype(np.bool).astype(np.int8).tocsr()\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).astype(np.bool).astype(np.int8).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "(385194, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "0 0.68135971202\n",
      "[ 0.07684783  0.08619268  0.04895015 ...,  0.04038424  0.          0.        ]\n",
      "[ 0.13111016  0.14705343  0.0835139  ...,  0.0688996   0.          0.        ]\n",
      "fold 1\n",
      "prepare train\n",
      "(385194, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "1 0.663118094943\n",
      "[ 0.05970712  0.0895149   0.06341106 ...,  0.02558964  0.          0.        ]\n",
      "[ 0.09207724  0.13804524  0.09778926 ...,  0.03946303  0.          0.        ]\n",
      "fold 2\n",
      "prepare train\n",
      "(385194, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "2 0.661822823645\n",
      "[ 0.07713342  0.09433205  0.0546173  ...,  0.03627067  0.          0.        ]\n",
      "[ 0.11766539  0.14390155  0.08331753 ...,  0.05533014  0.          0.        ]\n",
      "fold 3\n",
      "prepare train\n",
      "(385194, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "3 0.672444359835\n",
      "[ 0.07481077  0.0947302   0.05549809 ...,  0.03726765  0.          0.        ]\n",
      "[ 0.11658251  0.14762425  0.0864863  ...,  0.05807662  0.          0.        ]\n",
      "fold 4\n",
      "prepare train\n",
      "(385195, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "4 0.661029716822\n",
      "[ 0.08711351  0.08881015  0.04223377 ...,  0.03559928  0.          0.        ]\n",
      "[ 0.14728608  0.15015464  0.07140622 ...,  0.06018903  0.          0.        ]\n",
      "fold 5\n",
      "prepare train\n",
      "(385195, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "5 0.666807774084\n",
      "[ 0.07745938  0.09012157  0.04602725 ...,  0.03193549  0.          0.        ]\n",
      "[ 0.12537831  0.14587375  0.07450124 ...,  0.05169184  0.          0.        ]\n",
      "fold 6\n",
      "prepare train\n",
      "(385195, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "6 0.66662234053\n",
      "[ 0.0780789   0.09731534  0.06614446 ...,  0.03878931  0.          0.        ]\n",
      "[ 0.12629404  0.15740932  0.10698986 ...,  0.06274241  0.          0.        ]\n",
      "fold 7\n",
      "prepare train\n",
      "(385195, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "7 0.67539423378\n",
      "[ 0.07665234  0.08460563  0.05806747 ...,  0.04153171  0.          0.        ]\n",
      "[ 0.11848771  0.13078176  0.08975958 ...,  0.06419892  0.          0.        ]\n",
      "fold 8\n",
      "prepare train\n",
      "(385195, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "8 0.670203037639\n",
      "[ 0.08533656  0.09966171  0.04893853 ...,  0.04270891  0.00113313  0.        ]\n",
      "[ 0.12739789  0.14878373  0.07305973 ...,  0.0637596   0.00169163  0.        ]\n",
      "fold 9\n",
      "prepare train\n",
      "(385195, 1647)\n",
      "prepare valid\n",
      "prepare test\n",
      "9 0.668388879929\n",
      "[ 0.0715857   0.10707612  0.05222806 ...,  0.03809234  0.          0.        ]\n",
      "[ 0.11076241  0.16567568  0.08081091 ...,  0.05893914  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier,VWRegressor\n",
    "\n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "              'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days','max_f1','max_f2','max_f3',\n",
    "              'svd_description_1','svd_description_2','svd_description_3','svd_description_4','svd_description_5',\n",
    "              'svd_description_6','svd_description_7','svd_description_8','svd_description_9','svd_description_10'] + ['svd_title_'+str(i+1) for i in range(10)]\n",
    "    \n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    \n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols].fillna(0)\n",
    "    X_va = X_va[train_cols].fillna(0)\n",
    "    X_te = X_te[train_cols].fillna(0)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=0.7)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([\n",
    "        X_tr, sp_train_mat\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    print('prepare valid')\n",
    "    X_va = sp.hstack([\n",
    "        X_va, sp_val_mat\n",
    "    ]).tocsr()    \n",
    "    print('prepare test')\n",
    "    X_te = sp.hstack([\n",
    "        X_te, sp_test_mat\n",
    "    ]).tocsr()     \n",
    "    \n",
    "    model = VWRegressor(power_t=0.99, ftrl=True, l1=15, l2=0.1)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    \n",
    "    yhat = model.predict(X_va)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "   \n",
    "    ytst = model.predict(X_te)\n",
    "    print(ytst)\n",
    "    print(minmax_scale(ytst))\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    ifold += 1 \n",
    "#model = VWRegressor(power_t=0.99, ftrl=True, l1=15, l2=0.1)\n",
    "#0.678657863271\n",
    "#0.654927206221\n",
    "\n",
    "#0.8 perc\n",
    "# 0 0.681140205086\n",
    "# 1 0.664159676552\n",
    "# 2 0.661224538511\n",
    "# 3 0.671735521827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68135971202016565, 0.66311809494326535, 0.66182282364461864, 0.67244435983496054, 0.66102971682157374, 0.66680777408355585, 0.66662234052982638, 0.67539423377967267, 0.67020303763930711, 0.66838887992879226]\n",
      "0.668719097323 0.00605505760655\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66823684230830271"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(X.target.values, y_oof.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.340757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.091455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.137531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.063049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.071593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.340757\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.091455\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.137531\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.063049\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.071593"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'vw_07perc'\n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
