{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/mlboot_dataset/'\n",
    "model_name = 'nn_3br_fm_style'\n",
    "results_dir = './results/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 2053602) (609018, 2812610) (609018, 1057788)\n",
      "(609018, 101794) (609018, 20261) (609018, 9386)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir + 'preprocessed_new.csv') \n",
    "q = pd.read_csv(data_dir + 'sessions.csv')\n",
    "df = df.merge(q, on='uid', how='left')\n",
    "del q\n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index\n",
    "\n",
    "mat1 = sp.load_npz(data_dir+'dmat1.npz').tolil()\n",
    "mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil()\n",
    "mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil()\n",
    "print(mat1.shape, mat2.shape, mat3.shape)\n",
    "\n",
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "df['max_f1'] = mat1.tocsr().max(axis=1).todense()\n",
    "df['max_f2'] = mat2.tocsr().max(axis=1).todense()\n",
    "df['max_f3'] = mat3.tocsr().max(axis=1).todense()\n",
    "\n",
    "limit = 9\n",
    "mat1 = mat1.tocsc()[:, np.where((train_mat1.getnnz(axis=0) > limit) & (test_mat1.getnnz(axis=0) > limit))[0]].tocsr()\n",
    "mat2 = mat2.tocsc()[:, np.where((train_mat2.getnnz(axis=0) > limit) & (test_mat2.getnnz(axis=0) > limit))[0]].tocsr()\n",
    "mat3 = mat3.tocsc()[:, np.where((train_mat3.getnnz(axis=0) > limit) & (test_mat3.getnnz(axis=0) > limit))[0]].tocsr()\n",
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "\n",
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)\n",
    "\n",
    "mat_pca = np.load(data_dir + 'pca_cat100.npy')\n",
    "\n",
    "scaler_mat = MaxAbsScaler()\n",
    "mat_pca = scaler_mat.fit_transform(mat_pca)\n",
    "train_mat_pcat = mat_pca[df_train_index.tolist()]\n",
    "test_mat_pcat = mat_pca[df_test_index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.6 s, sys: 6.62 s, total: 1min 2s\n",
      "Wall time: 53.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scaler_mat = MaxAbsScaler()\n",
    "mat1 = scaler_mat.fit_transform(mat1)\n",
    "scaler_mat = MaxAbsScaler()\n",
    "mat2 = scaler_mat.fit_transform(mat2)\n",
    "scaler_mat = MaxAbsScaler()\n",
    "mat3 = scaler_mat.fit_transform(mat3)\n",
    "\n",
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "import gc\n",
    "del mat1,mat2,mat3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import Sequence\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "       'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "       'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "       'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "       'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "       'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "       'mean_day_cntr', 'diff_num_cats', 'unique_days',\n",
    "        'sess_keys_mean', 'sess_keys_max', 'diff_key1_mean',\n",
    "       'diff_key1_max', 'diff_key2_mean', 'diff_key2_max', 'diff_key3_mean',\n",
    "       'diff_key3_max', 'quot_key1_mean', 'quot_key1_max', 'quot_key2_mean',\n",
    "       'quot_key2_max', 'quot_key3_mean', 'quot_key3_max'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X[train_cols].fillna(0).values)\n",
    "X[train_cols] = scaler.transform(X[train_cols].fillna(0).values)\n",
    "x_te[train_cols] = scaler.transform(x_te[train_cols].fillna(0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import log_loss\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "            \n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, X_seq, y, name, interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.X_seq, self.y = X_seq, y\n",
    "        self.name = name\n",
    "        self.interval = interval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_generator(self.X_seq, steps=len(self.X_seq), \n",
    "                                                          use_multiprocessing=False, workers=1, \n",
    "                                                          max_queue_size=4*cpu_cores).ravel()\n",
    "            auc = roc_auc_score(self.y, y_pred)\n",
    "            logloss = log_loss(self.y, y_pred)\n",
    "            logs[self.name+\"_auc\"] = auc\n",
    "            logs[self.name+\"_logloss\"] = logloss\n",
    "            print((self.name+\"_auc: {:.8f}; \"+\"_logloss: {:.8f}; \").format(auc,logloss))\n",
    "            \n",
    "class FeatureSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, X, y, inx, batch_size, shuffle=False):\n",
    "        \n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.inx = inx\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.inx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.inx.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        batch_inx = self.inx[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        \n",
    "        batch = [x[batch_inx] for x in self.X[:2]] +  [x[batch_inx].todense() for x in self.X[-3:]]\n",
    "        #batch = [x[batch_inx].todense() for x in self.X]\n",
    "        return batch, self.y[batch_inx]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.inx)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sparse_inp1 (InputLayer)        (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_inp3 (InputLayer)        (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "num_inp (InputLayer)            (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 500)          2000        sparse_inp1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 500)          2000        sparse_inp3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sparse_inp2 (InputLayer)        (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 39)           156         num_inp[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 80)           40080       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 80)           40080       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 80)           40080       sparse_inp2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           2560        batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_inp (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 80)           320         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 80)           320         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 80)           320         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64)           256         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 100)          400         dense_inp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 80)           0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 80)           0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 80)           0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64)           0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 100)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 80)           0           dropout_12[0][0]                 \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 80)           0           dropout_12[0][0]                 \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 80)           0           dropout_13[0][0]                 \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 644)          0           dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "                                                                 dropout_14[0][0]                 \n",
      "                                                                 dropout_13[0][0]                 \n",
      "                                                                 multiply_5[0][0]                 \n",
      "                                                                 multiply_6[0][0]                 \n",
      "                                                                 multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1024)         660480      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 1024)         4096        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1024)         0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1668)         0           concatenate_4[0][0]              \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 512)          854528      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 512)          2048        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512)          0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 2180)         0           concatenate_4[0][0]              \n",
      "                                                                 dropout_15[0][0]                 \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 256)          558336      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 256)          1024        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 256)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128)          32896       dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128)          512         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 128)          0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            129         dropout_18[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,242,621\n",
      "Trainable params: 2,235,895\n",
      "Non-trainable params: 6,726\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = \"all_in_focal_loss\"\n",
    "\n",
    "def split_inputs(X):\n",
    "    return np.split(X, X.shape[-1], axis=-1)\n",
    "\n",
    "def buildBaseModel(sparse_len1, sparse_len2, sparse_len3):   \n",
    "    num_inp = Input((len(train_cols),), name='num_inp')\n",
    "    num_x = BatchNormalization()(num_inp)\n",
    "    num_x = Dense(64, activation=\"relu\")(num_x)\n",
    "    num_x = BatchNormalization()(num_x)\n",
    "    num_x = Dropout(0.3)(num_x)\n",
    "    \n",
    "    dense_inp = Input((train_mat_pcat.shape[1],), name='dense_inp')\n",
    "    dense_x = BatchNormalization()(dense_inp)\n",
    "    dense_x = Dropout(0.3)(dense_x)    \n",
    "    \n",
    "    sparse_inp1 = Input((sparse_len1,), name='sparse_inp1')\n",
    "    sparse1_x = sparse_inp1\n",
    "    sparse1_x = BatchNormalization()(sparse_inp1)\n",
    "    sparse1_x = Dense(80, activation=\"tanh\")(sparse1_x)\n",
    "    sparse1_x = BatchNormalization()(sparse1_x)\n",
    "    sparse1_x = Dropout(0.1)(sparse1_x)\n",
    "    \n",
    "    sparse_inp2 = Input((sparse_len2,), name='sparse_inp2')\n",
    "    sparse2_x = sparse_inp2\n",
    "    sparse2_x = Dense(80, activation=\"tanh\")(sparse2_x)\n",
    "    sparse2_x = BatchNormalization()(sparse2_x)\n",
    "    sparse2_x = Dropout(0.1)(sparse2_x)\n",
    "    \n",
    "    sparse_inp3 = Input((sparse_len3,), name='sparse_inp3')\n",
    "    sparse3_x = sparse_inp3\n",
    "    sparse3_x = BatchNormalization()(sparse_inp3)\n",
    "    sparse3_x = Dense(80, activation=\"tanh\")(sparse3_x)\n",
    "    sparse3_x = BatchNormalization()(sparse3_x)\n",
    "    sparse3_x = Dropout(0.1)(sparse3_x)    \n",
    "    \n",
    "    x = concatenate([num_x, dense_x, sparse1_x, sparse3_x, sparse2_x, \n",
    "                     multiply([sparse1_x, sparse2_x]),\n",
    "                     multiply([sparse1_x, sparse3_x]),\n",
    "                     multiply([sparse2_x, sparse3_x])\n",
    "                              ])\n",
    "    \n",
    "    x1 = Dense(1024, activation=\"relu\")(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "    x2 = concatenate([x, x1])\n",
    "    x2 = Dense(512, activation=\"relu\")(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    \n",
    "    x3 = concatenate([x, x1, x2])\n",
    "    x3 = Dense(256, activation=\"relu\")(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "    \n",
    "    x4 = x3\n",
    "    x4 = Dense(128, activation=\"relu\")(x4)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Dropout(0.15)(x4)\n",
    "    \n",
    "    x_output = Dense(1, activation=\"sigmoid\", name=\"output\")(x4)\n",
    "    return Model(inputs = [num_inp, dense_inp, sparse_inp1, sparse_inp2, sparse_inp3], outputs=x_output) #num_inp, ,  sparse_inp3\n",
    "\n",
    "model = buildBaseModel(500, 500, 500)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "TRN_BATCH_SIZE = 512\n",
    "INF_BATCH_SIZE = 512\n",
    "\n",
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=239)\n",
    "epochs = 32\n",
    "pred = np.zeros(y.shape)\n",
    "test_pred = 0\n",
    "ifold = 0\n",
    "\n",
    "fold_auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4332\n",
      "val_auc: 0.58954148; _logloss: 0.20338303; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.58954, saving model to ./results/all_in_focal_loss__f0.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2078\n",
      "val_auc: 0.65184405; _logloss: 0.19638243; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.58954 to 0.65184, saving model to ./results/all_in_focal_loss__f0.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1970\n",
      "val_auc: 0.66426135; _logloss: 0.19215834; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.65184 to 0.66426, saving model to ./results/all_in_focal_loss__f0.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1930\n",
      "val_auc: 0.66862091; _logloss: 0.19167730; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.66426 to 0.66862, saving model to ./results/all_in_focal_loss__f0.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1916\n",
      "val_auc: 0.66979541; _logloss: 0.19147457; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.66862 to 0.66980, saving model to ./results/all_in_focal_loss__f0.h5\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1915\n",
      "val_auc: 0.66973004; _logloss: 0.19283516; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1901\n",
      "val_auc: 0.67300188; _logloss: 0.19145008; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.66980 to 0.67300, saving model to ./results/all_in_focal_loss__f0.h5\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1879\n",
      "val_auc: 0.67407794; _logloss: 0.19191411; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.67300 to 0.67408, saving model to ./results/all_in_focal_loss__f0.h5\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1867\n",
      "val_auc: 0.67290380; _logloss: 0.19227149; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1866\n",
      "val_auc: 0.66929475; _logloss: 0.19228809; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1857\n",
      "val_auc: 0.67079483; _logloss: 0.19331253; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1838\n",
      "val_auc: 0.67118180; _logloss: 0.19296724; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Predicting fold 0\n",
      "fold: 0, auc: 0.6740779442343245\n",
      "fold: 0, logloss: 0.1919141081291919\n",
      "\n",
      "Training fold 1\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4285\n",
      "val_auc: 0.58762137; _logloss: 0.20521038; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.58762, saving model to ./results/all_in_focal_loss__f1.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2058\n",
      "val_auc: 0.63521569; _logloss: 0.19420577; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.58762 to 0.63522, saving model to ./results/all_in_focal_loss__f1.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1961\n",
      "val_auc: 0.64644572; _logloss: 0.19361908; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.63522 to 0.64645, saving model to ./results/all_in_focal_loss__f1.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1929\n",
      "val_auc: 0.65180944; _logloss: 0.19257901; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.64645 to 0.65181, saving model to ./results/all_in_focal_loss__f1.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1916\n",
      "val_auc: 0.65458643; _logloss: 0.19310858; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.65181 to 0.65459, saving model to ./results/all_in_focal_loss__f1.h5\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1911\n",
      "val_auc: 0.65201821; _logloss: 0.19209179; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1895\n",
      "val_auc: 0.65337288; _logloss: 0.19303801; \n",
      "\n",
      "Epoch 00007: val_auc did not improve\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1877\n",
      "val_auc: 0.65699845; _logloss: 0.19310239; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.65459 to 0.65700, saving model to ./results/all_in_focal_loss__f1.h5\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1868\n",
      "val_auc: 0.65804795; _logloss: 0.19219742; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.65700 to 0.65805, saving model to ./results/all_in_focal_loss__f1.h5\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1864\n",
      "val_auc: 0.65686830; _logloss: 0.19437669; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1856\n",
      "val_auc: 0.65765698; _logloss: 0.19313759; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1836\n",
      "val_auc: 0.65514944; _logloss: 0.19464141; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1824\n",
      "val_auc: 0.64925107; _logloss: 0.19647464; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 1\n",
      "fold: 1, auc: 0.6580479488663967\n",
      "fold: 1, logloss: 0.1921974252233691\n",
      "\n",
      "Training fold 2\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4323\n",
      "val_auc: 0.58686197; _logloss: 0.21008016; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.58686, saving model to ./results/all_in_focal_loss__f2.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2066\n",
      "val_auc: 0.62839439; _logloss: 0.20076526; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.58686 to 0.62839, saving model to ./results/all_in_focal_loss__f2.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1956\n",
      "val_auc: 0.64545251; _logloss: 0.19692027; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.62839 to 0.64545, saving model to ./results/all_in_focal_loss__f2.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1926\n",
      "val_auc: 0.64860999; _logloss: 0.19638838; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.64545 to 0.64861, saving model to ./results/all_in_focal_loss__f2.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1905\n",
      "val_auc: 0.65008479; _logloss: 0.19763420; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.64861 to 0.65008, saving model to ./results/all_in_focal_loss__f2.h5\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1902\n",
      "val_auc: 0.65067842; _logloss: 0.19682931; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.65008 to 0.65068, saving model to ./results/all_in_focal_loss__f2.h5\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1888\n",
      "val_auc: 0.64925742; _logloss: nan; \n",
      "\n",
      "Epoch 00007: val_auc did not improve\n",
      "Epoch 8/32\n",
      " 11/753 [..............................] - ETA: 12s - loss: 0.1947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/root/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1869\n",
      "val_auc: 0.65019471; _logloss: nan; \n",
      "\n",
      "Epoch 00008: val_auc did not improve\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1856\n",
      "val_auc: 0.65052205; _logloss: nan; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1854\n",
      "val_auc: 0.65012030; _logloss: 0.19882891; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Predicting fold 2\n",
      "fold: 2, auc: 0.6506784188990538\n",
      "fold: 2, logloss: 0.19682931269647194\n",
      "\n",
      "Training fold 3\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4375\n",
      "val_auc: 0.60296837; _logloss: 0.20455410; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.60297, saving model to ./results/all_in_focal_loss__f3.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2061\n",
      "val_auc: 0.64349107; _logloss: 0.19562605; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.60297 to 0.64349, saving model to ./results/all_in_focal_loss__f3.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1956\n",
      "val_auc: 0.65279454; _logloss: 0.19356655; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.64349 to 0.65279, saving model to ./results/all_in_focal_loss__f3.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1926\n",
      "val_auc: 0.65955275; _logloss: 0.19388844; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.65279 to 0.65955, saving model to ./results/all_in_focal_loss__f3.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1912\n",
      "val_auc: 0.66140970; _logloss: 0.19386957; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.65955 to 0.66141, saving model to ./results/all_in_focal_loss__f3.h5\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1907\n",
      "val_auc: 0.65594817; _logloss: 0.19428936; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1896\n",
      "val_auc: 0.66493955; _logloss: 0.19270065; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.66141 to 0.66494, saving model to ./results/all_in_focal_loss__f3.h5\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1877\n",
      "val_auc: 0.66486395; _logloss: 0.19311549; \n",
      "\n",
      "Epoch 00008: val_auc did not improve\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1862\n",
      "val_auc: 0.66456247; _logloss: 0.19453711; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1861\n",
      "val_auc: 0.65971734; _logloss: 0.19374942; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1853\n",
      "val_auc: 0.66250125; _logloss: inf; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "Predicting fold 3\n",
      "fold: 3, auc: 0.6649395510689554\n",
      "fold: 3, logloss: 0.19270065264812877\n",
      "\n",
      "Training fold 4\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4304\n",
      "val_auc: 0.59502498; _logloss: 0.20977807; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.59502, saving model to ./results/all_in_focal_loss__f4.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2060\n",
      "val_auc: 0.63992363; _logloss: 0.20032607; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.59502 to 0.63992, saving model to ./results/all_in_focal_loss__f4.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1960\n",
      "val_auc: 0.65408055; _logloss: 0.19740980; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.63992 to 0.65408, saving model to ./results/all_in_focal_loss__f4.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1922\n",
      "val_auc: 0.65823347; _logloss: 0.19821358; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.65408 to 0.65823, saving model to ./results/all_in_focal_loss__f4.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1907\n",
      "val_auc: 0.66146279; _logloss: 0.19691283; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.65823 to 0.66146, saving model to ./results/all_in_focal_loss__f4.h5\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1904\n",
      "val_auc: 0.65415442; _logloss: 0.20062318; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1889\n",
      "val_auc: 0.66075225; _logloss: 0.19864249; \n",
      "\n",
      "Epoch 00007: val_auc did not improve\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1867\n",
      "val_auc: 0.66129247; _logloss: 0.19884860; \n",
      "\n",
      "Epoch 00008: val_auc did not improve\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1858\n",
      "val_auc: 0.66182500; _logloss: 0.19865922; \n",
      "\n",
      "Epoch 00009: val_auc improved from 0.66146 to 0.66182, saving model to ./results/all_in_focal_loss__f4.h5\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1856\n",
      "val_auc: 0.65829413; _logloss: 0.19955732; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1845\n",
      "val_auc: 0.65685616; _logloss: 0.19994374; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1826\n",
      "val_auc: 0.65621334; _logloss: 0.20190735; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 13/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1817\n",
      "val_auc: 0.65361572; _logloss: 0.20119971; \n",
      "\n",
      "Epoch 00013: val_auc did not improve\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Predicting fold 4\n",
      "fold: 4, auc: 0.6618249954636792\n",
      "fold: 4, logloss: 0.19865921773591164\n",
      "\n",
      "Training fold 5\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4347\n",
      "val_auc: 0.60519387; _logloss: 0.20292657; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.60519, saving model to ./results/all_in_focal_loss__f5.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2069\n",
      "val_auc: 0.64881307; _logloss: 0.19491186; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.60519 to 0.64881, saving model to ./results/all_in_focal_loss__f5.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1967\n",
      "val_auc: 0.65728056; _logloss: 0.19312390; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.64881 to 0.65728, saving model to ./results/all_in_focal_loss__f5.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1930\n",
      "val_auc: 0.66146342; _logloss: 0.19263136; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.65728 to 0.66146, saving model to ./results/all_in_focal_loss__f5.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1916\n",
      "val_auc: 0.65962711; _logloss: 0.19300255; \n",
      "\n",
      "Epoch 00005: val_auc did not improve\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1907\n",
      "val_auc: 0.65746950; _logloss: 0.19356579; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1896\n",
      "val_auc: 0.65871139; _logloss: 0.19437816; \n",
      "\n",
      "Epoch 00007: val_auc did not improve\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1878\n",
      "val_auc: 0.66190970; _logloss: 0.19289817; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.66146 to 0.66191, saving model to ./results/all_in_focal_loss__f5.h5\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1864\n",
      "val_auc: 0.66124942; _logloss: 0.19354102; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1861\n",
      "val_auc: 0.66117373; _logloss: 0.19433623; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1852\n",
      "val_auc: 0.66120792; _logloss: 0.19572747; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1836\n",
      "val_auc: 0.65809041; _logloss: 0.19540750; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Predicting fold 5\n",
      "fold: 5, auc: 0.6619096958972566\n",
      "fold: 5, logloss: 0.19289817113379915\n",
      "\n",
      "Training fold 6\n",
      "306 406 282\n",
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4349\n",
      "val_auc: 0.58813237; _logloss: 0.20010186; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.58813, saving model to ./results/all_in_focal_loss__f6.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2058\n",
      "val_auc: 0.63884144; _logloss: 0.19505873; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.58813 to 0.63884, saving model to ./results/all_in_focal_loss__f6.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1955\n",
      "val_auc: 0.64890345; _logloss: 0.19287273; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.63884 to 0.64890, saving model to ./results/all_in_focal_loss__f6.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1924\n",
      "val_auc: 0.65371072; _logloss: 0.19292580; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.64890 to 0.65371, saving model to ./results/all_in_focal_loss__f6.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1913\n",
      "val_auc: 0.65323367; _logloss: 0.19282838; \n",
      "\n",
      "Epoch 00005: val_auc did not improve\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1907\n",
      "val_auc: 0.65345327; _logloss: 0.19195900; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1895\n",
      "val_auc: 0.65338403; _logloss: 0.19252246; \n",
      "\n",
      "Epoch 00007: val_auc did not improve\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1874\n",
      "val_auc: 0.65623061; _logloss: 0.19293336; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.65371 to 0.65623, saving model to ./results/all_in_focal_loss__f6.h5\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1866\n",
      "val_auc: 0.65270643; _logloss: 0.19353027; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1859\n",
      "val_auc: 0.64934296; _logloss: 0.19461469; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1854\n",
      "val_auc: 0.65378240; _logloss: 0.19414877; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1832\n",
      "val_auc: 0.64700079; _logloss: 0.19514907; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Predicting fold 6\n",
      "fold: 6, auc: 0.6562306074352757\n",
      "fold: 6, logloss: 0.19293336437176686\n",
      "\n",
      "Training fold 7\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4290\n",
      "val_auc: 0.59672635; _logloss: 0.19989485; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.59673, saving model to ./results/all_in_focal_loss__f7.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2074\n",
      "val_auc: 0.65275911; _logloss: 0.19133303; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.59673 to 0.65276, saving model to ./results/all_in_focal_loss__f7.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1963\n",
      "val_auc: 0.66312915; _logloss: 0.19180815; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.65276 to 0.66313, saving model to ./results/all_in_focal_loss__f7.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1929\n",
      "val_auc: 0.66754062; _logloss: 0.19020655; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.66313 to 0.66754, saving model to ./results/all_in_focal_loss__f7.h5\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1912\n",
      "val_auc: 0.66752481; _logloss: 0.19024798; \n",
      "\n",
      "Epoch 00005: val_auc did not improve\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1909\n",
      "val_auc: 0.66737003; _logloss: 0.19086343; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1894\n",
      "val_auc: 0.66604201; _logloss: 0.19238530; \n",
      "\n",
      "Epoch 00007: val_auc did not improve\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1876\n",
      "val_auc: 0.66917317; _logloss: 0.19140935; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.66754 to 0.66917, saving model to ./results/all_in_focal_loss__f7.h5\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1863\n",
      "val_auc: 0.66556612; _logloss: 0.19228132; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1863\n",
      "val_auc: 0.66578958; _logloss: 0.19327310; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1853\n",
      "val_auc: 0.66548101; _logloss: 0.19323728; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1835\n",
      "val_auc: 0.66349224; _logloss: 0.19434547; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Predicting fold 7\n",
      "fold: 7, auc: 0.6691731672095117\n",
      "fold: 7, logloss: 0.19140934598877157\n",
      "\n",
      "Training fold 8\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4337\n",
      "val_auc: 0.59009074; _logloss: 0.20353494; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.59009, saving model to ./results/all_in_focal_loss__f8.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2064\n",
      "val_auc: 0.63756489; _logloss: 0.19646150; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.59009 to 0.63756, saving model to ./results/all_in_focal_loss__f8.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1962\n",
      "val_auc: 0.65177287; _logloss: 0.19397239; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.63756 to 0.65177, saving model to ./results/all_in_focal_loss__f8.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1927\n",
      "val_auc: 0.65117228; _logloss: 0.19343070; \n",
      "\n",
      "Epoch 00004: val_auc did not improve\n",
      "Epoch 5/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1913\n",
      "val_auc: 0.64990520; _logloss: 0.19417899; \n",
      "\n",
      "Epoch 00005: val_auc did not improve\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1910\n",
      "val_auc: 0.64899741; _logloss: 0.19284528; \n",
      "\n",
      "Epoch 00006: val_auc did not improve\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1894\n",
      "val_auc: 0.65683412; _logloss: 0.19378002; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.65177 to 0.65683, saving model to ./results/all_in_focal_loss__f8.h5\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1876\n",
      "val_auc: 0.65720703; _logloss: 0.19505564; \n",
      "\n",
      "Epoch 00008: val_auc improved from 0.65683 to 0.65721, saving model to ./results/all_in_focal_loss__f8.h5\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1862\n",
      "val_auc: 0.65588664; _logloss: 0.19587368; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1858\n",
      "val_auc: 0.65379241; _logloss: 0.19530109; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1846\n",
      "val_auc: 0.65396269; _logloss: 0.19680753; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 12/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1829\n",
      "val_auc: 0.64921467; _logloss: 0.19671313; \n",
      "\n",
      "Epoch 00012: val_auc did not improve\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Predicting fold 8\n",
      "fold: 8, auc: 0.6572070263125164\n",
      "fold: 8, logloss: 0.1950556429585302\n",
      "\n",
      "Training fold 9\n",
      "306 406 282\n",
      "Epoch 1/32\n",
      "753/753 [==============================] - 12s 16ms/step - loss: 0.4251\n",
      "val_auc: 0.58802420; _logloss: 0.20603237; \n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.58802, saving model to ./results/all_in_focal_loss__f9.h5\n",
      "Epoch 2/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.2062\n",
      "val_auc: 0.64021806; _logloss: 0.20292046; \n",
      "\n",
      "Epoch 00002: val_auc improved from 0.58802 to 0.64022, saving model to ./results/all_in_focal_loss__f9.h5\n",
      "Epoch 3/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1956\n",
      "val_auc: 0.65043677; _logloss: 0.19716966; \n",
      "\n",
      "Epoch 00003: val_auc improved from 0.64022 to 0.65044, saving model to ./results/all_in_focal_loss__f9.h5\n",
      "Epoch 4/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1919\n",
      "val_auc: 0.65526474; _logloss: 0.19698654; \n",
      "\n",
      "Epoch 00004: val_auc improved from 0.65044 to 0.65526, saving model to ./results/all_in_focal_loss__f9.h5\n",
      "Epoch 5/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1910\n",
      "val_auc: 0.65696417; _logloss: 0.19803512; \n",
      "\n",
      "Epoch 00005: val_auc improved from 0.65526 to 0.65696, saving model to ./results/all_in_focal_loss__f9.h5\n",
      "Epoch 6/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1906\n",
      "val_auc: 0.65819376; _logloss: 0.19927630; \n",
      "\n",
      "Epoch 00006: val_auc improved from 0.65696 to 0.65819, saving model to ./results/all_in_focal_loss__f9.h5\n",
      "Epoch 7/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1892\n",
      "val_auc: 0.66147049; _logloss: 0.19661835; \n",
      "\n",
      "Epoch 00007: val_auc improved from 0.65819 to 0.66147, saving model to ./results/all_in_focal_loss__f9.h5\n",
      "Epoch 8/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1875\n",
      "val_auc: 0.65994794; _logloss: 0.19862332; \n",
      "\n",
      "Epoch 00008: val_auc did not improve\n",
      "Epoch 9/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1865\n",
      "val_auc: 0.65982803; _logloss: 0.19979364; \n",
      "\n",
      "Epoch 00009: val_auc did not improve\n",
      "Epoch 10/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1858\n",
      "val_auc: 0.65466405; _logloss: 0.20003551; \n",
      "\n",
      "Epoch 00010: val_auc did not improve\n",
      "Epoch 11/32\n",
      "753/753 [==============================] - 11s 14ms/step - loss: 0.1850\n",
      "val_auc: 0.65759008; _logloss: inf; \n",
      "\n",
      "Epoch 00011: val_auc did not improve\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "Predicting fold 9\n",
      "fold: 9, auc: 0.6614704902785897\n",
      "fold: 9, logloss: 0.19661834747679102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cpu_cores = 4\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha, gamma=0.5):\n",
    "    alpha = K.variable(alpha)\n",
    "    pt = K.abs(1. - y_true - y_pred)\n",
    "    pt = K.clip(pt, K.epsilon(), 1. - K.epsilon())\n",
    "    return K.mean(-alpha * K.pow(1. - pt, gamma) * K.log(pt), axis=-1)\n",
    "\n",
    "\n",
    "for trn_inx, val_inx in kf.split(y):\n",
    "    print(\"Training fold {}\".format(ifold))\n",
    "    K.clear_session()\n",
    "\n",
    "    \n",
    "    model_file_name = model_name+\"__f\"+str(ifold)\n",
    "    model_file = results_dir+model_file_name+'.h5'   \n",
    "    \n",
    "    sp_train_mat1 = train_mat1[trn_inx]\n",
    "    sp_val_mat1 = train_mat1[val_inx]\n",
    "    sp_test_mat1 = test_mat1\n",
    "    \n",
    "    sp_train_mat2 = train_mat2[trn_inx]\n",
    "    sp_val_mat2 = train_mat2[val_inx]\n",
    "    sp_test_mat2 = test_mat2  \n",
    "    \n",
    "    sp_train_mat3 = train_mat3[trn_inx]\n",
    "    sp_val_mat3 = train_mat3[val_inx]\n",
    "    sp_test_mat3 = test_mat3      \n",
    "    \n",
    "    yy = X.target.values[trn_inx]\n",
    "    ssp = SelectPercentile(percentile=0.3)  \n",
    "    ssp.fit(sp_train_mat1, yy)   \n",
    "    sp_train_mat1 = ssp.transform(sp_train_mat1)\n",
    "    sp_val_mat1 = ssp.transform(sp_val_mat1)\n",
    "    sp_test_mat1 = ssp.transform(sp_test_mat1) \n",
    "    del ssp\n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    scaler.fit(sp_train_mat1)\n",
    "    sp_train_mat1 = scaler.transform(sp_train_mat1)\n",
    "    sp_val_mat1 = scaler.transform(sp_val_mat1)\n",
    "    sp_test_mat1 = scaler.transform(sp_test_mat1)\n",
    "    del scaler\n",
    "    \n",
    "    ssp = SelectPercentile(percentile=2)  \n",
    "    ssp.fit(sp_train_mat2, yy)   \n",
    "    sp_train_mat2 = ssp.transform(sp_train_mat2)\n",
    "    sp_val_mat2 = ssp.transform(sp_val_mat2)\n",
    "    sp_test_mat2 = ssp.transform(sp_test_mat2) \n",
    "    del ssp\n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    scaler.fit(sp_train_mat2)\n",
    "    sp_train_mat2 = scaler.transform(sp_train_mat2)\n",
    "    sp_val_mat2 = scaler.transform(sp_val_mat2)\n",
    "    sp_test_mat2 = scaler.transform(sp_test_mat2)\n",
    "    del scaler    \n",
    "    \n",
    "    ssp = SelectPercentile(percentile=3)  \n",
    "    ssp.fit(sp_train_mat3, yy)   \n",
    "    sp_train_mat3 = ssp.transform(sp_train_mat3)\n",
    "    sp_val_mat3 = ssp.transform(sp_val_mat3)\n",
    "    sp_test_mat3 = ssp.transform(sp_test_mat3) \n",
    "    del ssp    \n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    scaler.fit(sp_train_mat3)\n",
    "    sp_train_mat3 = scaler.transform(sp_train_mat3)\n",
    "    sp_val_mat3 = scaler.transform(sp_val_mat3)\n",
    "    sp_test_mat3 = scaler.transform(sp_test_mat3)\n",
    "    del scaler\n",
    "    \n",
    "    print(sp_train_mat1.shape[1],sp_train_mat2.shape[1],sp_train_mat3.shape[1])    \n",
    "    \n",
    "    model = buildBaseModel(sp_train_mat1.shape[1],sp_train_mat2.shape[1],sp_train_mat3.shape[1])    \n",
    "    \n",
    "    trn_seq = FeatureSequence([X.loc[trn_inx,train_cols].values, \n",
    "                               train_mat_pcat[trn_inx],\n",
    "                               sp_train_mat1,\n",
    "                               sp_train_mat2,\n",
    "                               sp_train_mat3\n",
    "                              ], \n",
    "                              y[trn_inx], np.array(list(range(len(trn_inx)))), TRN_BATCH_SIZE, shuffle=True)\n",
    "    val_seq = FeatureSequence([X.loc[val_inx,train_cols].values, \n",
    "                               train_mat_pcat[val_inx],\n",
    "                               sp_val_mat1,\n",
    "                               sp_val_mat2,\n",
    "                               sp_val_mat3,\n",
    "                              ], \n",
    "                              y[val_inx], np.array(list(range(len(val_inx)))), INF_BATCH_SIZE, shuffle=False)\n",
    "    te_seq = FeatureSequence([x_te[train_cols].values, \n",
    "                               test_mat_pcat,\n",
    "                               sp_test_mat1,\n",
    "                               sp_test_mat2,\n",
    "                               sp_test_mat3\n",
    "                             ], \n",
    "                              y, np.array(list(range(len(x_te)))), INF_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # Callbacks\n",
    "    model_checkpoint = ModelCheckpoint(model_file, monitor='val_auc', verbose=1, mode='max',\n",
    "                                       save_best_only=True, save_weights_only=False, period=1)\n",
    "    clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=2*math.ceil(len(trn_seq)), mode='triangular2')\n",
    "    early_stop = EarlyStopping(monitor='val_auc', min_delta=0, patience=4, verbose=1, mode='max')\n",
    "    mse_eval = RocAucEvaluation(val_seq, y[val_inx], 'val')\n",
    "    \n",
    "    # Training\n",
    "    opt=optimizers.Nadam()\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "\n",
    "    model.fit_generator(\n",
    "        generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "        initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "        callbacks=[mse_eval, model_checkpoint, early_stop, clr], #\n",
    "        use_multiprocessing=False, workers=1, max_queue_size=4*cpu_cores)\n",
    "    \n",
    "     \n",
    "    # Predicting\n",
    "    print(\"\\nPredicting fold {}\".format(ifold))\n",
    "    del model  \n",
    "    model = load_model(model_file, compile=True, custom_objects={'<lambda>':lambda y_true, y_pred: focal_loss(y_true, y_pred, 1.6, 2)})\n",
    "    pred[val_inx] = model.predict_generator(val_seq, steps=len(val_seq), \n",
    "                                                    use_multiprocessing=False, workers=1, \n",
    "                                                    max_queue_size=4*cpu_cores).ravel()\n",
    "    \n",
    "    auc = roc_auc_score(y[val_inx], pred[val_inx])\n",
    "    logloss = log_loss(y[val_inx], pred[val_inx])\n",
    "    fold_auc.append(auc)\n",
    "    print(\"fold: {}, auc: {}\".format(ifold, auc))\n",
    "    print(\"fold: {}, logloss: {}\".format(ifold, logloss))\n",
    "    print()\n",
    "    \n",
    "    test_pred += minmax_scale(model.predict_generator(te_seq, steps=len(te_seq), \n",
    "                                                    use_multiprocessing=False, workers=1, \n",
    "                                                    max_queue_size=4*cpu_cores).ravel())/n_folds\n",
    "    ifold += 1\n",
    "    \n",
    "    del sp_train_mat1,sp_val_mat1,sp_test_mat1\n",
    "    del sp_train_mat2,sp_val_mat2,sp_test_mat2\n",
    "    del sp_train_mat3,sp_val_mat3,sp_test_mat3    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67407794423432454, 0.65804794886639673, 0.65067841889905376, 0.66493955106895541, 0.66182499546367923, 0.66190969589725657, 0.65623060743527573, 0.6691731672095117, 0.65720702631251637, 0.66147049027858973]\n",
      "0.661555984567 0.00635904933355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66055827542353485"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'nn_3br_fm_style_v2'\n",
    "print(fold_auc)\n",
    "print(np.mean(fold_auc), np.std(fold_auc))\n",
    "roc_auc_score(X.target.values, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(results_dir + 'train_' + model_name +'.npy', pred)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.078401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.050142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.060386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.043359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.027910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.078401\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.050142\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.060386\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.043359\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.027910"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = test_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67407794423432454, 0.65804794886639673, 0.65067841889905376, 0.66493955106895541, 0.66182499546367923, 0.66190969589725657, 0.65623060743527573, 0.6691731672095117, 0.65720702631251637, 0.66147049027858973]\n",
      "0.661555984567 0.00635904933355\n"
     ]
    }
   ],
   "source": [
    "print(fold_auc)\n",
    "print(np.mean(fold_auc), np.std(fold_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.058912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.047257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.059204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.042887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.030320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.058912\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.047257\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.059204\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.042887\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.030320"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
