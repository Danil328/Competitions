{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/mlboot_dataset/'\n",
    "model_name = 'xgb_b1'\n",
    "results_dir = './results/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.sparse as sp\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 2053602) (609018, 2812610) (609018, 1057788)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir + 'preprocessed_new.csv') \n",
    "q = pd.read_csv(data_dir + 'sessions.csv')\n",
    "df = df.merge(q, on='uid', how='left')\n",
    "del q\n",
    "y = pd.read_table(data_dir + 'mlboot_train_answers.tsv')\n",
    "y.columns = ['uid','target']\n",
    "df = df.merge(y, on='uid', how='left')\n",
    "\n",
    "df_train_index = df[~df.target.isnull()].index\n",
    "df_test_index = df[df.target.isnull()].index\n",
    "\n",
    "mat1 = sp.load_npz(data_dir+'dmat1.npz').tolil()\n",
    "mat2 = sp.load_npz(data_dir+'dmat2.npz').tolil()\n",
    "mat3 = sp.load_npz(data_dir+'dmat3.npz').tolil()\n",
    "print(mat1.shape, mat2.shape, mat3.shape)\n",
    "\n",
    "df['max_f1'] = mat1.tocsr().max(axis=1).todense()\n",
    "df['max_f2'] = mat2.tocsr().max(axis=1).todense()\n",
    "df['max_f3'] = mat3.tocsr().max(axis=1).todense()\n",
    "\n",
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "limit = 11\n",
    "mat1 = mat1.tocsc()[:, np.where((train_mat1.getnnz(axis=0) > limit) & (test_mat1.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat2 = mat2.tocsc()[:, np.where((train_mat2.getnnz(axis=0) > limit) & (test_mat2.getnnz(axis=0) > 0))[0]].tocsr()\n",
    "mat3 = mat3.tocsc()[:, np.where((train_mat3.getnnz(axis=0) > limit) & (test_mat3.getnnz(axis=0) > 0))[0]].tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609018, 195733) (609018, 20268) (609018, 9415)\n"
     ]
    }
   ],
   "source": [
    "print(mat1.shape, mat2.shape, mat3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'pca_cat10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_description_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_svd = pd.DataFrame(np.load(data_dir + 'bin_pca_dim10.npy'), index=df.index)\n",
    "data_svd.columns = ['svd_title_'+str(i+1) for i in range(10)]\n",
    "df = pd.concat([df, data_svd], axis=1)    \n",
    "del data_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat1 = mat1[df_train_index.tolist()]\n",
    "test_mat1 = mat1[df_test_index.tolist()]\n",
    "train_mat2 = mat2[df_train_index.tolist()]\n",
    "test_mat2 = mat2[df_test_index.tolist()]\n",
    "train_mat3 = mat3[df_train_index.tolist()]\n",
    "test_mat3 = mat3[df_test_index.tolist()]\n",
    "\n",
    "del mat1,mat2,mat3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[~df.target.isnull(),:].reset_index(drop=True)\n",
    "x_te = df.loc[df.target.isnull(),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
       "       'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
       "       'records', 'max_days', 'min_days',\n",
       "       ...\n",
       "       'svd_title_1', 'svd_title_2', 'svd_title_3', 'svd_title_4',\n",
       "       'svd_title_5', 'svd_title_6', 'svd_title_7', 'svd_title_8',\n",
       "       'svd_title_9', 'svd_title_10'],\n",
       "      dtype='object', length=107)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_baseline_sparse_10folds.npy'))\n",
    "sample_sub.columns = ['uid','lgbm1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_all_in_focal_loss.npy'))\n",
    "sample_sub.columns = ['uid','nnet2']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m3.npy'))\n",
    "sample_sub.columns = ['uid','nnet3']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_focal_loss_m1.npy'))\n",
    "sample_sub.columns = ['uid','nnet1']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_ftrl_50.npy'))\n",
    "sample_sub.columns = ['uid','ftrl_50']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_ftrl_70.npy'))\n",
    "sample_sub.columns = ['uid','ftrl']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model.npy'))\n",
    "sample_sub.columns = ['uid','nnet4']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_catpca.npy'))\n",
    "sample_sub.columns = ['uid','nnet5']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br.npy'))\n",
    "sample_sub.columns = ['uid','nnet6']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_wcc.npy'))\n",
    "sample_sub.columns = ['uid','nnet7']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_tanh.npy'))\n",
    "sample_sub.columns = ['uid','nnet8']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_binary_lgbm.npy'))\n",
    "sample_sub.columns = ['uid','lgbmb']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_v4.npy'))\n",
    "sample_sub.columns = ['uid','nnet10']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_v5.npy'))\n",
    "sample_sub.columns = ['uid','nnet11']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_xgb_single.npy'))\n",
    "sample_sub.columns = ['uid','xgb_single']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_3br_bin_v2.npy'))\n",
    "sample_sub.columns = ['uid','nnet12']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n",
    "\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "sample_sub['target'] = minmax_scale(np.load(results_dir + 'test_nn_advanced_model_3br_bin.npy'))\n",
    "sample_sub.columns = ['uid','nnet13']\n",
    "x_te = x_te.merge(sample_sub, on='uid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['lgbm1'] = minmax_scale(np.load(results_dir + 'train_baseline_sparse_10folds.npy'))\n",
    "X['lgbmb'] = minmax_scale(np.load(results_dir + 'train_binary_lgbm.npy'))\n",
    "X['nnet2'] = minmax_scale(np.load(results_dir + 'train_all_in_focal_loss.npy'))\n",
    "X['nnet1'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m1.npy'))\n",
    "X['nnet3'] = minmax_scale(np.load(results_dir + 'train_focal_loss_m3.npy'))\n",
    "X['nnet4'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model.npy'))\n",
    "X['nnet5'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_catpca.npy'))\n",
    "X['nnet6'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br.npy'))\n",
    "X['nnet7'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_wcc.npy'))\n",
    "X['nnet8'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_tanh.npy'))\n",
    "X['ftrl_50']  = minmax_scale(np.load(results_dir + 'train_ftrl_50.npy'))\n",
    "X['ftrl']  = minmax_scale(np.load(results_dir + 'train_ftrl_70.npy'))\n",
    "X['nnet10'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_v4.npy'))\n",
    "X['nnet11'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_v5.npy'))\n",
    "X['xgb_single']  = minmax_scale(np.load(results_dir + 'train_xgb_single.npy'))\n",
    "X['nnet12'] = minmax_scale(np.load(results_dir + 'train_nn_3br_bin_v2.npy'))\n",
    "X['nnet13'] = minmax_scale(np.load(results_dir + 'train_nn_advanced_model_3br_bin.npy'))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm1 0.682562603224\n",
      "nnet2 0.649222662589\n",
      "nnet1 0.623698784984\n",
      "nnet3 0.574520607881\n",
      "nnet4 0.647936920552\n",
      "nnet5 0.648281500429\n",
      "ftrl 0.622924063126\n",
      "nnet6 0.658849449887\n",
      "nnet8 0.657409030233\n",
      "ftrl_50 0.623440994459\n",
      "nnet11 0.660053076044\n",
      "xgb_single 0.660175309563\n",
      "lgbmb 0.66792997872\n",
      "nnet12 0.67274153339\n",
      "nnet13 0.67264163497\n"
     ]
    }
   ],
   "source": [
    "for f in ['lgbm1','nnet2','nnet1','nnet3','nnet4','nnet5','ftrl','nnet6','nnet8','ftrl_50','nnet11','xgb_single','lgbmb','nnet12','nnet13']:\n",
    "    print(f,roc_auc_score(X.target, X[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.815829</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nnet12    nnet13\n",
       "nnet12  1.000000  0.815829\n",
       "nnet13  0.815829  1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[['nnet12','nnet13']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nnet12</th>\n",
       "      <th>nnet13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nnet12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nnet13</th>\n",
       "      <td>0.872521</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nnet12    nnet13\n",
       "nnet12  1.000000  0.872521\n",
       "nnet13  0.872521  1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te[['nnet12','nnet13']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = sp.hstack([train_mat1,train_mat2,train_mat3]).astype(np.bool).astype(np.int8).tocsr()\n",
    "test_mat = sp.hstack([test_mat1,test_mat2,test_mat3]).astype(np.bool).astype(np.int8).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "prepare train\n",
      "(385194, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "0 0.682757517347\n",
      "[ 0.15378015  0.16846473  0.14767842 ...,  0.13099262  0.10172154\n",
      "  0.07977919]\n",
      "[ 0.30420652  0.32652455  0.29493295 ...,  0.26957341  0.22508644\n",
      "  0.19173786]\n",
      "fold 1\n",
      "prepare train\n",
      "(385194, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "1 0.669360305267\n",
      "[ 0.15348868  0.16917869  0.1557815  ...,  0.13032513  0.10438966\n",
      "  0.08908784]\n",
      "[ 0.30772025  0.33100839  0.31112341 ...,  0.2733394   0.23484429\n",
      "  0.21213234]\n",
      "fold 2\n",
      "prepare train\n",
      "(385194, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "2 0.662374118036\n",
      "[ 0.15722409  0.16854382  0.14393448 ...,  0.13413106  0.10349783\n",
      "  0.1029726 ]\n",
      "[ 0.32738146  0.34429726  0.30752192 ...,  0.29287204  0.2470948\n",
      "  0.24630992]\n",
      "fold 3\n",
      "prepare train\n",
      "(385194, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "3 0.674173020505\n",
      "[ 0.15494507  0.16905671  0.1554008  ...,  0.13589355  0.1150761\n",
      "  0.09477496]\n",
      "[ 0.31990943  0.34190512  0.32061976 ...,  0.290214    0.25776601\n",
      "  0.22612281]\n",
      "fold 4\n",
      "prepare train\n",
      "(385195, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "4 0.662000360666\n",
      "[ 0.16055847  0.16349317  0.15118961 ...,  0.13190648  0.10812967\n",
      "  0.09044872]\n",
      "[ 0.33789404  0.34221067  0.32411345 ...,  0.29575001  0.26077685\n",
      "  0.23477005]\n",
      "fold 5\n",
      "prepare train\n",
      "(385195, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "5 0.667452900122\n",
      "[ 0.15708487  0.17092779  0.14849096 ...,  0.13024356  0.10593129\n",
      "  0.09919991]\n",
      "[ 0.34656639  0.36703713  0.33385781 ...,  0.30687377  0.27092106\n",
      "  0.26096678]\n",
      "fold 6\n",
      "prepare train\n",
      "(385195, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "6 0.667152103127\n",
      "[ 0.15403321  0.17707251  0.15544698 ...,  0.13061885  0.09538026\n",
      "  0.09662725]\n",
      "[ 0.34115662  0.37562462  0.34327168 ...,  0.3061275   0.25340872\n",
      "  0.25527428]\n",
      "fold 7\n",
      "prepare train\n",
      "(385195, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "7 0.679349506419\n",
      "[ 0.15604412  0.1701623   0.15800949 ...,  0.13598315  0.10208292\n",
      "  0.08914781]\n",
      "[ 0.3137887   0.33516223  0.31676407 ...,  0.28341836  0.23209675\n",
      "  0.21251426]\n",
      "fold 8\n",
      "prepare train\n",
      "(385195, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "8 0.671073835287\n",
      "[ 0.1574934   0.17101809  0.14792484 ...,  0.1366885   0.1046247\n",
      "  0.09492456]\n",
      "[ 0.31373181  0.33372501  0.29958688 ...,  0.28297649  0.23557744\n",
      "  0.22123798]\n",
      "fold 9\n",
      "prepare train\n",
      "(385195, 2121)\n",
      "prepare valid\n",
      "prepare test\n",
      "9 0.670287972974\n",
      "[ 0.15500694  0.17413995  0.15495938 ...,  0.13425242  0.11031882\n",
      "  0.10783321]\n",
      "[ 0.30106864  0.33065693  0.30099509 ...,  0.26897275  0.23196057\n",
      "  0.2281167 ]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier,VWRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "def mean_encode_test(df, y, test,k,column):\n",
    "    mean_0 = np.zeros((test.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)  \n",
    "    y0s = df[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "    y0s.columns = [column,'target_mean']\n",
    "    vc = df[column].value_counts().reset_index()\n",
    "    vc.columns = [column,'counts']\n",
    "    test = test.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "    test['mean_target'] = (test.target_mean * test.counts + k * m0)/(test.counts + k)\n",
    "    mean_0 = np.array(test['mean_target']).reshape(-1,1)\n",
    "    return mean_0    \n",
    "\n",
    "def mean_encode_self(df, y, kf, k, column):\n",
    "    mean_0 = np.zeros((y.shape[0],1))\n",
    "    df['target'] = y\n",
    "    m0 = np.mean(y)\n",
    "    for dev_index, val_index in kf: \n",
    "        dev_X, val_X = df.iloc[dev_index,:], df.iloc[val_index,:]\n",
    "        y0s = dev_X[['target',column]].groupby(column).agg(np.mean).reset_index()\n",
    "        y0s.columns = [column,'target_mean']\n",
    "        vc = dev_X[column].value_counts().reset_index()\n",
    "        vc.columns = [column,'counts']\n",
    "        val_X = val_X.merge(y0s, on = column,how= 'left').merge(vc, on = column,how= 'left')\n",
    "        val_X['mean_target'] = (val_X.target_mean * val_X.counts + k * m0)/(val_X.counts + k)\n",
    "        mean_0[val_index,:] = np.array(val_X['mean_target']).reshape(-1,1)       \n",
    "    return mean_0\n",
    "\n",
    "def make_agg_features(X, train_index, test_index, test_data):\n",
    "    te_cols = ['most_freq_cat']\n",
    "    kf = KFold(n_splits = 5, random_state=2018, shuffle=True)\n",
    "    for c in te_cols:\n",
    "        X.loc[test_index,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), X.loc[test_index,:].copy(), 10.0, c)\n",
    "        test_data.loc[:,c + '_te'] = mean_encode_test(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), test_data.copy(), 10.0, c)\n",
    "        X.loc[train_index,c + '_te'] = mean_encode_self(X.loc[train_index,:].copy(), X.loc[train_index,'target'].copy(), kf.split(X.loc[train_index,:]), 10.0, c)\n",
    "    return X.loc[train_index,:], X.loc[test_index,:], test_data\n",
    "    \n",
    "train_cols = ['sess_keys_mean','sess_keys_max','diff_key1_mean','diff_key1_max','diff_key2_mean',\n",
    "              'diff_key2_max','diff_key3_mean','diff_key3_max','quot_key1_mean','quot_key1_max',\n",
    "              'quot_key2_mean','quot_key2_max','quot_key3_mean','quot_key3_max',\n",
    "              'num_times_cat_eq_0', 'num_times_cat_eq_1', 'num_times_cat_eq_2',\n",
    "              'num_times_cat_eq_3', 'num_times_cat_eq_4', 'num_times_cat_eq_5',\n",
    "              'records', 'max_days', 'min_days', 'sum_values_f1_max',\n",
    "              'num_keys_f1_max', 'sum_values_f2_max', 'num_keys_f2_max',\n",
    "              'sum_values_f3_max', 'num_keys_f3_max', 'sum_values_f1_mean',\n",
    "              'num_keys_f1_mean', 'sum_values_f2_mean', 'num_keys_f2_mean',\n",
    "              'sum_values_f3_mean', 'num_keys_f3_mean', 'max_day_cntr',\n",
    "              'mean_day_cntr', 'nuniq_keys_f1', 'nuniq_keys_f1.1',\n",
    "              'nuniq_keys_f1.2', 'sumval_keys_f1', 'sumval_keys_f1.1',\n",
    "              'sumval_keys_f1.2', 'most_freq_cat_te', 'diff_num_cats', 'unique_days','max_f1','max_f2','max_f3',\n",
    "              'svd_description_1','svd_description_2','svd_description_3','svd_description_4','svd_description_5',\n",
    "              'svd_description_6','svd_description_7','svd_description_8','svd_description_9','svd_description_10'] + ['svd_title_'+str(i+1) for i in range(10)]\n",
    "    \n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=239)\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "y_pred = 0\n",
    "y_oof = X[['uid','target']].copy()\n",
    "y_oof['target'] = np.nan\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in kf.split(X):\n",
    "    print('fold', ifold)\n",
    "       \n",
    "    y_tr,y_va = X.loc[train_index,'target'].values,X.loc[test_index,'target'].values\n",
    "    \n",
    "    X_tr,X_va,X_te = make_agg_features(X,train_index,test_index,x_te)\n",
    "    X_tr = X_tr[train_cols].fillna(0)\n",
    "    X_va = X_va[train_cols].fillna(0)\n",
    "    X_te = X_te[train_cols].fillna(0)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    \n",
    "    yy = y_tr\n",
    "    ssp = SelectPercentile(percentile=0.91)  \n",
    "    ssp.fit(train_mat[train_index], yy)\n",
    "    sp_train_mat = ssp.transform(train_mat[train_index])\n",
    "    sp_val_mat = ssp.transform(train_mat[test_index])\n",
    "    sp_test_mat = ssp.transform(test_mat)   \n",
    "    \n",
    "    print('prepare train')\n",
    "    X_tr = sp.hstack([\n",
    "        X_tr, sp_train_mat\n",
    "    ]).tocsr()\n",
    "    print(X_tr.shape)\n",
    "    print('prepare valid')\n",
    "    X_va = sp.hstack([\n",
    "        X_va, sp_val_mat\n",
    "    ]).tocsr()    \n",
    "    print('prepare test')\n",
    "    X_te = sp.hstack([\n",
    "        X_te, sp_test_mat\n",
    "    ]).tocsr()     \n",
    "    \n",
    "    model = SGDRegressor(loss='squared_epsilon_insensitive', penalty='l2', alpha=0.0001, l1_ratio=0.1, fit_intercept=True, \n",
    "                         max_iter=500000, tol=1e-7, shuffle=True, verbose=0, epsilon=0.1, random_state=None, \n",
    "                         learning_rate='invscaling', eta0=0.0001, power_t=0.25, warm_start=False, average=False)\n",
    "    model.fit(X_tr,y_tr)\n",
    "    \n",
    "    yhat = model.predict(X_va)\n",
    "    scores.append(roc_auc_score(y_va,yhat))\n",
    "    print(ifold,roc_auc_score(y_va,yhat))\n",
    "    y_oof.loc[test_index,'target'] = yhat\n",
    "   \n",
    "    ytst = model.predict(X_te)\n",
    "    print(ytst)\n",
    "    print(minmax_scale(ytst))\n",
    "    y_pred += minmax_scale(ytst)*0.1\n",
    "    \n",
    "    del X_tr,X_va,X_te, sp_train_mat, sp_val_mat, sp_test_mat\n",
    "    gc.collect()    \n",
    "    \n",
    "    ifold += 1 \n",
    "#model = VWRegressor(power_t=0.99, ftrl=True, l1=15, l2=0.1)\n",
    "#0.678657863271\n",
    "#0.654927206221\n",
    "\n",
    "#0.8 perc\n",
    "# 0 0.681140205086\n",
    "# 1 0.664159676552\n",
    "# 2 0.661224538511\n",
    "# 3 0.671735521827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6827575173468734, 0.66936030526719603, 0.66237411803553914, 0.67417302050538952, 0.66200036066550627, 0.66745290012204883, 0.66715210312688822, 0.67934950641895919, 0.67107383528717579, 0.67028797297444553]\n",
      "0.670598163975 0.00634346449094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67043403295451942"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "roc_auc_score(X.target.values, y_oof.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isnull? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuid</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888b238b4d14c03173baa375a739f6bc</td>\n",
       "      <td>0.488274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac4b8244f3ae82df511b002257473c11</td>\n",
       "      <td>0.319160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483d8b91e49522c8a5bbe37f3872c749</td>\n",
       "      <td>0.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c7ec46a0e88a7e1e1cedd2d526d5d61</td>\n",
       "      <td>0.290135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fdbfba9842ff0bf86d600eb334c7c42b</td>\n",
       "      <td>0.315942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cuid    target\n",
       "0  888b238b4d14c03173baa375a739f6bc  0.488274\n",
       "1  ac4b8244f3ae82df511b002257473c11  0.319160\n",
       "2  483d8b91e49522c8a5bbe37f3872c749  0.348200\n",
       "3  4c7ec46a0e88a7e1e1cedd2d526d5d61  0.290135\n",
       "4  fdbfba9842ff0bf86d600eb334c7c42b  0.315942"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'sgd_091perc'\n",
    "\n",
    "np.save(results_dir + 'train_' + model_name +'.npy', y_oof.target.values)\n",
    "sample_sub = pd.read_table(data_dir+'mlboot_test.tsv')\n",
    "\n",
    "sub = x_te[['uid','target']].copy()\n",
    "sub['target'] = y_pred\n",
    "sub.columns = ['cuid','target']\n",
    "sample_sub = sample_sub.merge(sub, on='cuid', how='left')\n",
    "np.save(results_dir + 'test_' + model_name +'.npy', sample_sub.target.values)\n",
    "print('isnull?',sample_sub.target.isnull().any())\n",
    "sample_sub[['target']].to_csv(results_dir + model_name + '.csv', header=False, index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
